
# Source: /content/embedchain/embedchain/models/data_type.py
from enum import Enum


class DirectDataType(Enum):
    """
    DirectDataType enum contains data types that contain raw data directly.
    """

    TEXT = "text"


class IndirectDataType(Enum):
    """
    IndirectDataType enum contains data types that contain references to data stored elsewhere.
    """

    YOUTUBE_VIDEO = "youtube_video"
    PDF_FILE = "pdf_file"
    WEB_PAGE = "web_page"
    SITEMAP = "sitemap"
    XML = "xml"
    DOCX = "docx"
    DOCS_SITE = "docs_site"
    NOTION = "notion"
    CSV = "csv"
    MDX = "mdx"
    IMAGES = "images"
    UNSTRUCTURED = "unstructured"
    JSON = "json"
    OPENAPI = "openapi"
    GMAIL = "gmail"


class SpecialDataType(Enum):
    """
    SpecialDataType enum contains data types that are neither direct nor indirect, or simply require special attention.
    """

    QNA_PAIR = "qna_pair"


class DataType(Enum):
    TEXT = DirectDataType.TEXT.value
    YOUTUBE_VIDEO = IndirectDataType.YOUTUBE_VIDEO.value
    PDF_FILE = IndirectDataType.PDF_FILE.value
    WEB_PAGE = IndirectDataType.WEB_PAGE.value
    SITEMAP = IndirectDataType.SITEMAP.value
    XML = IndirectDataType.XML.value
    DOCX = IndirectDataType.DOCX.value
    DOCS_SITE = IndirectDataType.DOCS_SITE.value
    NOTION = IndirectDataType.NOTION.value
    CSV = IndirectDataType.CSV.value
    MDX = IndirectDataType.MDX.value
    QNA_PAIR = SpecialDataType.QNA_PAIR.value
    IMAGES = IndirectDataType.IMAGES.value
    UNSTRUCTURED = IndirectDataType.UNSTRUCTURED.value
    JSON = IndirectDataType.JSON.value
    OPENAPI = IndirectDataType.OPENAPI.value
    GMAIL = IndirectDataType.GMAIL.value



# Source: /content/embedchain/embedchain/models/vector_dimensions.py
from enum import Enum


# vector length created by embedding fn
class VectorDimensions(Enum):
    GPT4ALL = 384
    OPENAI = 1536
    VERTEX_AI = 768
    HUGGING_FACE = 384



# Source: /content/embedchain/embedchain/models/__init__.py
from .embedding_functions import EmbeddingFunctions  # noqa: F401
from .providers import Providers  # noqa: F401
from .vector_dimensions import VectorDimensions  # noqa: F401



# Source: /content/embedchain/embedchain/models/embedding_functions.py
from enum import Enum


class EmbeddingFunctions(Enum):
    OPENAI = "OPENAI"
    HUGGING_FACE = "HUGGING_FACE"
    VERTEX_AI = "VERTEX_AI"
    GPT4ALL = "GPT4ALL"



# Source: /content/embedchain/embedchain/models/clip_processor.py
try:
    from PIL import Image, UnidentifiedImageError
    from sentence_transformers import SentenceTransformer
except ImportError:
    raise ImportError("Images requires extra dependencies. Install with `pip install 'embedchain[images]'") from None

MODEL_NAME = "clip-ViT-B-32"


class ClipProcessor:
    @staticmethod
    def load_model():
        """Load data from a director of images."""
        # load model and image preprocessing
        model = SentenceTransformer(MODEL_NAME)
        return model

    @staticmethod
    def get_image_features(image_url, model):
        """
        Applies the CLIP model to evaluate the vector representation of the supplied image
        """
        try:
            # load image
            image = Image.open(image_url)
        except FileNotFoundError:
            raise FileNotFoundError("The supplied file does not exist`")
        except UnidentifiedImageError:
            raise UnidentifiedImageError("The supplied file is not an image`")

        image_features = model.encode(image)
        meta_data = {"url": image_url}
        return {"content": image_url, "embedding": image_features.tolist(), "meta_data": meta_data}

    @staticmethod
    def get_text_features(query):
        """
        Applies the CLIP model to evaluate the vector representation of the supplied text
        """
        model = ClipProcessor.load_model()
        text_features = model.encode(query)
        return text_features.tolist()



# Source: /content/embedchain/embedchain/models/providers.py
from enum import Enum


class Providers(Enum):
    OPENAI = "OPENAI"
    ANTHROPHIC = "ANTHPROPIC"
    VERTEX_AI = "VERTEX_AI"
    GPT4ALL = "GPT4ALL"
    AZURE_OPENAI = "AZURE_OPENAI"



# Source: /content/embedchain/embedchain/bots/base.py
from typing import Any

from embedchain import App
from embedchain.config import AddConfig, AppConfig, BaseLlmConfig
from embedchain.embedder.openai import OpenAIEmbedder
from embedchain.helper.json_serializable import (JSONSerializable,
                                                 register_deserializable)
from embedchain.llm.openai import OpenAILlm
from embedchain.vectordb.chroma import ChromaDB


@register_deserializable
class BaseBot(JSONSerializable):
    def __init__(self):
        self.app = App(config=AppConfig(), llm=OpenAILlm(), db=ChromaDB(), embedder=OpenAIEmbedder())

    def add(self, data: Any, config: AddConfig = None):
        """
        Add data to the bot (to the vector database).
        Auto-dectects type only, so some data types might not be usable.

        :param data: data to embed
        :type data: Any
        :param config: configuration class instance, defaults to None
        :type config: AddConfig, optional
        """
        config = config if config else AddConfig()
        self.app.add(data, config=config)

    def query(self, query: str, config: BaseLlmConfig = None) -> str:
        """
        Query the bot

        :param query: the user query
        :type query: str
        :param config: configuration class instance, defaults to None
        :type config: BaseLlmConfig, optional
        :return: Answer
        :rtype: str
        """
        config = config
        return self.app.query(query, config=config)

    def start(self):
        """Start the bot's functionality."""
        raise NotImplementedError("Subclasses must implement the start method.")



# Source: /content/embedchain/embedchain/bots/slack.py
import argparse
import logging
import os
import signal
import sys

from embedchain import App
from embedchain.helper.json_serializable import register_deserializable

from .base import BaseBot

try:
    from flask import Flask, request
    from slack_sdk import WebClient
except ModuleNotFoundError:
    raise ModuleNotFoundError(
        "The required dependencies for Slack are not installed."
        'Please install with `pip install --upgrade "embedchain[slack]"`'
    ) from None


SLACK_BOT_TOKEN = os.environ.get("SLACK_BOT_TOKEN")


@register_deserializable
class SlackBot(BaseBot):
    def __init__(self):
        self.client = WebClient(token=SLACK_BOT_TOKEN)
        self.chat_bot = App()
        self.recent_message = {"ts": 0, "channel": ""}
        super().__init__()

    def handle_message(self, event_data):
        message = event_data.get("event")
        if message and "text" in message and message.get("subtype") != "bot_message":
            text: str = message["text"]
            if float(message.get("ts")) > float(self.recent_message["ts"]):
                self.recent_message["ts"] = message["ts"]
                self.recent_message["channel"] = message["channel"]
                if text.startswith("query"):
                    _, question = text.split(" ", 1)
                    try:
                        response = self.chat_bot.chat(question)
                        self.send_slack_message(message["channel"], response)
                        logging.info("Query answered successfully!")
                    except Exception as e:
                        self.send_slack_message(message["channel"], "An error occurred. Please try again!")
                        logging.error("Error occurred during 'query' command:", e)
                elif text.startswith("add"):
                    _, data_type, url_or_text = text.split(" ", 2)
                    if url_or_text.startswith("<") and url_or_text.endswith(">"):
                        url_or_text = url_or_text[1:-1]
                    try:
                        self.chat_bot.add(url_or_text, data_type)
                        self.send_slack_message(message["channel"], f"Added {data_type} : {url_or_text}")
                    except ValueError as e:
                        self.send_slack_message(message["channel"], f"Error: {str(e)}")
                        logging.error("Error occurred during 'add' command:", e)
                    except Exception as e:
                        self.send_slack_message(message["channel"], f"Failed to add {data_type} : {url_or_text}")
                        logging.error("Error occurred during 'add' command:", e)

    def send_slack_message(self, channel, message):
        response = self.client.chat_postMessage(channel=channel, text=message)
        return response

    def start(self, host="0.0.0.0", port=5000, debug=True):
        app = Flask(__name__)

        def signal_handler(sig, frame):
            logging.info("\nGracefully shutting down the SlackBot...")
            sys.exit(0)

        signal.signal(signal.SIGINT, signal_handler)

        @app.route("/", methods=["POST"])
        def chat():
            # Check if the request is a verification request
            if request.json.get("challenge"):
                return str(request.json.get("challenge"))

            response = self.handle_message(request.json)
            return str(response)

        app.run(host=host, port=port, debug=debug)


def start_command():
    parser = argparse.ArgumentParser(description="EmbedChain SlackBot command line interface")
    parser.add_argument("--host", default="0.0.0.0", help="Host IP to bind")
    parser.add_argument("--port", default=5000, type=int, help="Port to bind")
    args = parser.parse_args()

    slack_bot = SlackBot()
    slack_bot.start(host=args.host, port=args.port)


if __name__ == "__main__":
    start_command()



# Source: /content/embedchain/embedchain/bots/whatsapp.py
import argparse
import importlib
import logging
import signal
import sys

from embedchain.helper.json_serializable import register_deserializable

from .base import BaseBot


@register_deserializable
class WhatsAppBot(BaseBot):
    def __init__(self):
        try:
            self.flask = importlib.import_module("flask")
            self.twilio = importlib.import_module("twilio")
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                "The required dependencies for WhatsApp are not installed. "
                'Please install with `pip install --upgrade "embedchain[whatsapp]"`'
            ) from None
        super().__init__()

    def handle_message(self, message):
        if message.startswith("add "):
            response = self.add_data(message)
        else:
            response = self.ask_bot(message)
        return response

    def add_data(self, message):
        data = message.split(" ")[-1]
        try:
            self.add(data)
            response = f"Added data from: {data}"
        except Exception:
            logging.exception(f"Failed to add data {data}.")
            response = "Some error occurred while adding data."
        return response

    def ask_bot(self, message):
        try:
            response = self.query(message)
        except Exception:
            logging.exception(f"Failed to query {message}.")
            response = "An error occurred. Please try again!"
        return response

    def start(self, host="0.0.0.0", port=5000, debug=True):
        app = self.flask.Flask(__name__)

        def signal_handler(sig, frame):
            logging.info("\nGracefully shutting down the WhatsAppBot...")
            sys.exit(0)

        signal.signal(signal.SIGINT, signal_handler)

        @app.route("/chat", methods=["POST"])
        def chat():
            incoming_message = self.flask.request.values.get("Body", "").lower()
            response = self.handle_message(incoming_message)
            twilio_response = self.twilio.twiml.messaging_response.MessagingResponse()
            twilio_response.message(response)
            return str(twilio_response)

        app.run(host=host, port=port, debug=debug)


def start_command():
    parser = argparse.ArgumentParser(description="EmbedChain WhatsAppBot command line interface")
    parser.add_argument("--host", default="0.0.0.0", help="Host IP to bind")
    parser.add_argument("--port", default=5000, type=int, help="Port to bind")
    args = parser.parse_args()

    whatsapp_bot = WhatsAppBot()
    whatsapp_bot.start(host=args.host, port=args.port)


if __name__ == "__main__":
    start_command()



# Source: /content/embedchain/embedchain/bots/__init__.py
from embedchain.bots.poe import PoeBot  # noqa: F401
from embedchain.bots.whatsapp import WhatsAppBot  # noqa: F401

# TODO: fix discord import
# from embedchain.bots.discord import DiscordBot



# Source: /content/embedchain/embedchain/bots/discord.py
import argparse
import logging
import os

from embedchain.helper.json_serializable import register_deserializable

from .base import BaseBot

try:
    import discord
    from discord import app_commands
    from discord.ext import commands
except ModuleNotFoundError:
    raise ModuleNotFoundError(
        "The required dependencies for Discord are not installed."
        'Please install with `pip install "embedchain[discord]"`'
    ) from None


intents = discord.Intents.default()
intents.message_content = True
client = discord.Client(intents=intents)
tree = app_commands.CommandTree(client)

# Invite link example
# https://discord.com/api/oauth2/authorize?client_id={DISCORD_CLIENT_ID}&permissions=2048&scope=bot


@register_deserializable
class DiscordBot(BaseBot):
    def __init__(self, *args, **kwargs):
        BaseBot.__init__(self, *args, **kwargs)

    def add_data(self, message):
        data = message.split(" ")[-1]
        try:
            self.add(data)
            response = f"Added data from: {data}"
        except Exception:
            logging.exception(f"Failed to add data {data}.")
            response = "Some error occurred while adding data."
        return response

    def ask_bot(self, message):
        try:
            response = self.query(message)
        except Exception:
            logging.exception(f"Failed to query {message}.")
            response = "An error occurred. Please try again!"
        return response

    def start(self):
        client.run(os.environ["DISCORD_BOT_TOKEN"])


# @tree decorator cannot be used in a class. A global discord_bot is used as a workaround.


@tree.command(name="question", description="ask embedchain")
async def query_command(interaction: discord.Interaction, question: str):
    await interaction.response.defer()
    member = client.guilds[0].get_member(client.user.id)
    logging.info(f"User: {member}, Query: {question}")
    try:
        answer = discord_bot.ask_bot(question)
        if args.include_question:
            response = f"> {question}\n\n{answer}"
        else:
            response = answer
        await interaction.followup.send(response)
    except Exception as e:
        await interaction.followup.send("An error occurred. Please try again!")
        logging.error("Error occurred during 'query' command:", e)


@tree.command(name="add", description="add new content to the embedchain database")
async def add_command(interaction: discord.Interaction, url_or_text: str):
    await interaction.response.defer()
    member = client.guilds[0].get_member(client.user.id)
    logging.info(f"User: {member}, Add: {url_or_text}")
    try:
        response = discord_bot.add_data(url_or_text)
        await interaction.followup.send(response)
    except Exception as e:
        await interaction.followup.send("An error occurred. Please try again!")
        logging.error("Error occurred during 'add' command:", e)


@tree.command(name="ping", description="Simple ping pong command")
async def ping(interaction: discord.Interaction):
    await interaction.response.send_message("Pong", ephemeral=True)


@tree.error
async def on_app_command_error(interaction: discord.Interaction, error: discord.app_commands.AppCommandError) -> None:
    if isinstance(error, commands.CommandNotFound):
        await interaction.followup.send("Invalid command. Please refer to the documentation for correct syntax.")
    else:
        logging.error("Error occurred during command execution:", error)


@client.event
async def on_ready():
    # TODO: Sync in admin command, to not hit rate limits.
    # This might be overkill for most users, and it would require to set a guild or user id, where sync is allowed.
    await tree.sync()
    logging.debug("Command tree synced")
    logging.info(f"Logged in as {client.user.name}")


def start_command():
    parser = argparse.ArgumentParser(description="EmbedChain DiscordBot command line interface")
    parser.add_argument(
        "--include-question",
        help="include question in query reply, otherwise it is hidden behind the slash command.",
        action="store_true",
    )
    global args
    args = parser.parse_args()

    global discord_bot
    discord_bot = DiscordBot()
    discord_bot.start()


if __name__ == "__main__":
    start_command()



# Source: /content/embedchain/embedchain/bots/poe.py
import argparse
import logging
import os
from typing import List, Optional

from embedchain.helper.json_serializable import register_deserializable

from .base import BaseBot

try:
    from fastapi_poe import PoeBot, run
except ModuleNotFoundError:
    raise ModuleNotFoundError(
        "The required dependencies for Poe are not installed." 'Please install with `pip install "embedchain[poe]"`'
    ) from None


def start_command():
    parser = argparse.ArgumentParser(description="EmbedChain PoeBot command line interface")
    # parser.add_argument("--host", default="0.0.0.0", help="Host IP to bind")
    parser.add_argument("--port", default=8080, type=int, help="Port to bind")
    parser.add_argument("--api-key", type=str, help="Poe API key")
    # parser.add_argument(
    #     "--history-length",
    #     default=5,
    #     type=int,
    #     help="Set the max size of the chat history. Multiplies cost, but improves conversation awareness.",
    # )
    args = parser.parse_args()

    # FIXME: Arguments are automatically loaded by Poebot's ArgumentParser which causes it to fail.
    # the port argument here is also just for show, it actually works because poe has the same argument.

    run(PoeBot(), api_key=args.api_key or os.environ.get("POE_API_KEY"))


@register_deserializable
class PoeBot(BaseBot, PoeBot):
    def __init__(self):
        self.history_length = 5
        super().__init__()

    async def get_response(self, query):
        last_message = query.query[-1].content
        try:
            history = (
                [f"{m.role}: {m.content}" for m in query.query[-(self.history_length + 1) : -1]]
                if len(query.query) > 0
                else None
            )
        except Exception as e:
            logging.error(f"Error when processing the chat history. Message is being sent without history. Error: {e}")
        answer = self.handle_message(last_message, history)
        yield self.text_event(answer)

    def handle_message(self, message, history: Optional[List[str]] = None):
        if message.startswith("/add "):
            response = self.add_data(message)
        else:
            response = self.ask_bot(message, history)
        return response

    # def add_data(self, message):
    #     data = message.split(" ")[-1]
    #     try:
    #         self.add(data)
    #         response = f"Added data from: {data}"
    #     except Exception:
    #         logging.exception(f"Failed to add data {data}.")
    #         response = "Some error occurred while adding data."
    #     return response

    def ask_bot(self, message, history: List[str]):
        try:
            self.app.llm.set_history(history=history)
            response = self.query(message)
        except Exception:
            logging.exception(f"Failed to query {message}.")
            response = "An error occurred. Please try again!"
        return response

    def start(self):
        start_command()


if __name__ == "__main__":
    start_command()



# Source: /content/embedchain/embedchain/embedder/openai.py
import os
from typing import Optional

from langchain.embeddings import OpenAIEmbeddings

from embedchain.config import BaseEmbedderConfig
from embedchain.embedder.base import BaseEmbedder
from embedchain.models import VectorDimensions

try:
    from chromadb.utils import embedding_functions
except RuntimeError:
    from embedchain.utils import use_pysqlite3

    use_pysqlite3()
    from chromadb.utils import embedding_functions


class OpenAIEmbedder(BaseEmbedder):
    def __init__(self, config: Optional[BaseEmbedderConfig] = None):
        super().__init__(config=config)
        if self.config.model is None:
            self.config.model = "text-embedding-ada-002"

        if self.config.deployment_name:
            embeddings = OpenAIEmbeddings(deployment=self.config.deployment_name)
            embedding_fn = BaseEmbedder._langchain_default_concept(embeddings)
        else:
            if os.getenv("OPENAI_API_KEY") is None and os.getenv("OPENAI_ORGANIZATION") is None:
                raise ValueError(
                    "OPENAI_API_KEY or OPENAI_ORGANIZATION environment variables not provided"
                )  # noqa:E501
            embedding_fn = embedding_functions.OpenAIEmbeddingFunction(
                api_key=os.getenv("OPENAI_API_KEY"),
                organization_id=os.getenv("OPENAI_ORGANIZATION"),
                model_name=self.config.model,
            )

        self.set_embedding_fn(embedding_fn=embedding_fn)
        self.set_vector_dimension(vector_dimension=VectorDimensions.OPENAI.value)



# Source: /content/embedchain/embedchain/embedder/vertexai.py
from typing import Optional

from langchain.embeddings import VertexAIEmbeddings

from embedchain.config import BaseEmbedderConfig
from embedchain.embedder.base import BaseEmbedder
from embedchain.models import VectorDimensions


class VertexAIEmbedder(BaseEmbedder):
    def __init__(self, config: Optional[BaseEmbedderConfig] = None):
        super().__init__(config=config)

        embeddings = VertexAIEmbeddings(model_name=config.model)
        embedding_fn = BaseEmbedder._langchain_default_concept(embeddings)
        self.set_embedding_fn(embedding_fn=embedding_fn)

        vector_dimension = VectorDimensions.VERTEX_AI.value
        self.set_vector_dimension(vector_dimension=vector_dimension)



# Source: /content/embedchain/embedchain/embedder/base.py
from typing import Any, Callable, Optional

from embedchain.config.embedder.base import BaseEmbedderConfig

try:
    from chromadb.api.types import Documents, Embeddings
except RuntimeError:
    from embedchain.utils import use_pysqlite3

    use_pysqlite3()
    from chromadb.api.types import Documents, Embeddings


class BaseEmbedder:
    """
    Class that manages everything regarding embeddings. Including embedding function, loaders and chunkers.

    Embedding functions and vector dimensions are set based on the child class you choose.
    To manually overwrite you can use this classes `set_...` methods.
    """

    def __init__(self, config: Optional[BaseEmbedderConfig] = None):
        """
        Intialize the embedder class.

        :param config: embedder configuration option class, defaults to None
        :type config: Optional[BaseEmbedderConfig], optional
        """
        if config is None:
            self.config = BaseEmbedderConfig()
        else:
            self.config = config
        self.vector_dimension: int

    def set_embedding_fn(self, embedding_fn: Callable[[list[str]], list[str]]):
        """
        Set or overwrite the embedding function to be used by the database to store and retrieve documents.

        :param embedding_fn: Function to be used to generate embeddings.
        :type embedding_fn: Callable[[list[str]], list[str]]
        :raises ValueError: Embedding function is not callable.
        """
        if not hasattr(embedding_fn, "__call__"):
            raise ValueError("Embedding function is not a function")
        self.embedding_fn = embedding_fn

    def set_vector_dimension(self, vector_dimension: int):
        """
        Set or overwrite the vector dimension size

        :param vector_dimension: vector dimension size
        :type vector_dimension: int
        """
        if not isinstance(vector_dimension, int):
            raise TypeError("vector dimension must be int")
        self.vector_dimension = vector_dimension

    @staticmethod
    def _langchain_default_concept(embeddings: Any):
        """
        Langchains default function layout for embeddings.

        :param embeddings: Langchain embeddings
        :type embeddings: Any
        :return: embedding function
        :rtype: Callable
        """

        def embed_function(texts: Documents) -> Embeddings:
            return embeddings.embed_documents(texts)

        return embed_function



# Source: /content/embedchain/embedchain/embedder/gpt4all.py
from typing import Optional

from embedchain.config import BaseEmbedderConfig
from embedchain.embedder.base import BaseEmbedder
from embedchain.models import VectorDimensions


class GPT4AllEmbedder(BaseEmbedder):
    def __init__(self, config: Optional[BaseEmbedderConfig] = None):
        super().__init__(config=config)

        from langchain.embeddings import \
            GPT4AllEmbeddings as LangchainGPT4AllEmbeddings

        embeddings = LangchainGPT4AllEmbeddings()
        embedding_fn = BaseEmbedder._langchain_default_concept(embeddings)
        self.set_embedding_fn(embedding_fn=embedding_fn)

        vector_dimension = VectorDimensions.GPT4ALL.value
        self.set_vector_dimension(vector_dimension=vector_dimension)



# Source: /content/embedchain/embedchain/embedder/__init__.py



# Source: /content/embedchain/embedchain/embedder/huggingface.py
from typing import Optional

from langchain.embeddings import HuggingFaceEmbeddings

from embedchain.config import BaseEmbedderConfig
from embedchain.embedder.base import BaseEmbedder
from embedchain.models import VectorDimensions


class HuggingFaceEmbedder(BaseEmbedder):
    def __init__(self, config: Optional[BaseEmbedderConfig] = None):
        super().__init__(config=config)

        embeddings = HuggingFaceEmbeddings(model_name=self.config.model)
        embedding_fn = BaseEmbedder._langchain_default_concept(embeddings)
        self.set_embedding_fn(embedding_fn=embedding_fn)

        vector_dimension = VectorDimensions.HUGGING_FACE.value
        self.set_vector_dimension(vector_dimension=vector_dimension)



# Source: /content/embedchain/embedchain/telemetry/__init__.py



# Source: /content/embedchain/embedchain/telemetry/posthog.py
import json
import logging
import os
import uuid
from pathlib import Path

from posthog import Posthog

import embedchain

HOME_DIR = str(Path.home())
CONFIG_DIR = os.path.join(HOME_DIR, ".embedchain")
CONFIG_FILE = os.path.join(CONFIG_DIR, "config.json")

logger = logging.getLogger(__name__)


class AnonymousTelemetry:
    def __init__(self, host="https://app.posthog.com", enabled=True):
        self.project_api_key = "phc_XnMmNHzwxE7PVHX4mD2r8K6nfxVM48a2sq2U3N1p2lO"
        self.host = host
        self.posthog = Posthog(project_api_key=self.project_api_key, host=self.host)
        self.user_id = self.get_user_id()
        self.enabled = enabled

        # Check if telemetry tracking is disabled via environment variable
        if "EC_TELEMETRY" in os.environ and os.environ["EC_TELEMETRY"].lower() not in [
            "1",
            "true",
            "yes",
        ]:
            self.enabled = False

        if not self.enabled:
            self.posthog.disabled = True

        # Silence posthog logging
        posthog_logger = logging.getLogger("posthog")
        posthog_logger.disabled = True

    def get_user_id(self):
        if not os.path.exists(CONFIG_DIR):
            os.makedirs(CONFIG_DIR)

        if os.path.exists(CONFIG_FILE):
            with open(CONFIG_FILE, "r") as f:
                data = json.load(f)
                if "user_id" in data:
                    return data["user_id"]

        user_id = str(uuid.uuid4())
        with open(CONFIG_FILE, "w") as f:
            json.dump({"user_id": user_id}, f)
        return user_id

    def capture(self, event_name, properties=None):
        default_properties = {
            "version": embedchain.__version__,
            "language": "python",
            "pid": os.getpid(),
        }
        properties.update(default_properties)

        try:
            self.posthog.capture(self.user_id, event_name, properties)
        except Exception:
            logger.exception(f"Failed to send telemetry {event_name=}")



# Source: /content/embedchain/embedchain/helper/json_serializable.py
import json
import logging
from string import Template
from typing import Any, Dict, Type, TypeVar, Union

T = TypeVar("T", bound="JSONSerializable")

# NOTE: Through inheritance, all of our classes should be children of JSONSerializable. (highest level)
# NOTE: The @register_deserializable decorator should be added to all user facing child classes. (lowest level)


def register_deserializable(cls: Type[T]) -> Type[T]:
    """
    A class decorator to register a class as deserializable.

    When a class is decorated with @register_deserializable, it becomes
    a part of the set of classes that the JSONSerializable class can
    deserialize.

    Deserialization is in essence loading attributes from a json file.
    This decorator is a security measure put in place to make sure that
    you don't load attributes that were initially part of another class.

    Example:
        @register_deserializable
        class ChildClass(JSONSerializable):
            def __init__(self, ...):
                # initialization logic

    Args:
        cls (Type): The class to be registered.

    Returns:
        Type: The same class, after registration.
    """
    JSONSerializable.register_class_as_deserializable(cls)
    return cls


class JSONSerializable:
    """
    A class to represent a JSON serializable object.

    This class provides methods to serialize and deserialize objects,
    as well as save serialized objects to a file and load them back.
    """

    _deserializable_classes = set()  # Contains classes that are whitelisted for deserialization.

    def serialize(self) -> str:
        """
        Serialize the object to a JSON-formatted string.

        Returns:
            str: A JSON string representation of the object.
        """
        try:
            return json.dumps(self, default=self._auto_encoder, ensure_ascii=False)
        except Exception as e:
            logging.error(f"Serialization error: {e}")
            return "{}"

    @classmethod
    def deserialize(cls, json_str: str) -> Any:
        """
        Deserialize a JSON-formatted string to an object.
        If it fails, a default class is returned instead.
        Note: This *returns* an instance, it's not automatically loaded on the calling class.

        Example:
            app = App.deserialize(json_str)

        Args:
            json_str (str): A JSON string representation of an object.

        Returns:
            Object: The deserialized object.
        """
        try:
            return json.loads(json_str, object_hook=cls._auto_decoder)
        except Exception as e:
            logging.error(f"Deserialization error: {e}")
            # Return a default instance in case of failure
            return cls()

    @staticmethod
    def _auto_encoder(obj: Any) -> Union[Dict[str, Any], None]:
        """
        Automatically encode an object for JSON serialization.

        Args:
            obj (Object): The object to be encoded.

        Returns:
            dict: A dictionary representation of the object.
        """
        if hasattr(obj, "__dict__"):
            dct = obj.__dict__.copy()
            for key, value in list(
                dct.items()
            ):  # We use list() to get a copy of items to avoid dictionary size change during iteration.
                try:
                    # Recursive: If the value is an instance of a subclass of JSONSerializable,
                    # serialize it using the JSONSerializable serialize method.
                    if isinstance(value, JSONSerializable):
                        serialized_value = value.serialize()
                        # The value is stored as a serialized string.
                        dct[key] = json.loads(serialized_value)
                    # Custom rules (subclass is not json serializable by default)
                    elif isinstance(value, Template):
                        dct[key] = {"__type__": "Template", "data": value.template}
                    # Future custom types we can follow a similar pattern
                    # elif isinstance(value, SomeOtherType):
                    #     dct[key] = {
                    #         "__type__": "SomeOtherType",
                    #         "data": value.some_method()
                    #     }
                    # NOTE: Keep in mind that this logic needs to be applied to the decoder too.
                    else:
                        json.dumps(value)  # Try to serialize the value.
                except TypeError:
                    del dct[key]  # If it fails, remove the key-value pair from the dictionary.

            dct["__class__"] = obj.__class__.__name__
            return dct
        raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

    @classmethod
    def _auto_decoder(cls, dct: Dict[str, Any]) -> Any:
        """
        Automatically decode a dictionary to an object during JSON deserialization.

        Args:
            dct (dict): The dictionary representation of an object.

        Returns:
            Object: The decoded object or the original dictionary if decoding is not possible.
        """
        class_name = dct.pop("__class__", None)
        if class_name:
            if not hasattr(cls, "_deserializable_classes"):  # Additional safety check
                raise AttributeError(f"`{class_name}` has no registry of allowed deserializations.")
            if class_name not in {cl.__name__ for cl in cls._deserializable_classes}:
                raise KeyError(f"Deserialization of class `{class_name}` is not allowed.")
            target_class = next((cl for cl in cls._deserializable_classes if cl.__name__ == class_name), None)
            if target_class:
                obj = target_class.__new__(target_class)
                for key, value in dct.items():
                    if isinstance(value, dict) and "__type__" in value:
                        if value["__type__"] == "Template":
                            value = Template(value["data"])
                        # For future custom types we can follow a similar pattern
                        # elif value["__type__"] == "SomeOtherType":
                        #     value = SomeOtherType.some_constructor(value["data"])
                    default_value = getattr(target_class, key, None)
                    setattr(obj, key, value or default_value)
                return obj
        return dct

    def save_to_file(self, filename: str) -> None:
        """
        Save the serialized object to a file.

        Args:
            filename (str): The path to the file where the object should be saved.
        """
        with open(filename, "w", encoding="utf-8") as f:
            f.write(self.serialize())

    @classmethod
    def load_from_file(cls, filename: str) -> Any:
        """
        Load and deserialize an object from a file.

        Args:
            filename (str): The path to the file from which the object should be loaded.

        Returns:
            Object: The deserialized object.
        """
        with open(filename, "r", encoding="utf-8") as f:
            json_str = f.read()
            return cls.deserialize(json_str)

    @classmethod
    def register_class_as_deserializable(cls, target_class: Type[T]) -> None:
        """
        Register a class as deserializable. This is a classmethod and globally shared.

        This method adds the target class to the set of classes that
        can be deserialized. This is a security measure to ensure only
        whitelisted classes are deserialized.

        Args:
            target_class (Type): The class to be registered.
        """
        cls._deserializable_classes.add(target_class)



# Source: /content/embedchain/embedchain/loaders/notion.py
import hashlib
import logging
import os

try:
    from llama_hub.notion.base import NotionPageReader
except ImportError:
    raise ImportError(
        "Notion requires extra dependencies. Install with `pip install --upgrade embedchain[community]`"
    ) from None


from embedchain.helper.json_serializable import register_deserializable
from embedchain.loaders.base_loader import BaseLoader
from embedchain.utils import clean_string


@register_deserializable
class NotionLoader(BaseLoader):
    def load_data(self, source):
        """Load data from a PDF file."""

        # Reformat Id to match notion expectation
        id = source[-32:]
        formatted_id = f"{id[:8]}-{id[8:12]}-{id[12:16]}-{id[16:20]}-{id[20:]}"
        logging.debug(f"Extracted notion page id as: {formatted_id}")

        # Get page through the notion api
        integration_token = os.getenv("NOTION_INTEGRATION_TOKEN")
        reader = NotionPageReader(integration_token=integration_token)
        documents = reader.load_data(page_ids=[formatted_id])

        # Extract text
        raw_text = documents[0].text

        # Clean text
        text = clean_string(raw_text)
        doc_id = hashlib.sha256((text + source).encode()).hexdigest()
        return {
            "doc_id": doc_id,
            "data": [
                {
                    "content": text,
                    "meta_data": {"url": f"notion-{formatted_id}"},
                }
            ],
        }



# Source: /content/embedchain/embedchain/loaders/gmail.py
import hashlib
import logging
import os
import quopri
from textwrap import dedent

from bs4 import BeautifulSoup

try:
    from llama_hub.gmail.base import GmailReader
except ImportError:
    raise ImportError("Gmail requires extra dependencies. Install with `pip install embedchain[gmail]`") from None

from embedchain.loaders.base_loader import BaseLoader
from embedchain.utils import clean_string


def get_header(text: str, header: str) -> str:
    start_string_position = text.find(header)
    pos_start = text.find(":", start_string_position) + 1
    pos_end = text.find("\n", pos_start)
    header = text[pos_start:pos_end]
    return header.strip()


class GmailLoader(BaseLoader):
    def load_data(self, query):
        """Load data from gmail."""
        if not os.path.isfile("credentials.json"):
            raise FileNotFoundError(
                "You must download the valid credentials file from your google \
                dev account. Refer this `https://cloud.google.com/docs/authentication/api-keys`"
            )

        loader = GmailReader(query=query, service=None, results_per_page=20)
        documents = loader.load_data()
        logging.info(f"Gmail Loader: {len(documents)} mails found for query- {query}")

        data = []
        data_contents = []
        logging.info(f"Gmail Loader: {len(documents)} mails found")
        for document in documents:
            original_size = len(document.text)

            snippet = document.metadata.get("snippet")
            meta_data = {
                "url": document.metadata.get("id"),
                "date": get_header(document.text, "Date"),
                "subject": get_header(document.text, "Subject"),
                "from": get_header(document.text, "From"),
                "to": get_header(document.text, "To"),
                "search_query": query,
            }

            # Decode
            decoded_bytes = quopri.decodestring(document.text)
            decoded_str = decoded_bytes.decode("utf-8", errors="replace")

            # Slice
            mail_start = decoded_str.find("<!DOCTYPE")
            email_data = decoded_str[mail_start:]

            # Web Page HTML Processing
            soup = BeautifulSoup(email_data, "html.parser")

            tags_to_exclude = [
                "nav",
                "aside",
                "form",
                "header",
                "noscript",
                "svg",
                "canvas",
                "footer",
                "script",
                "style",
            ]

            for tag in soup(tags_to_exclude):
                tag.decompose()

            ids_to_exclude = ["sidebar", "main-navigation", "menu-main-menu"]
            for id in ids_to_exclude:
                tags = soup.find_all(id=id)
                for tag in tags:
                    tag.decompose()

            classes_to_exclude = [
                "elementor-location-header",
                "navbar-header",
                "nav",
                "header-sidebar-wrapper",
                "blog-sidebar-wrapper",
                "related-posts",
            ]

            for class_name in classes_to_exclude:
                tags = soup.find_all(class_=class_name)
                for tag in tags:
                    tag.decompose()

            content = soup.get_text()
            content = clean_string(content)

            cleaned_size = len(content)
            if original_size != 0:
                logging.info(
                    f"[{id}] Cleaned page size: {cleaned_size} characters, down from {original_size} (shrunk: {original_size-cleaned_size} chars, {round((1-(cleaned_size/original_size)) * 100, 2)}%)"  # noqa:E501
                )

            result = f"""
            email from '{meta_data.get('from')}' to '{meta_data.get('to')}'
            subject: {meta_data.get('subject')}
            date: {meta_data.get('date')}
            preview: {snippet}
            content: f{content}
            """
            data_content = dedent(result)
            data.append({"content": data_content, "meta_data": meta_data})
            data_contents.append(data_content)
        doc_id = hashlib.sha256((query + ", ".join(data_contents)).encode()).hexdigest()
        response_data = {"doc_id": doc_id, "data": data}
        return response_data



# Source: /content/embedchain/embedchain/loaders/openapi.py
import hashlib
from io import StringIO
from urllib.parse import urlparse

import requests
import yaml

from embedchain.loaders.base_loader import BaseLoader


class OpenAPILoader(BaseLoader):
    @staticmethod
    def _get_file_content(content):
        url = urlparse(content)
        if all([url.scheme, url.netloc]) and url.scheme not in ["file", "http", "https"]:
            raise ValueError("Not a valid URL.")

        if url.scheme in ["http", "https"]:
            response = requests.get(content)
            response.raise_for_status()
            return StringIO(response.text)
        elif url.scheme == "file":
            path = url.path
            return open(path)
        else:
            return open(content)

    @staticmethod
    def load_data(content):
        """Load yaml file of openapi. Each pair is a document."""
        data = []
        file_path = content
        data_content = []
        with OpenAPILoader._get_file_content(content=content) as file:
            yaml_data = yaml.load(file, Loader=yaml.Loader)
            for i, (key, value) in enumerate(yaml_data.items()):
                string_data = f"{key}: {value}"
                meta_data = {"url": file_path, "row": i + 1}
                data.append({"content": string_data, "meta_data": meta_data})
                data_content.append(string_data)
        doc_id = hashlib.sha256((content + ", ".join(data_content)).encode()).hexdigest()
        return {"doc_id": doc_id, "data": data}



# Source: /content/embedchain/embedchain/loaders/csv.py
import csv
import hashlib
from io import StringIO
from urllib.parse import urlparse

import requests

from embedchain.loaders.base_loader import BaseLoader


class CsvLoader(BaseLoader):
    @staticmethod
    def _detect_delimiter(first_line):
        delimiters = [",", "\t", ";", "|"]
        counts = {delimiter: first_line.count(delimiter) for delimiter in delimiters}
        return max(counts, key=counts.get)

    @staticmethod
    def _get_file_content(content):
        url = urlparse(content)
        if all([url.scheme, url.netloc]) and url.scheme not in ["file", "http", "https"]:
            raise ValueError("Not a valid URL.")

        if url.scheme in ["http", "https"]:
            response = requests.get(content)
            response.raise_for_status()
            return StringIO(response.text)
        elif url.scheme == "file":
            path = url.path
            return open(path, newline="")  # Open the file using the path from the URI
        else:
            return open(content, newline="")  # Treat content as a regular file path

    @staticmethod
    def load_data(content):
        """Load a csv file with headers. Each line is a document"""
        result = []
        lines = []
        with CsvLoader._get_file_content(content) as file:
            first_line = file.readline()
            delimiter = CsvLoader._detect_delimiter(first_line)
            file.seek(0)  # Reset the file pointer to the start
            reader = csv.DictReader(file, delimiter=delimiter)
            for i, row in enumerate(reader):
                line = ", ".join([f"{field}: {value}" for field, value in row.items()])
                lines.append(line)
                result.append({"content": line, "meta_data": {"url": content, "row": i + 1}})
        doc_id = hashlib.sha256((content + " ".join(lines)).encode()).hexdigest()
        return {"doc_id": doc_id, "data": result}



# Source: /content/embedchain/embedchain/loaders/pdf_file.py
import hashlib

try:
    from langchain.document_loaders import PyPDFLoader
except ImportError:
    raise ImportError(
        'PDF File requires extra dependencies. Install with `pip install --upgrade "embedchain[dataloaders]"`'
    ) from None
from embedchain.helper.json_serializable import register_deserializable
from embedchain.loaders.base_loader import BaseLoader
from embedchain.utils import clean_string


@register_deserializable
class PdfFileLoader(BaseLoader):
    def load_data(self, url):
        """Load data from a PDF file."""
        loader = PyPDFLoader(url)
        data = []
        all_content = []
        pages = loader.load_and_split()
        if not len(pages):
            raise ValueError("No data found")
        for page in pages:
            content = page.page_content
            content = clean_string(content)
            meta_data = page.metadata
            meta_data["url"] = url
            data.append(
                {
                    "content": content,
                    "meta_data": meta_data,
                }
            )
            all_content.append(content)
        doc_id = hashlib.sha256((" ".join(all_content) + url).encode()).hexdigest()
        return {
            "doc_id": doc_id,
            "data": data,
        }



# Source: /content/embedchain/embedchain/loaders/__init__.py



# Source: /content/embedchain/embedchain/loaders/images.py
import hashlib
import logging
import os

from embedchain.loaders.base_loader import BaseLoader


class ImagesLoader(BaseLoader):
    def load_data(self, image_url):
        """
        Loads images from the supplied directory/file and applies CLIP model transformation to represent these images
        in vector form

        :param image_url: The URL from which the images are to be loaded
        """
        # load model and image preprocessing
        from embedchain.models.clip_processor import ClipProcessor

        model = ClipProcessor.load_model()
        if os.path.isfile(image_url):
            data = [ClipProcessor.get_image_features(image_url, model)]
        else:
            data = []
            for filename in os.listdir(image_url):
                filepath = os.path.join(image_url, filename)
                try:
                    data.append(ClipProcessor.get_image_features(filepath, model))
                except Exception as e:
                    # Log the file that was not loaded
                    logging.exception("Failed to load the file {}. Exception {}".format(filepath, e))
        # Get the metadata like Size, Last Modified and Last Created timestamps
        image_path_metadata = [
            str(os.path.getsize(image_url)),
            str(os.path.getmtime(image_url)),
            str(os.path.getctime(image_url)),
        ]
        doc_id = hashlib.sha256((" ".join(image_path_metadata) + image_url).encode()).hexdigest()
        return {
            "doc_id": doc_id,
            "data": data,
        }



# Source: /content/embedchain/embedchain/loaders/youtube_video.py
import hashlib

try:
    from langchain.document_loaders import YoutubeLoader
except ImportError:
    raise ImportError(
        'YouTube video requires extra dependencies. Install with `pip install --upgrade "embedchain[dataloaders]"`'
    ) from None
from embedchain.helper.json_serializable import register_deserializable
from embedchain.loaders.base_loader import BaseLoader
from embedchain.utils import clean_string


@register_deserializable
class YoutubeVideoLoader(BaseLoader):
    def load_data(self, url):
        """Load data from a Youtube video."""
        loader = YoutubeLoader.from_youtube_url(url, add_video_info=True)
        doc = loader.load()
        output = []
        if not len(doc):
            raise ValueError("No data found")
        content = doc[0].page_content
        content = clean_string(content)
        meta_data = doc[0].metadata
        meta_data["url"] = url

        output.append(
            {
                "content": content,
                "meta_data": meta_data,
            }
        )
        doc_id = hashlib.sha256((content + url).encode()).hexdigest()
        return {
            "doc_id": doc_id,
            "data": output,
        }



# Source: /content/embedchain/embedchain/loaders/xml.py
import hashlib

try:
    from langchain.document_loaders import UnstructuredXMLLoader
except ImportError:
    raise ImportError(
        'XML file requires extra dependencies. Install with `pip install --upgrade "embedchain[dataloaders]"`'
    ) from None
from embedchain.helper.json_serializable import register_deserializable
from embedchain.loaders.base_loader import BaseLoader
from embedchain.utils import clean_string


@register_deserializable
class XmlLoader(BaseLoader):
    def load_data(self, xml_url):
        """Load data from a XML file."""
        loader = UnstructuredXMLLoader(xml_url)
        data = loader.load()
        content = data[0].page_content
        content = clean_string(content)
        meta_data = data[0].metadata
        meta_data["url"] = meta_data["source"]
        del meta_data["source"]
        output = [{"content": content, "meta_data": meta_data}]
        doc_id = hashlib.sha256((content + xml_url).encode()).hexdigest()
        return {
            "doc_id": doc_id,
            "data": output,
        }



# Source: /content/embedchain/embedchain/loaders/sitemap.py
import hashlib
import logging

import requests

try:
    from bs4 import BeautifulSoup
    from bs4.builder import ParserRejectedMarkup
except ImportError:
    raise ImportError(
        'Sitemap requires extra dependencies. Install with `pip install --upgrade "embedchain[dataloaders]"`'
    ) from None

from embedchain.helper.json_serializable import register_deserializable
from embedchain.loaders.base_loader import BaseLoader
from embedchain.loaders.web_page import WebPageLoader
from embedchain.utils import is_readable


@register_deserializable
class SitemapLoader(BaseLoader):
    def load_data(self, sitemap_url):
        """
        This method takes a sitemap URL as input and retrieves
        all the URLs to use the WebPageLoader to load content
        of each page.
        """
        output = []
        web_page_loader = WebPageLoader()
        response = requests.get(sitemap_url)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "xml")

        links = [link.text for link in soup.find_all("loc") if link.parent.name == "url"]
        if len(links) == 0:
            # Get all <loc> tags as a fallback. This might include images.
            links = [link.text for link in soup.find_all("loc")]

        doc_id = hashlib.sha256((" ".join(links) + sitemap_url).encode()).hexdigest()

        for link in links:
            try:
                each_load_data = web_page_loader.load_data(link)
                if is_readable(each_load_data.get("data")[0].get("content")):
                    output.append(each_load_data.get("data"))
                else:
                    logging.warning(f"Page is not readable (too many invalid characters): {link}")
            except ParserRejectedMarkup as e:
                logging.error(f"Failed to parse {link}: {e}")
        return {"doc_id": doc_id, "data": [data[0] for data in output]}



# Source: /content/embedchain/embedchain/loaders/unstructured_file.py
import hashlib

try:
    from langchain.document_loaders import UnstructuredFileLoader
except ImportError:
    raise ImportError(
        'PDF File requires extra dependencies. Install with `pip install --upgrade "embedchain[dataloaders]"`'
    ) from None
from embedchain.helper.json_serializable import register_deserializable
from embedchain.loaders.base_loader import BaseLoader
from embedchain.utils import clean_string


@register_deserializable
class UnstructuredLoader(BaseLoader):
    def load_data(self, url):
        """Load data from a Unstructured file."""
        loader = UnstructuredFileLoader(url)
        data = []
        all_content = []
        pages = loader.load_and_split()
        if not len(pages):
            raise ValueError("No data found")
        for page in pages:
            content = page.page_content
            content = clean_string(content)
            meta_data = page.metadata
            meta_data["url"] = url
            data.append(
                {
                    "content": content,
                    "meta_data": meta_data,
                }
            )
            all_content.append(content)
        doc_id = hashlib.sha256((" ".join(all_content) + url).encode()).hexdigest()
        return {
            "doc_id": doc_id,
            "data": data,
        }



# Source: /content/embedchain/embedchain/loaders/docs_site_loader.py
import hashlib
import logging
from urllib.parse import urljoin, urlparse

import requests

try:
    from bs4 import BeautifulSoup
except ImportError:
    raise ImportError(
        'DocsSite requires extra dependencies. Install with `pip install --upgrade "embedchain[dataloaders]"`'
    ) from None


from embedchain.helper.json_serializable import register_deserializable
from embedchain.loaders.base_loader import BaseLoader


@register_deserializable
class DocsSiteLoader(BaseLoader):
    def __init__(self):
        self.visited_links = set()

    def _get_child_links_recursive(self, url):
        parsed_url = urlparse(url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
        current_path = parsed_url.path

        response = requests.get(url)
        if response.status_code != 200:
            logging.info(f"Failed to fetch the website: {response.status_code}")
            return

        soup = BeautifulSoup(response.text, "html.parser")
        all_links = [link.get("href") for link in soup.find_all("a")]

        child_links = [link for link in all_links if link and link.startswith(current_path) and link != current_path]

        absolute_paths = [urljoin(base_url, link) for link in child_links]

        for link in absolute_paths:
            if link not in self.visited_links:
                self.visited_links.add(link)
                self._get_child_links_recursive(link)

    def _get_all_urls(self, url):
        self.visited_links = set()
        self._get_child_links_recursive(url)
        urls = [link for link in self.visited_links if urlparse(link).netloc == urlparse(url).netloc]
        return urls

    def _load_data_from_url(self, url):
        response = requests.get(url)
        if response.status_code != 200:
            logging.info(f"Failed to fetch the website: {response.status_code}")
            return []

        soup = BeautifulSoup(response.content, "html.parser")
        selectors = [
            "article.bd-article",
            'article[role="main"]',
            "div.md-content",
            'div[role="main"]',
            "div.container",
            "div.section",
            "article",
            "main",
        ]

        output = []
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                content = element.prettify()
                break
        else:
            content = soup.get_text()

        soup = BeautifulSoup(content, "html.parser")
        ignored_tags = [
            "nav",
            "aside",
            "form",
            "header",
            "noscript",
            "svg",
            "canvas",
            "footer",
            "script",
            "style",
        ]
        for tag in soup(ignored_tags):
            tag.decompose()

        content = " ".join(soup.stripped_strings)
        output.append(
            {
                "content": content,
                "meta_data": {"url": url},
            }
        )

        return output

    def load_data(self, url):
        all_urls = self._get_all_urls(url)
        output = []
        for u in all_urls:
            output.extend(self._load_data_from_url(u))
        doc_id = hashlib.sha256((" ".join(all_urls) + url).encode()).hexdigest()
        return {
            "doc_id": doc_id,
            "data": output,
        }



# Source: /content/embedchain/embedchain/loaders/local_text.py
import hashlib

from embedchain.helper.json_serializable import register_deserializable
from embedchain.loaders.base_loader import BaseLoader


@register_deserializable
class LocalTextLoader(BaseLoader):
    def load_data(self, content):
        """Load data from a local text file."""
        url = "local"
        meta_data = {
            "url": url,
        }
        doc_id = hashlib.sha256((content + url).encode()).hexdigest()
        return {
            "doc_id": doc_id,
            "data": [
                {
                    "content": content,
                    "meta_data": meta_data,
                }
            ],
        }



# Source: /content/embedchain/embedchain/loaders/local_qna_pair.py
import hashlib

from embedchain.helper.json_serializable import register_deserializable
from embedchain.loaders.base_loader import BaseLoader


@register_deserializable
class LocalQnaPairLoader(BaseLoader):
    def load_data(self, content):
        """Load data from a local QnA pair."""
        question, answer = content
        content = f"Q: {question}\nA: {answer}"
        url = "local"
        meta_data = {"url": url, "question": question}
        doc_id = hashlib.sha256((content + url).encode()).hexdigest()
        return {
            "doc_id": doc_id,
            "data": [
                {
                    "content": content,
                    "meta_data": meta_data,
                }
            ],
        }



# Source: /content/embedchain/embedchain/loaders/json.py
import hashlib

from langchain.document_loaders.json_loader import \
    JSONLoader as LangchainJSONLoader

from embedchain.loaders.base_loader import BaseLoader

langchain_json_jq_schema = 'to_entries | map("\(.key): \(.value|tostring)") | .[]'


class JSONLoader(BaseLoader):
    @staticmethod
    def load_data(content):
        """Load a json file. Each data point is a key value pair."""
        data = []
        data_content = []
        loader = LangchainJSONLoader(content, text_content=False, jq_schema=langchain_json_jq_schema)
        docs = loader.load()
        for doc in docs:
            meta_data = doc.metadata
            data.append({"content": doc.page_content, "meta_data": {"url": content, "row": meta_data["seq_num"]}})
            data_content.append(doc.page_content)
        doc_id = hashlib.sha256((content + ", ".join(data_content)).encode()).hexdigest()
        return {"doc_id": doc_id, "data": data}



# Source: /content/embedchain/embedchain/loaders/mdx.py
import hashlib

from embedchain.helper.json_serializable import register_deserializable
from embedchain.loaders.base_loader import BaseLoader


@register_deserializable
class MdxLoader(BaseLoader):
    def load_data(self, url):
        """Load data from a mdx file."""
        with open(url, "r", encoding="utf-8") as infile:
            content = infile.read()
        meta_data = {
            "url": url,
        }
        doc_id = hashlib.sha256((content + url).encode()).hexdigest()
        return {
            "doc_id": doc_id,
            "data": [
                {
                    "content": content,
                    "meta_data": meta_data,
                }
            ],
        }



# Source: /content/embedchain/embedchain/loaders/docx_file.py
import hashlib

try:
    from langchain.document_loaders import Docx2txtLoader
except ImportError:
    raise ImportError(
        'Docx file requires extra dependencies. Install with `pip install --upgrade "embedchain[dataloaders]"`'
    ) from None
from embedchain.helper.json_serializable import register_deserializable
from embedchain.loaders.base_loader import BaseLoader


@register_deserializable
class DocxFileLoader(BaseLoader):
    def load_data(self, url):
        """Load data from a .docx file."""
        loader = Docx2txtLoader(url)
        output = []
        data = loader.load()
        content = data[0].page_content
        meta_data = data[0].metadata
        meta_data["url"] = "local"
        output.append({"content": content, "meta_data": meta_data})
        doc_id = hashlib.sha256((content + url).encode()).hexdigest()
        return {
            "doc_id": doc_id,
            "data": output,
        }



# Source: /content/embedchain/embedchain/loaders/web_page.py
import hashlib
import logging

import requests

try:
    from bs4 import BeautifulSoup
except ImportError:
    raise ImportError(
        'Webpage requires extra dependencies. Install with `pip install --upgrade "embedchain[dataloaders]"`'
    ) from None

from embedchain.helper.json_serializable import register_deserializable
from embedchain.loaders.base_loader import BaseLoader
from embedchain.utils import clean_string


@register_deserializable
class WebPageLoader(BaseLoader):
    def load_data(self, url):
        """Load data from a web page."""
        response = requests.get(url)
        data = response.content
        content = self._get_clean_content(data, url)

        meta_data = {
            "url": url,
        }

        doc_id = hashlib.sha256((content + url).encode()).hexdigest()
        return {
            "doc_id": doc_id,
            "data": [
                {
                    "content": content,
                    "meta_data": meta_data,
                }
            ],
        }

    def _get_clean_content(self, html, url) -> str:
        soup = BeautifulSoup(html, "html.parser")
        original_size = len(str(soup.get_text()))

        tags_to_exclude = [
            "nav",
            "aside",
            "form",
            "header",
            "noscript",
            "svg",
            "canvas",
            "footer",
            "script",
            "style",
        ]
        for tag in soup(tags_to_exclude):
            tag.decompose()

        ids_to_exclude = ["sidebar", "main-navigation", "menu-main-menu"]
        for id in ids_to_exclude:
            tags = soup.find_all(id=id)
            for tag in tags:
                tag.decompose()

        classes_to_exclude = [
            "elementor-location-header",
            "navbar-header",
            "nav",
            "header-sidebar-wrapper",
            "blog-sidebar-wrapper",
            "related-posts",
        ]
        for class_name in classes_to_exclude:
            tags = soup.find_all(class_=class_name)
            for tag in tags:
                tag.decompose()

        content = soup.get_text()
        content = clean_string(content)

        cleaned_size = len(content)
        if original_size != 0:
            logging.info(
                f"[{url}] Cleaned page size: {cleaned_size} characters, down from {original_size} (shrunk: {original_size-cleaned_size} chars, {round((1-(cleaned_size/original_size)) * 100, 2)}%)"  # noqa:E501
            )

        return content



# Source: /content/embedchain/embedchain/loaders/base_loader.py
from embedchain.helper.json_serializable import JSONSerializable


class BaseLoader(JSONSerializable):
    def __init__(self):
        pass

    def load_data():
        """
        Implemented by child classes
        """
        pass



# Source: /content/embedchain/embedchain/apps/__init__.py



# Source: /content/embedchain/embedchain/apps/app.py
from typing import Optional

import yaml

from embedchain.config import AppConfig, BaseEmbedderConfig, BaseLlmConfig
from embedchain.config.vectordb.base import BaseVectorDbConfig
from embedchain.embedchain import EmbedChain
from embedchain.embedder.base import BaseEmbedder
from embedchain.embedder.openai import OpenAIEmbedder
from embedchain.factory import EmbedderFactory, LlmFactory, VectorDBFactory
from embedchain.helper.json_serializable import register_deserializable
from embedchain.llm.base import BaseLlm
from embedchain.llm.openai import OpenAILlm
from embedchain.vectordb.base import BaseVectorDB
from embedchain.vectordb.chroma import ChromaDB


@register_deserializable
class App(EmbedChain):
    """
    The EmbedChain app in it's simplest and most straightforward form.
    An opinionated choice of LLM, vector database and embedding model.

    Methods:
    add(source, data_type): adds the data from the given URL to the vector db.
    query(query): finds answer to the given query using vector database and LLM.
    chat(query): finds answer to the given query using vector database and LLM, with conversation history.
    """

    def __init__(
        self,
        config: Optional[AppConfig] = None,
        llm: BaseLlm = None,
        llm_config: Optional[BaseLlmConfig] = None,
        db: BaseVectorDB = None,
        db_config: Optional[BaseVectorDbConfig] = None,
        embedder: BaseEmbedder = None,
        embedder_config: Optional[BaseEmbedderConfig] = None,
        system_prompt: Optional[str] = None,
    ):
        """
        Initialize a new `App` instance.

        :param config: Config for the app instance., defaults to None
        :type config: Optional[AppConfig], optional
        :param llm:  LLM Class instance. example: `from embedchain.llm.openai import OpenAILlm`, defaults to OpenAiLlm
        :type llm: BaseLlm, optional
        :param llm_config: Allows you to configure the LLM, e.g. how many documents to return,
        example: `from embedchain.config import BaseLlmConfig`, defaults to None
        :type llm_config: Optional[BaseLlmConfig], optional
        :param db: The database to use for storing and retrieving embeddings,
        example: `from embedchain.vectordb.chroma_db import ChromaDb`, defaults to ChromaDb
        :type db: BaseVectorDB, optional
        :param db_config: Allows you to configure the vector database,
        example: `from embedchain.config import ChromaDbConfig`, defaults to None
        :type db_config: Optional[BaseVectorDbConfig], optional
        :param embedder: The embedder (embedding model and function) use to calculate embeddings.
        example: `from embedchain.embedder.gpt4all_embedder import GPT4AllEmbedder`, defaults to OpenAIEmbedder
        :type embedder: BaseEmbedder, optional
        :param embedder_config: Allows you to configure the Embedder.
        example: `from embedchain.config import BaseEmbedderConfig`, defaults to None
        :type embedder_config: Optional[BaseEmbedderConfig], optional
        :param system_prompt: System prompt that will be provided to the LLM as such, defaults to None
        :type system_prompt: Optional[str], optional
        :raises TypeError: LLM, database or embedder or their config is not a valid class instance.
        """
        # Type check configs
        if config and not isinstance(config, AppConfig):
            raise TypeError(
                "Config is not a `AppConfig` instance. "
                "Please make sure the type is right and that you are passing an instance."
            )
        if llm_config and not isinstance(llm_config, BaseLlmConfig):
            raise TypeError(
                "`llm_config` is not a `BaseLlmConfig` instance. "
                "Please make sure the type is right and that you are passing an instance."
            )
        if db_config and not isinstance(db_config, BaseVectorDbConfig):
            raise TypeError(
                "`db_config` is not a `BaseVectorDbConfig` instance. "
                "Please make sure the type is right and that you are passing an instance."
            )
        if embedder_config and not isinstance(embedder_config, BaseEmbedderConfig):
            raise TypeError(
                "`embedder_config` is not a `BaseEmbedderConfig` instance. "
                "Please make sure the type is right and that you are passing an instance."
            )

        # Assign defaults
        if config is None:
            config = AppConfig()
        if llm is None:
            llm = OpenAILlm(config=llm_config)
        if db is None:
            db = ChromaDB(config=db_config)
        if embedder is None:
            embedder = OpenAIEmbedder(config=embedder_config)

        # Type check assignments
        if not isinstance(llm, BaseLlm):
            raise TypeError(
                "LLM is not a `BaseLlm` instance. "
                "Please make sure the type is right and that you are passing an instance."
            )
        if not isinstance(db, BaseVectorDB):
            raise TypeError(
                "Database is not a `BaseVectorDB` instance. "
                "Please make sure the type is right and that you are passing an instance."
            )
        if not isinstance(embedder, BaseEmbedder):
            raise TypeError(
                "Embedder is not a `BaseEmbedder` instance. "
                "Please make sure the type is right and that you are passing an instance."
            )
        super().__init__(config, llm=llm, db=db, embedder=embedder, system_prompt=system_prompt)

    @classmethod
    def from_config(cls, yaml_path: str):
        """
        Instantiate an App object from a YAML configuration file.

        :param yaml_path: Path to the YAML configuration file.
        :type yaml_path: str
        :return: An instance of the App class.
        :rtype: App
        """
        with open(yaml_path, "r") as file:
            config_data = yaml.safe_load(file)

        app_config_data = config_data.get("app", {})
        llm_config_data = config_data.get("llm", {})
        db_config_data = config_data.get("vectordb", {})
        embedder_config_data = config_data.get("embedder", {})

        app_config = AppConfig(**app_config_data.get("config", {}))

        llm_provider = llm_config_data.get("provider", "openai")
        llm = LlmFactory.create(llm_provider, llm_config_data.get("config", {}))

        db_provider = db_config_data.get("provider", "chroma")
        db = VectorDBFactory.create(db_provider, db_config_data.get("config", {}))

        embedder_provider = embedder_config_data.get("provider", "openai")
        embedder = EmbedderFactory.create(embedder_provider, embedder_config_data.get("config", {}))
        return cls(config=app_config, llm=llm, db=db, embedder=embedder)



# Source: /content/embedchain/embedchain/data_formatter/data_formatter.py
from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.chunkers.docs_site import DocsSiteChunker
from embedchain.chunkers.docx_file import DocxFileChunker
from embedchain.chunkers.gmail import GmailChunker
from embedchain.chunkers.images import ImagesChunker
from embedchain.chunkers.json import JSONChunker
from embedchain.chunkers.mdx import MdxChunker
from embedchain.chunkers.notion import NotionChunker
from embedchain.chunkers.openapi import OpenAPIChunker
from embedchain.chunkers.pdf_file import PdfFileChunker
from embedchain.chunkers.qna_pair import QnaPairChunker
from embedchain.chunkers.sitemap import SitemapChunker
from embedchain.chunkers.table import TableChunker
from embedchain.chunkers.text import TextChunker
from embedchain.chunkers.unstructured_file import UnstructuredFileChunker
from embedchain.chunkers.web_page import WebPageChunker
from embedchain.chunkers.xml import XmlChunker
from embedchain.chunkers.youtube_video import YoutubeVideoChunker
from embedchain.config import AddConfig
from embedchain.config.add_config import ChunkerConfig, LoaderConfig
from embedchain.helper.json_serializable import JSONSerializable
from embedchain.loaders.base_loader import BaseLoader
from embedchain.loaders.csv import CsvLoader
from embedchain.loaders.docs_site_loader import DocsSiteLoader
from embedchain.loaders.docx_file import DocxFileLoader
from embedchain.loaders.gmail import GmailLoader
from embedchain.loaders.images import ImagesLoader
from embedchain.loaders.json import JSONLoader
from embedchain.loaders.local_qna_pair import LocalQnaPairLoader
from embedchain.loaders.local_text import LocalTextLoader
from embedchain.loaders.mdx import MdxLoader
from embedchain.loaders.openapi import OpenAPILoader
from embedchain.loaders.pdf_file import PdfFileLoader
from embedchain.loaders.sitemap import SitemapLoader
from embedchain.loaders.unstructured_file import UnstructuredLoader
from embedchain.loaders.web_page import WebPageLoader
from embedchain.loaders.xml import XmlLoader
from embedchain.loaders.youtube_video import YoutubeVideoLoader
from embedchain.models.data_type import DataType


class DataFormatter(JSONSerializable):
    """
    DataFormatter is an internal utility class which abstracts the mapping for
    loaders and chunkers to the data_type entered by the user in their
    .add or .add_local method call
    """

    def __init__(self, data_type: DataType, config: AddConfig):
        """
        Initialize a dataformatter, set data type and chunker based on datatype.

        :param data_type: The type of the data to load and chunk.
        :type data_type: DataType
        :param config: AddConfig instance with nested loader and chunker config attributes.
        :type config: AddConfig
        """
        self.loader = self._get_loader(data_type=data_type, config=config.loader)
        self.chunker = self._get_chunker(data_type=data_type, config=config.chunker)

    def _get_loader(self, data_type: DataType, config: LoaderConfig) -> BaseLoader:
        """
        Returns the appropriate data loader for the given data type.

        :param data_type: The type of the data to load.
        :type data_type: DataType
        :param config: Config to initialize the loader with.
        :type config: LoaderConfig
        :raises ValueError: If an unsupported data type is provided.
        :return: The loader for the given data type.
        :rtype: BaseLoader
        """
        loaders = {
            DataType.YOUTUBE_VIDEO: YoutubeVideoLoader,
            DataType.PDF_FILE: PdfFileLoader,
            DataType.WEB_PAGE: WebPageLoader,
            DataType.QNA_PAIR: LocalQnaPairLoader,
            DataType.TEXT: LocalTextLoader,
            DataType.DOCX: DocxFileLoader,
            DataType.SITEMAP: SitemapLoader,
            DataType.XML: XmlLoader,
            DataType.DOCS_SITE: DocsSiteLoader,
            DataType.CSV: CsvLoader,
            DataType.MDX: MdxLoader,
            DataType.IMAGES: ImagesLoader,
            DataType.UNSTRUCTURED: UnstructuredLoader,
            DataType.JSON: JSONLoader,
            DataType.OPENAPI: OpenAPILoader,
            DataType.GMAIL: GmailLoader,
        }
        lazy_loaders = {DataType.NOTION}
        if data_type in loaders:
            loader_class: type = loaders[data_type]
            loader: BaseLoader = loader_class()
            return loader
        elif data_type in lazy_loaders:
            if data_type == DataType.NOTION:
                from embedchain.loaders.notion import NotionLoader

                return NotionLoader()
            else:
                raise ValueError(f"Unsupported data type: {data_type}")
        else:
            raise ValueError(f"Unsupported data type: {data_type}")

    def _get_chunker(self, data_type: DataType, config: ChunkerConfig) -> BaseChunker:
        """Returns the appropriate chunker for the given data type.

        :param data_type: The type of the data to chunk.
        :type data_type: DataType
        :param config: Config to initialize the chunker with.
        :type config: ChunkerConfig
        :raises ValueError: If an unsupported data type is provided.
        :return: The chunker for the given data type.
        :rtype: BaseChunker
        """
        chunker_classes = {
            DataType.YOUTUBE_VIDEO: YoutubeVideoChunker,
            DataType.PDF_FILE: PdfFileChunker,
            DataType.WEB_PAGE: WebPageChunker,
            DataType.QNA_PAIR: QnaPairChunker,
            DataType.TEXT: TextChunker,
            DataType.DOCX: DocxFileChunker,
            DataType.DOCS_SITE: DocsSiteChunker,
            DataType.SITEMAP: SitemapChunker,
            DataType.NOTION: NotionChunker,
            DataType.CSV: TableChunker,
            DataType.MDX: MdxChunker,
            DataType.IMAGES: ImagesChunker,
            DataType.XML: XmlChunker,
            DataType.UNSTRUCTURED: UnstructuredFileChunker,
            DataType.JSON: JSONChunker,
            DataType.OPENAPI: OpenAPIChunker,
            DataType.GMAIL: GmailChunker,
        }
        if data_type in chunker_classes:
            chunker_class: type = chunker_classes[data_type]
            chunker: BaseChunker = chunker_class(config)
            chunker.set_data_type(data_type)
            return chunker
        else:
            raise ValueError(f"Unsupported data type: {data_type}")



# Source: /content/embedchain/embedchain/data_formatter/__init__.py
from .data_formatter import DataFormatter  # noqa: F401



# Source: /content/embedchain/embedchain/chunkers/notion.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class NotionChunker(BaseChunker):
    """Chunker for notion."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=300, chunk_overlap=0, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/chunkers/gmail.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class GmailChunker(BaseChunker):
    """Chunker for gmail."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=1000, chunk_overlap=0, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/chunkers/openapi.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig


class OpenAPIChunker(BaseChunker):
    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=1000, chunk_overlap=0, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/chunkers/docs_site.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class DocsSiteChunker(BaseChunker):
    """Chunker for code docs site."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=500, chunk_overlap=50, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/chunkers/pdf_file.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class PdfFileChunker(BaseChunker):
    """Chunker for PDF file."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=1000, chunk_overlap=0, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/chunkers/__init__.py



# Source: /content/embedchain/embedchain/chunkers/images.py
import hashlib
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig


class ImagesChunker(BaseChunker):
    """Chunker for an Image."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=300, chunk_overlap=0, length_function=len)
        image_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(image_splitter)

    def create_chunks(self, loader, src, app_id=None):
        """
        Loads the image(s), and creates their corresponding embedding. This creates one chunk for each image

        :param loader: The loader whose `load_data` method is used to create
        the raw data.
        :param src: The data to be handled by the loader. Can be a URL for
        remote sources or local content for local loaders.
        """
        documents = []
        embeddings = []
        ids = []
        data_result = loader.load_data(src)
        data_records = data_result["data"]
        doc_id = data_result["doc_id"]
        doc_id = f"{app_id}--{doc_id}" if app_id is not None else doc_id
        metadatas = []
        for data in data_records:
            meta_data = data["meta_data"]
            # add data type to meta data to allow query using data type
            meta_data["data_type"] = self.data_type.value
            chunk_id = hashlib.sha256(meta_data["url"].encode()).hexdigest()
            ids.append(chunk_id)
            documents.append(data["content"])
            embeddings.append(data["embedding"])
            meta_data["doc_id"] = doc_id
            metadatas.append(meta_data)

        return {
            "documents": documents,
            "embeddings": embeddings,
            "ids": ids,
            "metadatas": metadatas,
            "doc_id": doc_id,
        }

    def get_word_count(self, documents):
        """
        The number of chunks and the corresponding word count for an image is fixed to 1, as 1 embedding is created for
        each image
        """
        return 1



# Source: /content/embedchain/embedchain/chunkers/youtube_video.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class YoutubeVideoChunker(BaseChunker):
    """Chunker for Youtube video."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=2000, chunk_overlap=0, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/chunkers/xml.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class XmlChunker(BaseChunker):
    """Chunker for XML files."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=500, chunk_overlap=50, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/chunkers/sitemap.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class SitemapChunker(BaseChunker):
    """Chunker for sitemap."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=500, chunk_overlap=0, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/chunkers/unstructured_file.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class UnstructuredFileChunker(BaseChunker):
    """Chunker for Unstructured file."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=1000, chunk_overlap=0, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/chunkers/base_chunker.py
import hashlib

from embedchain.helper.json_serializable import JSONSerializable
from embedchain.models.data_type import DataType


class BaseChunker(JSONSerializable):
    def __init__(self, text_splitter):
        """Initialize the chunker."""
        self.text_splitter = text_splitter
        self.data_type = None

    def create_chunks(self, loader, src, app_id=None):
        """
        Loads data and chunks it.

        :param loader: The loader which's `load_data` method is used to create
        the raw data.
        :param src: The data to be handled by the loader. Can be a URL for
        remote sources or local content for local loaders.
        :param app_id: App id used to generate the doc_id.
        """
        documents = []
        chunk_ids = []
        idMap = {}
        data_result = loader.load_data(src)
        data_records = data_result["data"]
        doc_id = data_result["doc_id"]
        # Prefix app_id in the document id if app_id is not None to
        # distinguish between different documents stored in the same
        # elasticsearch or opensearch index
        doc_id = f"{app_id}--{doc_id}" if app_id is not None else doc_id
        metadatas = []
        for data in data_records:
            content = data["content"]

            meta_data = data["meta_data"]
            # add data type to meta data to allow query using data type
            meta_data["data_type"] = self.data_type.value
            meta_data["doc_id"] = doc_id
            url = meta_data["url"]

            chunks = self.get_chunks(content)

            for chunk in chunks:
                chunk_id = hashlib.sha256((chunk + url).encode()).hexdigest()
                chunk_id = f"{app_id}--{chunk_id}" if app_id is not None else chunk_id
                if idMap.get(chunk_id) is None:
                    idMap[chunk_id] = True
                    chunk_ids.append(chunk_id)
                    documents.append(chunk)
                    metadatas.append(meta_data)
        return {
            "documents": documents,
            "ids": chunk_ids,
            "metadatas": metadatas,
            "doc_id": doc_id,
        }

    def get_chunks(self, content):
        """
        Returns chunks using text splitter instance.

        Override in child class if custom logic.
        """
        return self.text_splitter.split_text(content)

    def set_data_type(self, data_type: DataType):
        """
        set the data type of chunker
        """
        self.data_type = data_type

        # TODO: This should be done during initialization. This means it has to be done in the child classes.

    def get_word_count(self, documents):
        return sum([len(document.split(" ")) for document in documents])



# Source: /content/embedchain/embedchain/chunkers/qna_pair.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class QnaPairChunker(BaseChunker):
    """Chunker for QnA pair."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=300, chunk_overlap=0, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/chunkers/text.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class TextChunker(BaseChunker):
    """Chunker for text."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=300, chunk_overlap=0, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/chunkers/json.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class JSONChunker(BaseChunker):
    """Chunker for json."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=1000, chunk_overlap=0, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/chunkers/mdx.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class MdxChunker(BaseChunker):
    """Chunker for mdx files."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=1000, chunk_overlap=0, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/chunkers/table.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig


class TableChunker(BaseChunker):
    """Chunker for tables, for instance csv, google sheets or databases."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=300, chunk_overlap=0, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/chunkers/docx_file.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class DocxFileChunker(BaseChunker):
    """Chunker for .docx file."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=1000, chunk_overlap=0, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/chunkers/web_page.py
from typing import Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.config.add_config import ChunkerConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class WebPageChunker(BaseChunker):
    """Chunker for web page."""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        if config is None:
            config = ChunkerConfig(chunk_size=500, chunk_overlap=0, length_function=len)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=config.length_function,
        )
        super().__init__(text_splitter)



# Source: /content/embedchain/embedchain/vectordb/base.py
from embedchain.config.vectordb.base import BaseVectorDbConfig
from embedchain.embedder.base import BaseEmbedder
from embedchain.helper.json_serializable import JSONSerializable


class BaseVectorDB(JSONSerializable):
    """Base class for vector database."""

    def __init__(self, config: BaseVectorDbConfig):
        """Initialize the database. Save the config and client as an attribute.

        :param config: Database configuration class instance.
        :type config: BaseVectorDbConfig
        """
        self.client = self._get_or_create_db()
        self.config: BaseVectorDbConfig = config

    def _initialize(self):
        """
        This method is needed because `embedder` attribute needs to be set externally before it can be initialized.

        So it's can't be done in __init__ in one step.
        """
        raise NotImplementedError

    def _get_or_create_db(self):
        """Get or create the database."""
        raise NotImplementedError

    def _get_or_create_collection(self):
        """Get or create a named collection."""
        raise NotImplementedError

    def _set_embedder(self, embedder: BaseEmbedder):
        """
        The database needs to access the embedder sometimes, with this method you can persistently set it.

        :param embedder: Embedder to be set as the embedder for this database.
        :type embedder: BaseEmbedder
        """
        self.embedder = embedder

    def get(self):
        """Get database embeddings by id."""
        raise NotImplementedError

    def add(self):
        """Add to database"""
        raise NotImplementedError

    def query(self):
        """Query contents from vector data base based on vector similarity"""
        raise NotImplementedError

    def count(self) -> int:
        """
        Count number of documents/chunks embedded in the database.

        :return: number of documents
        :rtype: int
        """
        raise NotImplementedError

    def reset(self):
        """
        Resets the database. Deletes all embeddings irreversibly.
        """
        raise NotImplementedError

    def set_collection_name(self, name: str):
        """
        Set the name of the collection. A collection is an isolated space for vectors.

        :param name: Name of the collection.
        :type name: str
        """
        raise NotImplementedError



# Source: /content/embedchain/embedchain/vectordb/opensearch.py
import logging
from typing import Dict, List, Optional, Set, Tuple

try:
    from opensearchpy import OpenSearch
    from opensearchpy.helpers import bulk
except ImportError:
    raise ImportError(
        "OpenSearch requires extra dependencies. Install with `pip install --upgrade embedchain[opensearch]`"
    ) from None

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import OpenSearchVectorSearch

from embedchain.config import OpenSearchDBConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.vectordb.base import BaseVectorDB


@register_deserializable
class OpenSearchDB(BaseVectorDB):
    """
    OpenSearch as vector database
    """

    def __init__(self, config: OpenSearchDBConfig):
        """OpenSearch as vector database.

        :param config: OpenSearch domain config
        :type config: OpenSearchDBConfig
        """
        if config is None:
            raise ValueError("OpenSearchDBConfig is required")
        self.config = config
        self.client = OpenSearch(
            hosts=[self.config.opensearch_url],
            http_auth=self.config.http_auth,
            **self.config.extra_params,
        )
        info = self.client.info()
        logging.info(f"Connected to {info['version']['distribution']}. Version: {info['version']['number']}")
        # Remove auth credentials from config after successful connection
        super().__init__(config=self.config)

    def _initialize(self):
        logging.info(self.client.info())
        index_name = self._get_index()
        if self.client.indices.exists(index=index_name):
            print(f"Index '{index_name}' already exists.")
            return

        index_body = {
            "settings": {"knn": True},
            "mappings": {
                "properties": {
                    "text": {"type": "text"},
                    "embeddings": {
                        "type": "knn_vector",
                        "index": False,
                        "dimension": self.config.vector_dimension,
                    },
                }
            },
        }
        self.client.indices.create(index_name, body=index_body)
        print(self.client.indices.get(index_name))

    def _get_or_create_db(self):
        """Called during initialization"""
        return self.client

    def _get_or_create_collection(self, name):
        """Note: nothing to return here. Discuss later"""

    def get(
        self, ids: Optional[List[str]] = None, where: Optional[Dict[str, any]] = None, limit: Optional[int] = None
    ) -> Set[str]:
        """
        Get existing doc ids present in vector database

        :param ids: _list of doc ids to check for existence
        :type ids: List[str]
        :param where: to filter data
        :type where: Dict[str, any]
        :return: ids
        :type: Set[str]
        """
        query = {}
        if ids:
            query["query"] = {"bool": {"must": [{"ids": {"values": ids}}]}}
        else:
            query["query"] = {"bool": {"must": []}}

        if "app_id" in where:
            app_id = where["app_id"]
            query["query"]["bool"]["must"].append({"term": {"metadata.app_id.keyword": app_id}})

        # OpenSearch syntax is different from Elasticsearch
        response = self.client.search(index=self._get_index(), body=query, _source=True, size=limit)
        docs = response["hits"]["hits"]
        ids = [doc["_id"] for doc in docs]
        doc_ids = [doc["_source"]["metadata"]["doc_id"] for doc in docs]

        # Result is modified for compatibility with other vector databases
        # TODO: Add method in vector database to return result in a standard format
        result = {"ids": ids, "metadatas": []}

        for doc_id in doc_ids:
            result["metadatas"].append({"doc_id": doc_id})
        return result

    def add(
        self,
        embeddings: List[List[str]],
        documents: List[str],
        metadatas: List[object],
        ids: List[str],
        skip_embedding: bool,
    ):
        """add data in vector database

        :param embeddings: list of embeddings to add
        :type embeddings: List[List[str]]
        :param documents: list of texts to add
        :type documents: List[str]
        :param metadatas: list of metadata associated with docs
        :type metadatas: List[object]
        :param ids: ids of docs
        :type ids: List[str]
        :param skip_embedding: Optional. If True, then the embeddings are assumed to be already generated.
        :type skip_embedding: bool
        """

        docs = []
        if not skip_embedding:
            embeddings = self.embedder.embedding_fn(documents)
        for id, text, metadata, embeddings in zip(ids, documents, metadatas, embeddings):
            docs.append(
                {
                    "_index": self._get_index(),
                    "_id": id,
                    "_source": {"text": text, "metadata": metadata, "embeddings": embeddings},
                }
            )
        bulk(self.client, docs)
        self.client.indices.refresh(index=self._get_index())

    def query(
        self, input_query: List[str], n_results: int, where: Dict[str, any], skip_embedding: bool
    ) -> List[Tuple[str, str, str]]:
        """
        query contents from vector data base based on vector similarity

        :param input_query: list of query string
        :type input_query: List[str]
        :param n_results: no of similar documents to fetch from database
        :type n_results: int
        :param where: Optional. to filter data
        :type where: Dict[str, any]
        :param skip_embedding: Optional. If True, then the input_query is assumed to be already embedded.
        :type skip_embedding: bool
        :return: The content of the document that matched your query, url of the source, doc_id
        :rtype: List[Tuple[str,str,str]]
        """
        # TODO(rupeshbansal, deshraj): Add support for skip embeddings here if already exists
        embeddings = OpenAIEmbeddings()
        docsearch = OpenSearchVectorSearch(
            index_name=self._get_index(),
            embedding_function=embeddings,
            opensearch_url=f"{self.config.opensearch_url}",
            http_auth=self.config.http_auth,
            use_ssl=hasattr(self.config, "use_ssl") and self.config.use_ssl,
            verify_certs=hasattr(self.config, "verify_certs") and self.config.verify_certs,
        )

        pre_filter = {"match_all": {}}  # default
        if "app_id" in where:
            app_id = where["app_id"]
            pre_filter = {"bool": {"must": [{"term": {"metadata.app_id.keyword": app_id}}]}}
        docs = docsearch.similarity_search(
            input_query,
            search_type="script_scoring",
            space_type="cosinesimil",
            vector_field="embeddings",
            text_field="text",
            metadata_field="metadata",
            pre_filter=pre_filter,
            k=n_results,
        )

        contents = []
        for doc in docs:
            context = doc.page_content
            source = doc.metadata["url"]
            doc_id = doc.metadata["doc_id"]
            contents.append(tuple((context, source, doc_id)))
        return contents

    def set_collection_name(self, name: str):
        """
        Set the name of the collection. A collection is an isolated space for vectors.

        :param name: Name of the collection.
        :type name: str
        """
        if not isinstance(name, str):
            raise TypeError("Collection name must be a string")
        self.config.collection_name = name

    def count(self) -> int:
        """
        Count number of documents/chunks embedded in the database.

        :return: number of documents
        :rtype: int
        """
        query = {"query": {"match_all": {}}}
        response = self.client.count(index=self._get_index(), body=query)
        doc_count = response["count"]
        return doc_count

    def reset(self):
        """
        Resets the database. Deletes all embeddings irreversibly.
        """
        # Delete all data from the database
        if self.client.indices.exists(index=self._get_index()):
            # delete index in Es
            self.client.indices.delete(index=self._get_index())

    def delete(self, where):
        """Deletes a document from the OpenSearch index"""
        if "doc_id" not in where:
            raise ValueError("doc_id is required to delete a document")

        query = {"query": {"bool": {"must": [{"term": {"metadata.doc_id": where["doc_id"]}}]}}}
        self.client.delete_by_query(index=self._get_index(), body=query)

    def _get_index(self) -> str:
        """Get the OpenSearch index for a collection

        :return: OpenSearch index
        :rtype: str
        """
        return self.config.collection_name



# Source: /content/embedchain/embedchain/vectordb/pinecone.py
import os
from typing import Dict, List, Optional, Tuple

try:
    import pinecone
except ImportError:
    raise ImportError(
        "Pinecone requires extra dependencies. Install with `pip install --upgrade 'embedchain[pinecone]'`"
    ) from None

from embedchain.config.vectordb.pinecone import PineconeDBConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.vectordb.base import BaseVectorDB


@register_deserializable
class PineconeDB(BaseVectorDB):
    """
    Pinecone as vector database
    """

    BATCH_SIZE = 100

    def __init__(
        self,
        config: Optional[PineconeDBConfig] = None,
    ):
        """Pinecone as vector database.

        :param config: Pinecone database config, defaults to None
        :type config: PineconeDBConfig, optional
        :raises ValueError: No config provided
        """
        if config is None:
            self.config = PineconeDBConfig()
        else:
            if not isinstance(config, PineconeDBConfig):
                raise TypeError(
                    "config is not a `PineconeDBConfig` instance. "
                    "Please make sure the type is right and that you are passing an instance."
                )
            self.config = config
        self.client = self._setup_pinecone_index()
        # Call parent init here because embedder is needed
        super().__init__(config=self.config)

    def _initialize(self):
        """
        This method is needed because `embedder` attribute needs to be set externally before it can be initialized.
        """
        if not self.embedder:
            raise ValueError("Embedder not set. Please set an embedder with `set_embedder` before initialization.")

    # Loads the Pinecone index or creates it if not present.
    def _setup_pinecone_index(self):
        pinecone.init(
            api_key=os.environ.get("PINECONE_API_KEY"),
            environment=os.environ.get("PINECONE_ENV"),
            **self.config.extra_params,
        )
        self.index_name = self._get_index_name()
        indexes = pinecone.list_indexes()
        if indexes is None or self.index_name not in indexes:
            pinecone.create_index(
                name=self.index_name, metric=self.config.metric, dimension=self.config.vector_dimension
            )
        return pinecone.Index(self.index_name)

    def get(self, ids: Optional[List[str]] = None, where: Optional[Dict[str, any]] = None, limit: Optional[int] = None):
        """
        Get existing doc ids present in vector database

        :param ids: _list of doc ids to check for existence
        :type ids: List[str]
        :param where: to filter data
        :type where: Dict[str, any]
        :return: ids
        :rtype: Set[str]
        """
        existing_ids = list()
        if ids is not None:
            for i in range(0, len(ids), 1000):
                result = self.client.fetch(ids=ids[i : i + 1000])
                batch_existing_ids = list(result.get("vectors").keys())
                existing_ids.extend(batch_existing_ids)
        return {"ids": existing_ids}

    def add(
        self,
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[object],
        ids: List[str],
        skip_embedding: bool,
    ):
        """add data in vector database

        :param documents: list of texts to add
        :type documents: List[str]
        :param metadatas: list of metadata associated with docs
        :type metadatas: List[object]
        :param ids: ids of docs
        :type ids: List[str]
        """
        docs = []
        print("Adding documents to Pinecone...")

        embeddings = self.embedder.embedding_fn(documents)
        for id, text, metadata, embedding in zip(ids, documents, metadatas, embeddings):
            docs.append(
                {
                    "id": id,
                    "values": embedding,
                    "metadata": {**metadata, "text": text},
                }
            )

        for i in range(0, len(docs), self.BATCH_SIZE):
            self.client.upsert(docs[i : i + self.BATCH_SIZE])

    def query(
        self, input_query: List[str], n_results: int, where: Dict[str, any], skip_embedding: bool
    ) -> List[Tuple[str, str, str]]:
        """
        query contents from vector database based on vector similarity
        :param input_query: list of query string
        :type input_query: List[str]
        :param n_results: no of similar documents to fetch from database
        :type n_results: int
        :param where: Optional. to filter data
        :type where: Dict[str, any]
        :param skip_embedding: Optional. if True, input_query is already embedded
        :type skip_embedding: bool
        :return: The content of the document that matched your query, url of the source, doc_id
        :rtype: List[Tuple[str,str,str]]
        """
        if not skip_embedding:
            query_vector = self.embedder.embedding_fn([input_query])[0]
        else:
            query_vector = input_query
        data = self.client.query(vector=query_vector, filter=where, top_k=n_results, include_metadata=True)
        contents = []
        for doc in data["matches"]:
            metadata = doc["metadata"]
            context = metadata["text"]
            source = metadata["url"]
            doc_id = metadata["doc_id"]
            contents.append(tuple((context, source, doc_id)))
        return contents

    def set_collection_name(self, name: str):
        """
        Set the name of the collection. A collection is an isolated space for vectors.

        :param name: Name of the collection.
        :type name: str
        """
        if not isinstance(name, str):
            raise TypeError("Collection name must be a string")
        self.config.collection_name = name

    def count(self) -> int:
        """
        Count number of documents/chunks embedded in the database.

        :return: number of documents
        :rtype: int
        """
        return self.client.describe_index_stats()["total_vector_count"]

    def _get_or_create_db(self):
        """Called during initialization"""
        return self.client

    def reset(self):
        """
        Resets the database. Deletes all embeddings irreversibly.
        """
        # Delete all data from the database
        pinecone.delete_index(self.index_name)
        self._setup_pinecone_index()

    # Pinecone only allows alphanumeric characters and "-" in the index name
    def _get_index_name(self) -> str:
        """Get the Pinecone index for a collection

        :return: Pinecone index
        :rtype: str
        """
        return f"{self.config.collection_name}-{self.config.vector_dimension}".lower().replace("_", "-")



# Source: /content/embedchain/embedchain/vectordb/__init__.py



# Source: /content/embedchain/embedchain/vectordb/qdrant.py
import copy
import os
import uuid
from typing import Dict, List, Optional, Tuple

try:
    from qdrant_client import QdrantClient
    from qdrant_client.http import models
    from qdrant_client.http.models import Batch
    from qdrant_client.models import Distance, VectorParams
except ImportError:
    raise ImportError("Qdrant requires extra dependencies. Install with `pip install embedchain[qdrant]`") from None

from embedchain.config.vectordb.qdrant import QdrantDBConfig
from embedchain.vectordb.base import BaseVectorDB


class QdrantDB(BaseVectorDB):
    """
    Qdrant as vector database
    """

    BATCH_SIZE = 10

    def __init__(self, config: QdrantDBConfig = None):
        """
        Qdrant as vector database
        :param config. Qdrant database config to be used for connection
        """
        if config is None:
            config = QdrantDBConfig()
        else:
            if not isinstance(config, QdrantDBConfig):
                raise TypeError(
                    "config is not a `QdrantDBConfig` instance. "
                    "Please make sure the type is right and that you are passing an instance."
                )
        self.config = config
        self.client = QdrantClient(url=os.getenv("QDRANT_URL"), api_key=os.getenv("QDRANT_API_KEY"))
        # Call parent init here because embedder is needed
        super().__init__(config=self.config)

    def _initialize(self):
        """
        This method is needed because `embedder` attribute needs to be set externally before it can be initialized.
        """
        if not self.embedder:
            raise ValueError("Embedder not set. Please set an embedder with `set_embedder` before initialization.")

        self.collection_name = self._get_or_create_collection()
        self.metadata_keys = {"data_type", "doc_id", "url", "hash", "app_id", "text"}
        all_collections = self.client.get_collections()
        collection_names = [collection.name for collection in all_collections.collections]
        if self.collection_name not in collection_names:
            self.client.recreate_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=self.embedder.vector_dimension,
                    distance=Distance.COSINE,
                    hnsw_config=self.config.hnsw_config,
                    quantization_config=self.config.quantization_config,
                    on_disk=self.config.on_disk,
                ),
            )

    def _get_or_create_db(self):
        return self.client

    def _get_or_create_collection(self):
        return f"{self.config.collection_name}-{self.embedder.vector_dimension}".lower().replace("_", "-")

    def get(self, ids: Optional[List[str]] = None, where: Optional[Dict[str, any]] = None, limit: Optional[int] = None):
        """
        Get existing doc ids present in vector database

        :param ids: _list of doc ids to check for existence
        :type ids: List[str]
        :param where: to filter data
        :type where: Dict[str, any]
        :param limit: The number of entries to be fetched
        :type limit: Optional int, defaults to None
        :return: All the existing IDs
        :rtype: Set[str]
        """
        if ids is None or len(ids) == 0:
            return {"ids": []}

        keys = set(where.keys() if where is not None else set())

        qdrant_must_filters = [
            models.FieldCondition(
                key="identifier",
                match=models.MatchAny(
                    any=ids,
                ),
            )
        ]
        if len(keys.intersection(self.metadata_keys)) != 0:
            for key in keys.intersection(self.metadata_keys):
                qdrant_must_filters.append(
                    models.FieldCondition(
                        key="metadata.{}".format(key),
                        match=models.MatchValue(
                            value=where.get(key),
                        ),
                    )
                )

        offset = 0
        existing_ids = []
        while offset is not None:
            response = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=models.Filter(must=qdrant_must_filters),
                offset=offset,
                limit=self.BATCH_SIZE,
            )
            offset = response[1]
            for doc in response[0]:
                existing_ids.append(doc.payload["identifier"])
        return {"ids": existing_ids}

    def add(
        self,
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[object],
        ids: List[str],
        skip_embedding: bool,
    ):
        """add data in vector database
        :param embeddings: list of embeddings for the corresponding documents to be added
        :type documents: List[List[float]]
        :param documents: list of texts to add
        :type documents: List[str]
        :param metadatas: list of metadata associated with docs
        :type metadatas: List[object]
        :param ids: ids of docs
        :type ids: List[str]
        :param skip_embedding: A boolean flag indicating if the embedding for the documents to be added is to be
        generated or not
        :type skip_embedding: bool
        """
        if not skip_embedding:
            embeddings = self.embedder.embedding_fn(documents)

        payloads = []
        qdrant_ids = []
        for id, document, metadata in zip(ids, documents, metadatas):
            metadata["text"] = document
            qdrant_ids.append(str(uuid.uuid4()))
            payloads.append({"identifier": id, "text": document, "metadata": copy.deepcopy(metadata)})
        for i in range(0, len(qdrant_ids), self.BATCH_SIZE):
            self.client.upsert(
                collection_name=self.collection_name,
                points=Batch(
                    ids=qdrant_ids[i : i + self.BATCH_SIZE],
                    payloads=payloads[i : i + self.BATCH_SIZE],
                    vectors=embeddings[i : i + self.BATCH_SIZE],
                ),
            )

    def query(
        self, input_query: List[str], n_results: int, where: Dict[str, any], skip_embedding: bool
    ) -> List[Tuple[str, str, str]]:
        """
        query contents from vector database based on vector similarity
        :param input_query: list of query string
        :type input_query: List[str]
        :param n_results: no of similar documents to fetch from database
        :type n_results: int
        :param where: Optional. to filter data
        :type where: Dict[str, any]
        :param skip_embedding: A boolean flag indicating if the embedding for the documents to be added is to be
        generated or not
        :type skip_embedding: bool
        :return: The context of the document that matched your query, url of the source, doc_id
        :rtype: List[Tuple[str,str,str]]
        """
        if not skip_embedding:
            query_vector = self.embedder.embedding_fn([input_query])[0]
        else:
            query_vector = input_query

        keys = set(where.keys() if where is not None else set())

        qdrant_must_filters = []
        if len(keys.intersection(self.metadata_keys)) != 0:
            for key in keys.intersection(self.metadata_keys):
                qdrant_must_filters.append(
                    models.FieldCondition(
                        key="payload.metadata.{}".format(key),
                        match=models.MatchValue(
                            value=where.get(key),
                        ),
                    )
                )
        results = self.client.search(
            collection_name=self.collection_name,
            query_filter=models.Filter(must=qdrant_must_filters),
            query_vector=query_vector,
            limit=n_results,
        )

        response = []
        for result in results:
            context = result.payload["text"]
            metadata = result.payload["metadata"]
            source = metadata["url"]
            doc_id = metadata["doc_id"]
            response.append(tuple((context, source, doc_id)))
        return response

    def count(self) -> int:
        response = self.client.get_collection(collection_name=self.collection_name)
        return response.points_count

    def reset(self):
        self.client.delete_collection(collection_name=self.collection_name)
        self._initialize()

    def set_collection_name(self, name: str):
        """
        Set the name of the collection. A collection is an isolated space for vectors.

        :param name: Name of the collection.
        :type name: str
        """
        if not isinstance(name, str):
            raise TypeError("Collection name must be a string")
        self.config.collection_name = name
        self.collection_name = self._get_or_create_collection()



# Source: /content/embedchain/embedchain/vectordb/weaviate.py
import copy
import os
from typing import Dict, List, Optional, Tuple

try:
    import weaviate
except ImportError:
    raise ImportError(
        "Weaviate requires extra dependencies. Install with `pip install --upgrade 'embedchain[weaviate]'`"
    ) from None

from embedchain.config.vectordb.weaviate import WeaviateDBConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.vectordb.base import BaseVectorDB


@register_deserializable
class WeaviateDB(BaseVectorDB):
    """
    Weaviate as vector database
    """

    BATCH_SIZE = 100

    def __init__(
        self,
        config: Optional[WeaviateDBConfig] = None,
    ):
        """Weaviate as vector database.
        :param config: Weaviate database config, defaults to None
        :type config: WeaviateDBConfig, optional
        :raises ValueError: No config provided
        """
        if config is None:
            self.config = WeaviateDBConfig()
        else:
            if not isinstance(config, WeaviateDBConfig):
                raise TypeError(
                    "config is not a `WeaviateDBConfig` instance. "
                    "Please make sure the type is right and that you are passing an instance."
                )
            self.config = config
        self.client = weaviate.Client(
            url=os.environ.get("WEAVIATE_ENDPOINT"),
            auth_client_secret=weaviate.AuthApiKey(api_key=os.environ.get("WEAVIATE_API_KEY")),
            **self.config.extra_params,
        )

        # Call parent init here because embedder is needed
        super().__init__(config=self.config)

    def _initialize(self):
        """
        This method is needed because `embedder` attribute needs to be set externally before it can be initialized.
        """

        if not self.embedder:
            raise ValueError("Embedder not set. Please set an embedder with `set_embedder` before initialization.")

        self.index_name = self._get_index_name()
        self.metadata_keys = {"data_type", "doc_id", "url", "hash", "app_id", "text"}
        if not self.client.schema.exists(self.index_name):
            # id is a reserved field in Weaviate, hence we had to change the name of the id field to identifier
            # The none vectorizer is crucial as we have our own custom embedding function
            class_obj = {
                "classes": [
                    {
                        "class": self.index_name,
                        "vectorizer": "none",
                        "properties": [
                            {
                                "name": "identifier",
                                "dataType": ["text"],
                            },
                            {
                                "name": "text",
                                "dataType": ["text"],
                            },
                            {
                                "name": "metadata",
                                "dataType": [self.index_name + "_metadata"],
                            },
                        ],
                    },
                    {
                        "class": self.index_name + "_metadata",
                        "vectorizer": "none",
                        "properties": [
                            {
                                "name": "data_type",
                                "dataType": ["text"],
                            },
                            {
                                "name": "doc_id",
                                "dataType": ["text"],
                            },
                            {
                                "name": "url",
                                "dataType": ["text"],
                            },
                            {
                                "name": "hash",
                                "dataType": ["text"],
                            },
                            {
                                "name": "app_id",
                                "dataType": ["text"],
                            },
                            {
                                "name": "text",
                                "dataType": ["text"],
                            },
                        ],
                    },
                ]
            }

            self.client.schema.create(class_obj)

    def get(self, ids: Optional[List[str]] = None, where: Optional[Dict[str, any]] = None, limit: Optional[int] = None):
        """
        Get existing doc ids present in vector database
        :param ids: _list of doc ids to check for existance
        :type ids: List[str]
        :param where: to filter data
        :type where: Dict[str, any]
        :return: ids
        :rtype: Set[str]
        """

        if ids is None or len(ids) == 0:
            return {"ids": []}

        existing_ids = []
        cursor = None
        has_iterated_once = False
        while cursor is not None or not has_iterated_once:
            has_iterated_once = True
            results = self._query_with_cursor(
                self.client.query.get(self.index_name, ["identifier"])
                .with_additional(["id"])
                .with_limit(self.BATCH_SIZE),
                cursor,
            )
            fetched_results = results["data"]["Get"].get(self.index_name, [])
            if len(fetched_results) == 0:
                break
            for result in fetched_results:
                existing_ids.append(result["identifier"])
                cursor = result["_additional"]["id"]

        return {"ids": existing_ids}

    def add(
        self,
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[object],
        ids: List[str],
        skip_embedding: bool,
    ):
        """add data in vector database
        :param embeddings: list of embeddings for the corresponding documents to be added
        :type documents: List[List[float]]
        :param documents: list of texts to add
        :type documents: List[str]
        :param metadatas: list of metadata associated with docs
        :type metadatas: List[object]
        :param ids: ids of docs
        :type ids: List[str]
        :param skip_embedding: A boolean flag indicating if the embedding for the documents to be added is to be
        generated or not
        :type skip_embedding: bool
        """

        print("Adding documents to Weaviate...")
        if not skip_embedding:
            embeddings = self.embedder.embedding_fn(documents)
        self.client.batch.configure(batch_size=self.BATCH_SIZE, timeout_retries=3)  # Configure batch
        with self.client.batch as batch:  # Initialize a batch process
            for id, text, metadata, embedding in zip(ids, documents, metadatas, embeddings):
                doc = {"identifier": id, "text": text}
                updated_metadata = {"text": text}
                if metadata is not None:
                    updated_metadata.update(**metadata)

                obj_uuid = batch.add_data_object(
                    data_object=copy.deepcopy(doc), class_name=self.index_name, vector=embedding
                )
                metadata_uuid = batch.add_data_object(
                    data_object=copy.deepcopy(updated_metadata),
                    class_name=self.index_name + "_metadata",
                    vector=embedding,
                )
                batch.add_reference(obj_uuid, self.index_name, "metadata", metadata_uuid, self.index_name + "_metadata")

    def query(
        self, input_query: List[str], n_results: int, where: Dict[str, any], skip_embedding: bool
    ) -> List[Tuple[str, str, str]]:
        """
        query contents from vector database based on vector similarity
        :param input_query: list of query string
        :type input_query: List[str]
        :param n_results: no of similar documents to fetch from database
        :type n_results: int
        :param where: Optional. to filter data
        :type where: Dict[str, any]
        :param skip_embedding: A boolean flag indicating if the embedding for the documents to be added is to be
        generated or not
        :type skip_embedding: bool
        :return: The context of the document that matched your query, url of the source, doc_id
        :rtype: List[Tuple[str,str,str]]
        """
        if not skip_embedding:
            query_vector = self.embedder.embedding_fn([input_query])[0]
        else:
            query_vector = input_query
        keys = set(where.keys() if where is not None else set())
        data_fields = ["text"]
        if len(keys.intersection(self.metadata_keys)) != 0:
            weaviate_where_operands = []
            for key in keys:
                if key in self.metadata_keys:
                    weaviate_where_operands.append(
                        {
                            "path": ["metadata", self.index_name + "_metadata", key],
                            "operator": "Equal",
                            "valueText": where.get(key),
                        }
                    )
            if len(weaviate_where_operands) == 1:
                weaviate_where_clause = weaviate_where_operands[0]
            else:
                weaviate_where_clause = {"operator": "And", "operands": weaviate_where_operands}

            results = (
                self.client.query.get(self.index_name, data_fields)
                .with_where(weaviate_where_clause)
                .with_near_vector({"vector": query_vector})
                .with_limit(n_results)
                .do()
            )
        else:
            results = (
                self.client.query.get(self.index_name, data_fields)
                .with_near_vector({"vector": query_vector})
                .with_limit(n_results)
                .do()
            )
        contexts = results["data"]["Get"].get(self.index_name)
        return contexts

    def set_collection_name(self, name: str):
        """
        Set the name of the collection. A collection is an isolated space for vectors.
        :param name: Name of the collection.
        :type name: str
        """
        if not isinstance(name, str):
            raise TypeError("Collection name must be a string")
        self.config.collection_name = name

    def count(self) -> int:
        """
        Count number of documents/chunks embedded in the database.
        :return: number of documents
        :rtype: int
        """
        data = self.client.query.aggregate(self.index_name).with_meta_count().do()
        return data["data"]["Aggregate"].get(self.index_name)[0]["meta"]["count"]

    def _get_or_create_db(self):
        """Called during initialization"""
        return self.client

    def reset(self):
        """
        Resets the database. Deletes all embeddings irreversibly.
        """
        # Delete all data from the database
        self.client.batch.delete_objects(
            self.index_name, where={"path": ["identifier"], "operator": "Like", "valueText": ".*"}
        )

    # Weaviate internally by default capitalizes the class name
    def _get_index_name(self) -> str:
        """Get the Weaviate index for a collection
        :return: Weaviate index
        :rtype: str
        """
        return f"{self.config.collection_name}_{self.embedder.vector_dimension}".capitalize()

    def _query_with_cursor(self, query, cursor):
        if cursor is not None:
            query.with_after(cursor)
        results = query.do()
        return results



# Source: /content/embedchain/embedchain/vectordb/elasticsearch.py
import logging
from typing import Any, Dict, List, Optional, Tuple

try:
    from elasticsearch import Elasticsearch
    from elasticsearch.helpers import bulk
except ImportError:
    raise ImportError(
        "Elasticsearch requires extra dependencies. Install with `pip install --upgrade embedchain[elasticsearch]`"
    ) from None

from embedchain.config import ElasticsearchDBConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.vectordb.base import BaseVectorDB


@register_deserializable
class ElasticsearchDB(BaseVectorDB):
    """
    Elasticsearch as vector database
    """

    def __init__(
        self,
        config: Optional[ElasticsearchDBConfig] = None,
        es_config: Optional[ElasticsearchDBConfig] = None,  # Backwards compatibility
    ):
        """Elasticsearch as vector database.

        :param config: Elasticsearch database config, defaults to None
        :type config: ElasticsearchDBConfig, optional
        :param es_config: `es_config` is supported as an alias for `config` (for backwards compatibility),
        defaults to None
        :type es_config: ElasticsearchDBConfig, optional
        :raises ValueError: No config provided
        """
        if config is None and es_config is None:
            self.config = ElasticsearchDBConfig()
        else:
            if not isinstance(config, ElasticsearchDBConfig):
                raise TypeError(
                    "config is not a `ElasticsearchDBConfig` instance. "
                    "Please make sure the type is right and that you are passing an instance."
                )
            self.config = config or es_config
        self.client = Elasticsearch(self.config.ES_URL, **self.config.ES_EXTRA_PARAMS)

        # Call parent init here because embedder is needed
        super().__init__(config=self.config)

    def _initialize(self):
        """
        This method is needed because `embedder` attribute needs to be set externally before it can be initialized.
        """
        logging.info(self.client.info())
        index_settings = {
            "mappings": {
                "properties": {
                    "text": {"type": "text"},
                    "embeddings": {"type": "dense_vector", "index": False, "dims": self.embedder.vector_dimension},
                }
            }
        }
        es_index = self._get_index()
        if not self.client.indices.exists(index=es_index):
            # create index if not exist
            print("Creating index", es_index, index_settings)
            self.client.indices.create(index=es_index, body=index_settings)

    def _get_or_create_db(self):
        """Called during initialization"""
        return self.client

    def _get_or_create_collection(self, name):
        """Note: nothing to return here. Discuss later"""

    def get(self, ids: Optional[List[str]] = None, where: Optional[Dict[str, any]] = None, limit: Optional[int] = None):
        """
        Get existing doc ids present in vector database

        :param ids: _list of doc ids to check for existance
        :type ids: List[str]
        :param where: to filter data
        :type where: Dict[str, any]
        :return: ids
        :rtype: Set[str]
        """
        if ids:
            query = {"bool": {"must": [{"ids": {"values": ids}}]}}
        else:
            query = {"bool": {"must": []}}
        if "app_id" in where:
            app_id = where["app_id"]
            query["bool"]["must"].append({"term": {"metadata.app_id": app_id}})

        response = self.client.search(index=self._get_index(), query=query, _source=False, size=limit)
        docs = response["hits"]["hits"]
        ids = [doc["_id"] for doc in docs]
        return {"ids": set(ids)}

    def add(
        self,
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[object],
        ids: List[str],
        skip_embedding: bool,
    ) -> Any:
        """
        add data in vector database
        :param embeddings: list of embeddings to add
        :type embeddings: List[List[str]]
        :param documents: list of texts to add
        :type documents: List[str]
        :param metadatas: list of metadata associated with docs
        :type metadatas: List[object]
        :param ids: ids of docs
        :type ids: List[str]
        :param skip_embedding: Optional. If True, then the input_query is assumed to be already embedded.
        :type skip_embedding: bool
        """

        docs = []
        if not skip_embedding:
            embeddings = self.embedder.embedding_fn(documents)

        for id, text, metadata, embeddings in zip(ids, documents, metadatas, embeddings):
            docs.append(
                {
                    "_index": self._get_index(),
                    "_id": id,
                    "_source": {"text": text, "metadata": metadata, "embeddings": embeddings},
                }
            )
        bulk(self.client, docs)
        self.client.indices.refresh(index=self._get_index())

    def query(
        self, input_query: List[str], n_results: int, where: Dict[str, any], skip_embedding: bool
    ) -> List[Tuple[str, str, str]]:
        """
        query contents from vector data base based on vector similarity

        :param input_query: list of query string
        :type input_query: List[str]
        :param n_results: no of similar documents to fetch from database
        :type n_results: int
        :param where: Optional. to filter data
        :type where: Dict[str, any]
        :param skip_embedding: Optional. If True, then the input_query is assumed to be already embedded.
        :type skip_embedding: bool
        :return: The context of the document that matched your query, url of the source, doc_id

        :rtype: List[Tuple[str,str,str]]
        """
        if skip_embedding:
            query_vector = input_query
        else:
            input_query_vector = self.embedder.embedding_fn(input_query)
            query_vector = input_query_vector[0]

        # `https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-script-score-query.html`
        query = {
            "script_score": {
                "query": {"bool": {"must": [{"exists": {"field": "text"}}]}},
                "script": {
                    "source": "cosineSimilarity(params.input_query_vector, 'embeddings') + 1.0",
                    "params": {"input_query_vector": query_vector},
                },
            }
        }
        if "app_id" in where:
            app_id = where["app_id"]
            query["script_score"]["query"] = {"match": {"metadata.app_id": app_id}}
        _source = ["text", "metadata.url", "metadata.doc_id"]
        response = self.client.search(index=self._get_index(), query=query, _source=_source, size=n_results)
        docs = response["hits"]["hits"]
        contents = []
        for doc in docs:
            context = doc["_source"]["text"]
            metadata = doc["_source"]["metadata"]
            source = metadata["url"]
            doc_id = metadata["doc_id"]
            contents.append(tuple((context, source, doc_id)))
        return contents

    def set_collection_name(self, name: str):
        """
        Set the name of the collection. A collection is an isolated space for vectors.

        :param name: Name of the collection.
        :type name: str
        """
        if not isinstance(name, str):
            raise TypeError("Collection name must be a string")
        self.config.collection_name = name

    def count(self) -> int:
        """
        Count number of documents/chunks embedded in the database.

        :return: number of documents
        :rtype: int
        """
        query = {"match_all": {}}
        response = self.client.count(index=self._get_index(), query=query)
        doc_count = response["count"]
        return doc_count

    def reset(self):
        """
        Resets the database. Deletes all embeddings irreversibly.
        """
        # Delete all data from the database
        if self.client.indices.exists(index=self._get_index()):
            # delete index in Es
            self.client.indices.delete(index=self._get_index())

    def _get_index(self) -> str:
        """Get the Elasticsearch index for a collection

        :return: Elasticsearch index
        :rtype: str
        """
        # NOTE: The method is preferred to an attribute, because if collection name changes,
        # it's always up-to-date.
        return f"{self.config.collection_name}_{self.embedder.vector_dimension}".lower()



# Source: /content/embedchain/embedchain/vectordb/chroma.py
import logging
from typing import Any, Dict, List, Optional, Tuple

from chromadb import Collection, QueryResult
from langchain.docstore.document import Document

from embedchain.config import ChromaDbConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.vectordb.base import BaseVectorDB

try:
    import chromadb
    from chromadb.config import Settings
    from chromadb.errors import InvalidDimensionException
except RuntimeError:
    from embedchain.utils import use_pysqlite3

    use_pysqlite3()
    import chromadb
    from chromadb.config import Settings
    from chromadb.errors import InvalidDimensionException


@register_deserializable
class ChromaDB(BaseVectorDB):
    """Vector database using ChromaDB."""

    BATCH_SIZE = 100

    def __init__(self, config: Optional[ChromaDbConfig] = None):
        """Initialize a new ChromaDB instance

        :param config: Configuration options for Chroma, defaults to None
        :type config: Optional[ChromaDbConfig], optional
        """
        if config:
            self.config = config
        else:
            self.config = ChromaDbConfig()

        self.settings = Settings(anonymized_telemetry=False)
        self.settings.allow_reset = self.config.allow_reset if hasattr(self.config, "allow_reset") else False
        if self.config.chroma_settings:
            for key, value in self.config.chroma_settings.items():
                if hasattr(self.settings, key):
                    setattr(self.settings, key, value)

        if self.config.host and self.config.port:
            logging.info(f"Connecting to ChromaDB server: {self.config.host}:{self.config.port}")
            self.settings.chroma_server_host = self.config.host
            self.settings.chroma_server_http_port = self.config.port
            self.settings.chroma_api_impl = "chromadb.api.fastapi.FastAPI"
        else:
            if self.config.dir is None:
                self.config.dir = "db"

            self.settings.persist_directory = self.config.dir
            self.settings.is_persistent = True

        self.client = chromadb.Client(self.settings)
        super().__init__(config=self.config)

    def _initialize(self):
        """
        This method is needed because `embedder` attribute needs to be set externally before it can be initialized.
        """
        if not self.embedder:
            raise ValueError(
                "Embedder not set. Please set an embedder with `_set_embedder()` function before initialization."
            )
        self._get_or_create_collection(self.config.collection_name)

    def _get_or_create_db(self):
        """Called during initialization"""
        return self.client

    def _generate_where_clause(self, where: Dict[str, any]) -> str:
        # If only one filter is supplied, return it as is
        # (no need to wrap in $and based on chroma docs)
        if len(where.keys()) == 1:
            return where
        where_filters = []
        for k, v in where.items():
            if isinstance(v, str):
                where_filters.append({k: v})
        return {"$and": where_filters}

    def _get_or_create_collection(self, name: str) -> Collection:
        """
        Get or create a named collection.

        :param name: Name of the collection
        :type name: str
        :raises ValueError: No embedder configured.
        :return: Created collection
        :rtype: Collection
        """
        if not hasattr(self, "embedder") or not self.embedder:
            raise ValueError("Cannot create a Chroma database collection without an embedder.")
        self.collection = self.client.get_or_create_collection(
            name=name,
            embedding_function=self.embedder.embedding_fn,
        )
        return self.collection

    def get(self, ids: Optional[List[str]] = None, where: Optional[Dict[str, any]] = None, limit: Optional[int] = None):
        """
        Get existing doc ids present in vector database

        :param ids: list of doc ids to check for existence
        :type ids: List[str]
        :param where: Optional. to filter data
        :type where: Dict[str, Any]
        :param limit: Optional. maximum number of documents
        :type limit: Optional[int]
        :return: Existing documents.
        :rtype: List[str]
        """
        args = {}
        if ids:
            args["ids"] = ids
        if where:
            args["where"] = self._generate_where_clause(where)
        if limit:
            args["limit"] = limit
        return self.collection.get(**args)

    def add(
        self,
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[object],
        ids: List[str],
        skip_embedding: bool,
    ) -> Any:
        """
        Add vectors to chroma database

        :param embeddings: list of embeddings to add
        :type embeddings: List[List[str]]
        :param documents: Documents
        :type documents: List[str]
        :param metadatas: Metadatas
        :type metadatas: List[object]
        :param ids: ids
        :type ids: List[str]
        :param skip_embedding: Optional. If True, then the embeddings are assumed to be already generated.
        :type skip_embedding: bool
        """
        size = len(documents)
        if skip_embedding and (embeddings is None or len(embeddings) != len(documents)):
            raise ValueError("Cannot add documents to chromadb with inconsistent embeddings")

        if len(documents) != size or len(metadatas) != size or len(ids) != size:
            raise ValueError(
                "Cannot add documents to chromadb with inconsistent sizes. Documents size: {}, Metadata size: {},"
                " Ids size: {}".format(len(documents), len(metadatas), len(ids))
            )

        for i in range(0, len(documents), self.BATCH_SIZE):
            print("Inserting batches from {} to {} in chromadb".format(i, min(len(documents), i + self.BATCH_SIZE)))
            if skip_embedding:
                self.collection.add(
                    embeddings=embeddings[i : i + self.BATCH_SIZE],
                    documents=documents[i : i + self.BATCH_SIZE],
                    metadatas=metadatas[i : i + self.BATCH_SIZE],
                    ids=ids[i : i + self.BATCH_SIZE],
                )
            else:
                self.collection.add(
                    documents=documents[i : i + self.BATCH_SIZE],
                    metadatas=metadatas[i : i + self.BATCH_SIZE],
                    ids=ids[i : i + self.BATCH_SIZE],
                )

    def _format_result(self, results: QueryResult) -> list[tuple[Document, float]]:
        """
        Format Chroma results

        :param results: ChromaDB query results to format.
        :type results: QueryResult
        :return: Formatted results
        :rtype: list[tuple[Document, float]]
        """
        return [
            (Document(page_content=result[0], metadata=result[1] or {}), result[2])
            for result in zip(
                results["documents"][0],
                results["metadatas"][0],
                results["distances"][0],
            )
        ]

    def query(
        self, input_query: List[str], n_results: int, where: Dict[str, any], skip_embedding: bool
    ) -> List[Tuple[str, str, str]]:
        """
        Query contents from vector database based on vector similarity

        :param input_query: list of query string
        :type input_query: List[str]
        :param n_results: no of similar documents to fetch from database
        :type n_results: int
        :param where: to filter data
        :type where: Dict[str, Any]
        :param skip_embedding: Optional. If True, then the input_query is assumed to be already embedded.
        :type skip_embedding: bool
        :raises InvalidDimensionException: Dimensions do not match.
        :return: The content of the document that matched your query, url of the source, doc_id
        :rtype: List[Tuple[str,str,str]]
        """
        try:
            if skip_embedding:
                result = self.collection.query(
                    query_embeddings=[
                        input_query,
                    ],
                    n_results=n_results,
                    where=where,
                )
            else:
                result = self.collection.query(
                    query_texts=[
                        input_query,
                    ],
                    n_results=n_results,
                    where=where,
                )
        except InvalidDimensionException as e:
            raise InvalidDimensionException(
                e.message()
                + ". This is commonly a side-effect when an embedding function, different from the one used to add the"
                " embeddings, is used to retrieve an embedding from the database."
            ) from None
        results_formatted = self._format_result(result)
        contexts = []
        for result in results_formatted:
            context = result[0].page_content
            metadata = result[0].metadata
            source = metadata["url"]
            doc_id = metadata["doc_id"]
            contexts.append((context, source, doc_id))
        return contexts

    def set_collection_name(self, name: str):
        """
        Set the name of the collection. A collection is an isolated space for vectors.

        :param name: Name of the collection.
        :type name: str
        """
        if not isinstance(name, str):
            raise TypeError("Collection name must be a string")
        self.config.collection_name = name
        self._get_or_create_collection(self.config.collection_name)

    def count(self) -> int:
        """
        Count number of documents/chunks embedded in the database.

        :return: number of documents
        :rtype: int
        """
        return self.collection.count()

    def delete(self, where):
        return self.collection.delete(where=where)

    def reset(self):
        """
        Resets the database. Deletes all embeddings irreversibly.
        """
        # Delete all data from the collection
        try:
            self.client.delete_collection(self.config.collection_name)
        except ValueError:
            raise ValueError(
                "For safety reasons, resetting is disabled. "
                "Please enable it by setting `allow_reset=True` in your ChromaDbConfig"
            ) from None
        # Recreate
        self._get_or_create_collection(self.config.collection_name)

        # Todo: Automatically recreating a collection with the same name cannot be the best way to handle a reset.
        # A downside of this implementation is, if you have two instances,
        # the other instance will not get the updated `self.collection` attribute.
        # A better way would be to create the collection if it is called again after being reset.
        # That means, checking if collection exists in the db-consuming methods, and creating it if it doesn't.
        # That's an extra steps for all uses, just to satisfy a niche use case in a niche method. For now, this will do.



# Source: /content/embedchain/embedchain/vectordb/zilliz.py
import logging
from typing import Dict, List, Optional, Tuple

from embedchain.config import ZillizDBConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.vectordb.base import BaseVectorDB

try:
    from pymilvus import (Collection, CollectionSchema, DataType, FieldSchema,
                          MilvusClient, connections, utility)
except ImportError:
    raise ImportError(
        "Zilliz requires extra dependencies. Install with `pip install --upgrade embedchain[milvus]`"
    ) from None


@register_deserializable
class ZillizVectorDB(BaseVectorDB):
    """Base class for vector database."""

    def __init__(self, config: ZillizDBConfig = None):
        """Initialize the database. Save the config and client as an attribute.

        :param config: Database configuration class instance.
        :type config: ZillizDBConfig
        """

        if config is None:
            self.config = ZillizDBConfig()
        else:
            self.config = config

        self.client = MilvusClient(
            uri=self.config.uri,
            token=self.config.token,
        )

        self.connection = connections.connect(
            uri=self.config.uri,
            token=self.config.token,
        )

        super().__init__(config=self.config)

    def _initialize(self):
        """
        This method is needed because `embedder` attribute needs to be set externally before it can be initialized.

        So it's can't be done in __init__ in one step.
        """
        self._get_or_create_collection(self.config.collection_name)

    def _get_or_create_db(self):
        """Get or create the database."""
        return self.client

    def _get_or_create_collection(self, name):
        """
        Get or create a named collection.

        :param name: Name of the collection
        :type name: str
        """
        if utility.has_collection(name):
            logging.info(f"[ZillizDB]: found an existing collection {name}, make sure the auto-id is disabled.")
            self.collection = Collection(name)
        else:
            fields = [
                FieldSchema(name="id", dtype=DataType.VARCHAR, is_primary=True, max_length=512),
                FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=2048),
                FieldSchema(name="embeddings", dtype=DataType.FLOAT_VECTOR, dim=self.embedder.vector_dimension),
            ]

            schema = CollectionSchema(fields, enable_dynamic_field=True)
            self.collection = Collection(name=name, schema=schema)

            index = {
                "index_type": "AUTOINDEX",
                "metric_type": self.config.metric_type,
            }
            self.collection.create_index("embeddings", index)
        return self.collection

    def get(self, ids: Optional[List[str]] = None, where: Optional[Dict[str, any]] = None, limit: Optional[int] = None):
        """
        Get existing doc ids present in vector database

        :param ids: list of doc ids to check for existence
        :type ids: List[str]
        :param where: Optional. to filter data
        :type where: Dict[str, Any]
        :param limit: Optional. maximum number of documents
        :type limit: Optional[int]
        :return: Existing documents.
        :rtype: Set[str]
        """
        if ids is None or len(ids) == 0 or self.collection.num_entities == 0:
            return {"ids": []}

        if not (self.collection.is_empty):
            filter = f"id in {ids}"
            results = self.client.query(
                collection_name=self.config.collection_name, filter=filter, output_fields=["id"]
            )
            results = [res["id"] for res in results]

        return {"ids": set(results)}

    def add(
        self,
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[object],
        ids: List[str],
        skip_embedding: bool,
    ):
        """Add to database"""
        if not skip_embedding:
            embeddings = self.embedder.embedding_fn(documents)

        for id, doc, metadata, embedding in zip(ids, documents, metadatas, embeddings):
            data = {**metadata, "id": id, "text": doc, "embeddings": embedding}
            self.client.insert(collection_name=self.config.collection_name, data=data)

        self.collection.load()
        self.collection.flush()
        self.client.flush(self.config.collection_name)

    def query(
        self, input_query: List[str], n_results: int, where: Dict[str, any], skip_embedding: bool
    ) -> List[Tuple[str, str, str]]:
        """
        Query contents from vector data base based on vector similarity

        :param input_query: list of query string
        :type input_query: List[str]
        :param n_results: no of similar documents to fetch from database
        :type n_results: int
        :param where: to filter data
        :type where: str
        :raises InvalidDimensionException: Dimensions do not match.
        :return: The context of the document that matched your query, url of the source, doc_id
        :rtype: List[Tuple[str,str,str]]
        """

        if self.collection.is_empty:
            return []

        if not isinstance(where, str):
            where = None

        output_fields = ["text", "url", "doc_id"]
        if skip_embedding:
            query_vector = input_query
            query_result = self.client.search(
                collection_name=self.config.collection_name,
                data=query_vector,
                limit=n_results,
                output_fields=output_fields,
            )

        else:
            input_query_vector = self.embedder.embedding_fn([input_query])
            query_vector = input_query_vector[0]

            query_result = self.client.search(
                collection_name=self.config.collection_name,
                data=[query_vector],
                limit=n_results,
                output_fields=output_fields,
            )

        doc_list = []
        for query in query_result:
            data = query[0]["entity"]
            context = data["text"]
            source = data["url"]
            doc_id = data["doc_id"]
            doc_list.append(tuple((context, source, doc_id)))
        return doc_list

    def count(self) -> int:
        """
        Count number of documents/chunks embedded in the database.

        :return: number of documents
        :rtype: int
        """
        return self.collection.num_entities

    def reset(self, collection_names: List[str] = None):
        """
        Resets the database. Deletes all embeddings irreversibly.
        """
        if self.config.collection_name:
            if collection_names:
                for collection_name in collection_names:
                    if collection_name in self.client.list_collections():
                        self.client.drop_collection(collection_name=collection_name)
            else:
                self.client.drop_collection(collection_name=self.config.collection_name)
                self._get_or_create_collection(self.config.collection_name)

    def set_collection_name(self, name: str):
        """
        Set the name of the collection. A collection is an isolated space for vectors.

        :param name: Name of the collection.
        :type name: str
        """
        if not isinstance(name, str):
            raise TypeError("Collection name must be a string")
        self.config.collection_name = name



# Source: /content/embedchain/embedchain/llm/openai.py
from typing import Optional

from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

from embedchain.config import BaseLlmConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.llm.base import BaseLlm


@register_deserializable
class OpenAILlm(BaseLlm):
    def __init__(self, config: Optional[BaseLlmConfig] = None):
        super().__init__(config=config)

    def get_llm_model_answer(self, prompt) -> str:
        response = OpenAILlm._get_answer(prompt, self.config)
        return response

    def _get_answer(prompt: str, config: BaseLlmConfig) -> str:
        messages = []
        if config.system_prompt:
            messages.append(SystemMessage(content=config.system_prompt))
        messages.append(HumanMessage(content=prompt))
        kwargs = {
            "model": config.model or "gpt-3.5-turbo",
            "temperature": config.temperature,
            "max_tokens": config.max_tokens,
            "model_kwargs": {},
        }
        if config.top_p:
            kwargs["model_kwargs"]["top_p"] = config.top_p
        if config.stream:
            from langchain.callbacks.streaming_stdout import \
                StreamingStdOutCallbackHandler

            chat = ChatOpenAI(**kwargs, streaming=config.stream, callbacks=[StreamingStdOutCallbackHandler()])
        else:
            chat = ChatOpenAI(**kwargs)
        return chat(messages).content



# Source: /content/embedchain/embedchain/llm/llama2.py
import importlib
import os
from typing import Optional

from langchain.llms import Replicate

from embedchain.config import BaseLlmConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.llm.base import BaseLlm


@register_deserializable
class Llama2Llm(BaseLlm):
    def __init__(self, config: Optional[BaseLlmConfig] = None):
        try:
            importlib.import_module("replicate")
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                "The required dependencies for Llama2 are not installed."
                'Please install with `pip install --upgrade "embedchain[llama2]"`'
            ) from None
        if "REPLICATE_API_TOKEN" not in os.environ:
            raise ValueError("Please set the REPLICATE_API_TOKEN environment variable.")

        # Set default config values specific to this llm
        if not config:
            config = BaseLlmConfig()
            # Add variables to this block that have a default value in the parent class
            config.max_tokens = 500
            config.temperature = 0.75
        # Add variables that are `none` by default to this block.
        if not config.model:
            config.model = (
                "a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5"
            )

        super().__init__(config=config)

    def get_llm_model_answer(self, prompt):
        # TODO: Move the model and other inputs into config
        if self.config.system_prompt:
            raise ValueError("Llama2 does not support `system_prompt`")
        llm = Replicate(
            model=self.config.model,
            input={
                "temperature": self.config.temperature,
                "max_length": self.config.max_tokens,
                "top_p": self.config.top_p,
            },
        )
        return llm(prompt)



# Source: /content/embedchain/embedchain/llm/base.py
import logging
from typing import Any, Dict, Generator, List, Optional

from langchain.memory import ConversationBufferMemory
from langchain.schema import BaseMessage

from embedchain.config import BaseLlmConfig
from embedchain.config.llm.base import (DEFAULT_PROMPT,
                                        DEFAULT_PROMPT_WITH_HISTORY_TEMPLATE,
                                        DOCS_SITE_PROMPT_TEMPLATE)
from embedchain.helper.json_serializable import JSONSerializable


class BaseLlm(JSONSerializable):
    def __init__(self, config: Optional[BaseLlmConfig] = None):
        """Initialize a base LLM class

        :param config: LLM configuration option class, defaults to None
        :type config: Optional[BaseLlmConfig], optional
        """
        if config is None:
            self.config = BaseLlmConfig()
        else:
            self.config = config

        self.memory = ConversationBufferMemory()
        self.is_docs_site_instance = False
        self.online = False
        self.history: Any = None

    def get_llm_model_answer(self):
        """
        Usually implemented by child class
        """
        raise NotImplementedError

    def set_history(self, history: Any):
        """
        Provide your own history.
        Especially interesting for the query method, which does not internally manage conversation history.

        :param history: History to set
        :type history: Any
        """
        self.history = history

    def update_history(self):
        """Update class history attribute with history in memory (for chat method)"""
        chat_history = self.memory.load_memory_variables({})["history"]
        if chat_history:
            self.set_history(chat_history)

    def generate_prompt(self, input_query: str, contexts: List[str], **kwargs: Dict[str, Any]) -> str:
        """
        Generates a prompt based on the given query and context, ready to be
        passed to an LLM

        :param input_query: The query to use.
        :type input_query: str
        :param contexts: List of similar documents to the query used as context.
        :type contexts: List[str]
        :return: The prompt
        :rtype: str
        """
        context_string = (" | ").join(contexts)
        web_search_result = kwargs.get("web_search_result", "")
        if web_search_result:
            context_string = self._append_search_and_context(context_string, web_search_result)

        template_contains_history = self.config._validate_template_history(self.config.template)
        if template_contains_history:
            # Template contains history
            # If there is no history yet, we insert `- no history -`
            prompt = self.config.template.substitute(
                context=context_string, query=input_query, history=self.history or "- no history -"
            )
        elif self.history and not template_contains_history:
            # History is present, but not included in the template.
            # check if it's the default template without history
            if (
                not self.config._validate_template_history(self.config.template)
                and self.config.template.template == DEFAULT_PROMPT
            ):
                # swap in the template with history
                prompt = DEFAULT_PROMPT_WITH_HISTORY_TEMPLATE.substitute(
                    context=context_string, query=input_query, history=self.history
                )
            else:
                # If we can't swap in the default, we still proceed but tell users that the history is ignored.
                logging.warning(
                    "Your bot contains a history, but template does not include `$history` key. History is ignored."
                )
                prompt = self.config.template.substitute(context=context_string, query=input_query)
        else:
            # basic use case, no history.
            prompt = self.config.template.substitute(context=context_string, query=input_query)
        return prompt

    def _append_search_and_context(self, context: str, web_search_result: str) -> str:
        """Append web search context to existing context

        :param context: Existing context
        :type context: str
        :param web_search_result: Web search result
        :type web_search_result: str
        :return: Concatenated web search result
        :rtype: str
        """
        return f"{context}\nWeb Search Result: {web_search_result}"

    def get_answer_from_llm(self, prompt: str):
        """
        Gets an answer based on the given query and context by passing it
        to an LLM.

        :param prompt: Gets an answer based on the given query and context by passing it to an LLM.
        :type prompt: str
        :return: The answer.
        :rtype: _type_
        """
        return self.get_llm_model_answer(prompt)

    def access_search_and_get_results(self, input_query: str):
        """
        Search the internet for additional context

        :param input_query: search query
        :type input_query: str
        :return: Search results
        :rtype: Unknown
        """
        try:
            from langchain.tools import DuckDuckGoSearchRun
        except ImportError:
            raise ImportError(
                'Searching requires extra dependencies. Install with `pip install --upgrade "embedchain[dataloaders]"`'
            ) from None
        search = DuckDuckGoSearchRun()
        logging.info(f"Access search to get answers for {input_query}")
        return search.run(input_query)

    def _stream_query_response(self, answer: Any) -> Generator[Any, Any, None]:
        """Generator to be used as streaming response

        :param answer: Answer chunk from llm
        :type answer: Any
        :yield: Answer chunk from llm
        :rtype: Generator[Any, Any, None]
        """
        streamed_answer = ""
        for chunk in answer:
            streamed_answer = streamed_answer + chunk
            yield chunk
        logging.info(f"Answer: {streamed_answer}")

    def _stream_chat_response(self, answer: Any) -> Generator[Any, Any, None]:
        """Generator to be used as streaming response

        :param answer: Answer chunk from llm
        :type answer: Any
        :yield: Answer chunk from llm
        :rtype: Generator[Any, Any, None]
        """
        streamed_answer = ""
        for chunk in answer:
            streamed_answer = streamed_answer + chunk
            yield chunk
        self.memory.chat_memory.add_ai_message(streamed_answer)
        logging.info(f"Answer: {streamed_answer}")

    def query(self, input_query: str, contexts: List[str], config: BaseLlmConfig = None, dry_run=False):
        """
        Queries the vector database based on the given input query.
        Gets relevant doc based on the query and then passes it to an
        LLM as context to get the answer.

        :param input_query: The query to use.
        :type input_query: str
        :param contexts: Embeddings retrieved from the database to be used as context.
        :type contexts: List[str]
        :param config: The `BaseLlmConfig` instance to use as configuration options. This is used for one method call.
        To persistently use a config, declare it during app init., defaults to None
        :type config: Optional[BaseLlmConfig], optional
        :param dry_run: A dry run does everything except send the resulting prompt to
        the LLM. The purpose is to test the prompt, not the response., defaults to False
        :type dry_run: bool, optional
        :return: The answer to the query or the dry run result
        :rtype: str
        """
        try:
            if config:
                # A config instance passed to this method will only be applied temporarily, for one call.
                # So we will save the previous config and restore it at the end of the execution.
                # For this we use the serializer.
                prev_config = self.config.serialize()
                self.config = config

            if config is not None and config.query_type == "Images":
                return contexts

            if self.is_docs_site_instance:
                self.config.template = DOCS_SITE_PROMPT_TEMPLATE
                self.config.number_documents = 5
            k = {}
            if self.online:
                k["web_search_result"] = self.access_search_and_get_results(input_query)
            prompt = self.generate_prompt(input_query, contexts, **k)
            logging.info(f"Prompt: {prompt}")
            if dry_run:
                return prompt

            answer = self.get_answer_from_llm(prompt)

            if isinstance(answer, str):
                logging.info(f"Answer: {answer}")
                return answer
            else:
                return self._stream_query_response(answer)
        finally:
            if config:
                # Restore previous config
                self.config: BaseLlmConfig = BaseLlmConfig.deserialize(prev_config)

    def chat(self, input_query: str, contexts: List[str], config: BaseLlmConfig = None, dry_run=False):
        """
        Queries the vector database on the given input query.
        Gets relevant doc based on the query and then passes it to an
        LLM as context to get the answer.

        Maintains the whole conversation in memory.

        :param input_query: The query to use.
        :type input_query: str
        :param contexts: Embeddings retrieved from the database to be used as context.
        :type contexts: List[str]
        :param config: The `BaseLlmConfig` instance to use as configuration options. This is used for one method call.
        To persistently use a config, declare it during app init., defaults to None
        :type config: Optional[BaseLlmConfig], optional
        :param dry_run: A dry run does everything except send the resulting prompt to
        the LLM. The purpose is to test the prompt, not the response., defaults to False
        :type dry_run: bool, optional
        :return: The answer to the query or the dry run result
        :rtype: str
        """
        try:
            if config:
                # A config instance passed to this method will only be applied temporarily, for one call.
                # So we will save the previous config and restore it at the end of the execution.
                # For this we use the serializer.
                prev_config = self.config.serialize()
                self.config = config

            if self.is_docs_site_instance:
                self.config.template = DOCS_SITE_PROMPT_TEMPLATE
                self.config.number_documents = 5
            k = {}
            if self.online:
                k["web_search_result"] = self.access_search_and_get_results(input_query)

            self.update_history()

            prompt = self.generate_prompt(input_query, contexts, **k)
            logging.info(f"Prompt: {prompt}")

            if dry_run:
                return prompt

            answer = self.get_answer_from_llm(prompt)

            self.memory.chat_memory.add_user_message(input_query)

            if isinstance(answer, str):
                self.memory.chat_memory.add_ai_message(answer)
                logging.info(f"Answer: {answer}")

                # NOTE: Adding to history before and after. This could be seen as redundant.
                # If we change it, we have to change the tests (no big deal).
                self.update_history()

                return answer
            else:
                # this is a streamed response and needs to be handled differently.
                return self._stream_chat_response(answer)
        finally:
            if config:
                # Restore previous config
                self.config: BaseLlmConfig = BaseLlmConfig.deserialize(prev_config)

    @staticmethod
    def _get_messages(prompt: str, system_prompt: Optional[str] = None) -> List[BaseMessage]:
        """
        Construct a list of langchain messages

        :param prompt: User prompt
        :type prompt: str
        :param system_prompt: System prompt, defaults to None
        :type system_prompt: Optional[str], optional
        :return: List of messages
        :rtype: List[BaseMessage]
        """
        from langchain.schema import HumanMessage, SystemMessage

        messages = []
        if system_prompt:
            messages.append(SystemMessage(content=system_prompt))
        messages.append(HumanMessage(content=prompt))
        return messages



# Source: /content/embedchain/embedchain/llm/jina.py
import os
from typing import Optional

from langchain.chat_models import JinaChat
from langchain.schema import HumanMessage, SystemMessage

from embedchain.config import BaseLlmConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.llm.base import BaseLlm


@register_deserializable
class JinaLlm(BaseLlm):
    def __init__(self, config: Optional[BaseLlmConfig] = None):
        if "JINACHAT_API_KEY" not in os.environ:
            raise ValueError("Please set the JINACHAT_API_KEY environment variable.")
        super().__init__(config=config)

    def get_llm_model_answer(self, prompt):
        response = JinaLlm._get_answer(prompt, self.config)
        return response

    @staticmethod
    def _get_answer(prompt: str, config: BaseLlmConfig) -> str:
        messages = []
        if config.system_prompt:
            messages.append(SystemMessage(content=config.system_prompt))
        messages.append(HumanMessage(content=prompt))
        kwargs = {
            "temperature": config.temperature,
            "max_tokens": config.max_tokens,
            "model_kwargs": {},
        }
        if config.top_p:
            kwargs["model_kwargs"]["top_p"] = config.top_p
        if config.stream:
            from langchain.callbacks.streaming_stdout import \
                StreamingStdOutCallbackHandler

            chat = JinaChat(**kwargs, streaming=config.stream, callbacks=[StreamingStdOutCallbackHandler()])
        else:
            chat = JinaChat(**kwargs)
        return chat(messages).content



# Source: /content/embedchain/embedchain/llm/cohere.py
import importlib
import os
from typing import Optional

from langchain.llms import Cohere

from embedchain.config import BaseLlmConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.llm.base import BaseLlm


@register_deserializable
class CohereLlm(BaseLlm):
    def __init__(self, config: Optional[BaseLlmConfig] = None):
        if "COHERE_API_KEY" not in os.environ:
            raise ValueError("Please set the COHERE_API_KEY environment variable.")

        try:
            importlib.import_module("cohere")
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                "The required dependencies for Cohere are not installed."
                'Please install with `pip install --upgrade "embedchain[cohere]"`'
            ) from None

        super().__init__(config=config)

    def get_llm_model_answer(self, prompt):
        if self.config.system_prompt:
            raise ValueError("CohereLlm does not support `system_prompt`")
        return CohereLlm._get_answer(prompt=prompt, config=self.config)

    @staticmethod
    def _get_answer(prompt: str, config: BaseLlmConfig) -> str:
        llm = Cohere(
            cohere_api_key=os.environ["COHERE_API_KEY"],
            model=config.model,
            max_tokens=config.max_tokens,
            temperature=config.temperature,
            p=config.top_p,
        )

        return llm(prompt)



# Source: /content/embedchain/embedchain/llm/anthropic.py
import logging
import os
from typing import Optional

from embedchain.config import BaseLlmConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.llm.base import BaseLlm


@register_deserializable
class AnthropicLlm(BaseLlm):
    def __init__(self, config: Optional[BaseLlmConfig] = None):
        if "ANTHROPIC_API_KEY" not in os.environ:
            raise ValueError("Please set the ANTHROPIC_API_KEY environment variable.")
        super().__init__(config=config)

    def get_llm_model_answer(self, prompt):
        return AnthropicLlm._get_answer(prompt=prompt, config=self.config)

    @staticmethod
    def _get_answer(prompt: str, config: BaseLlmConfig) -> str:
        from langchain.chat_models import ChatAnthropic

        chat = ChatAnthropic(
            anthropic_api_key=os.environ["ANTHROPIC_API_KEY"], temperature=config.temperature, model=config.model
        )

        if config.max_tokens and config.max_tokens != 1000:
            logging.warning("Config option `max_tokens` is not supported by this model.")

        messages = BaseLlm._get_messages(prompt, system_prompt=config.system_prompt)

        return chat(messages).content



# Source: /content/embedchain/embedchain/llm/azure_openai.py
import logging
from typing import Optional

from embedchain.config import BaseLlmConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.llm.base import BaseLlm


@register_deserializable
class AzureOpenAILlm(BaseLlm):
    def __init__(self, config: Optional[BaseLlmConfig] = None):
        super().__init__(config=config)

    def get_llm_model_answer(self, prompt):
        return AzureOpenAILlm._get_answer(prompt=prompt, config=self.config)

    @staticmethod
    def _get_answer(prompt: str, config: BaseLlmConfig) -> str:
        from langchain.chat_models import AzureChatOpenAI

        if not config.deployment_name:
            raise ValueError("Deployment name must be provided for Azure OpenAI")

        chat = AzureChatOpenAI(
            deployment_name=config.deployment_name,
            openai_api_version="2023-05-15",
            model_name=config.model or "gpt-3.5-turbo",
            temperature=config.temperature,
            max_tokens=config.max_tokens,
            streaming=config.stream,
        )

        if config.top_p and config.top_p != 1:
            logging.warning("Config option `top_p` is not supported by this model.")

        messages = BaseLlm._get_messages(prompt, system_prompt=config.system_prompt)

        return chat(messages).content



# Source: /content/embedchain/embedchain/llm/gpt4all.py
from typing import Iterable, Optional, Union

from langchain.callbacks.stdout import StdOutCallbackHandler
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

from embedchain.config import BaseLlmConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.llm.base import BaseLlm


@register_deserializable
class GPT4ALLLlm(BaseLlm):
    def __init__(self, config: Optional[BaseLlmConfig] = None):
        super().__init__(config=config)
        if self.config.model is None:
            self.config.model = "orca-mini-3b.ggmlv3.q4_0.bin"
        self.instance = GPT4ALLLlm._get_instance(self.config.model)
        self.instance.streaming = self.config.stream

    def get_llm_model_answer(self, prompt):
        return self._get_answer(prompt=prompt, config=self.config)

    @staticmethod
    def _get_instance(model):
        try:
            from langchain.llms.gpt4all import GPT4All as LangchainGPT4All
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                "The GPT4All python package is not installed. Please install it with `pip install --upgrade embedchain[opensource]`"  # noqa E501
            ) from None

        return LangchainGPT4All(model=model, allow_download=True)

    def _get_answer(self, prompt: str, config: BaseLlmConfig) -> Union[str, Iterable]:
        if config.model and config.model != self.config.model:
            raise RuntimeError(
                "GPT4ALLLlm does not support switching models at runtime. Please create a new app instance."
            )

        messages = []
        if config.system_prompt:
            messages.append(config.system_prompt)
        messages.append(prompt)
        kwargs = {
            "temp": config.temperature,
            "max_tokens": config.max_tokens,
        }
        if config.top_p:
            kwargs["top_p"] = config.top_p

        callbacks = [StreamingStdOutCallbackHandler()] if config.stream else [StdOutCallbackHandler()]

        response = self.instance.generate(prompts=messages, callbacks=callbacks, **kwargs)
        answer = ""
        for generations in response.generations:
            answer += " ".join(map(lambda generation: generation.text, generations))
        return answer



# Source: /content/embedchain/embedchain/llm/__init__.py



# Source: /content/embedchain/embedchain/llm/huggingface.py
import importlib
import os
from typing import Optional

from langchain.llms import HuggingFaceHub

from embedchain.config import BaseLlmConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.llm.base import BaseLlm


@register_deserializable
class HuggingFaceLlm(BaseLlm):
    def __init__(self, config: Optional[BaseLlmConfig] = None):
        if "HUGGINGFACE_ACCESS_TOKEN" not in os.environ:
            raise ValueError("Please set the HUGGINGFACE_ACCESS_TOKEN environment variable.")

        try:
            importlib.import_module("huggingface_hub")
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                "The required dependencies for HuggingFaceHub are not installed."
                'Please install with `pip install --upgrade "embedchain[huggingface_hub]"`'
            ) from None

        super().__init__(config=config)

    def get_llm_model_answer(self, prompt):
        if self.config.system_prompt:
            raise ValueError("HuggingFaceLlm does not support `system_prompt`")
        return HuggingFaceLlm._get_answer(prompt=prompt, config=self.config)

    @staticmethod
    def _get_answer(prompt: str, config: BaseLlmConfig) -> str:
        model_kwargs = {
            "temperature": config.temperature or 0.1,
            "max_new_tokens": config.max_tokens,
        }

        if config.top_p > 0.0 and config.top_p < 1.0:
            model_kwargs["top_p"] = config.top_p
        else:
            raise ValueError("`top_p` must be > 0.0 and < 1.0")

        llm = HuggingFaceHub(
            huggingfacehub_api_token=os.environ["HUGGINGFACE_ACCESS_TOKEN"],
            repo_id=config.model or "google/flan-t5-xxl",
            model_kwargs=model_kwargs,
        )

        return llm(prompt)



# Source: /content/embedchain/embedchain/llm/vertex_ai.py
import importlib
import logging
from typing import Optional

from embedchain.config import BaseLlmConfig
from embedchain.helper.json_serializable import register_deserializable
from embedchain.llm.base import BaseLlm


@register_deserializable
class VertexAILlm(BaseLlm):
    def __init__(self, config: Optional[BaseLlmConfig] = None):
        try:
            importlib.import_module("vertexai")
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                "The required dependencies for VertexAI are not installed."
                'Please install with `pip install --upgrade "embedchain[vertexai]"`'
            ) from None
        super().__init__(config=config)

    def get_llm_model_answer(self, prompt):
        return VertexAILlm._get_answer(prompt=prompt, config=self.config)

    @staticmethod
    def _get_answer(prompt: str, config: BaseLlmConfig) -> str:
        from langchain.chat_models import ChatVertexAI

        chat = ChatVertexAI(temperature=config.temperature, model=config.model)

        if config.top_p and config.top_p != 1:
            logging.warning("Config option `top_p` is not supported by this model.")

        messages = BaseLlm._get_messages(prompt, system_prompt=config.system_prompt)

        return chat(messages).content



# Source: /content/embedchain/embedchain/config/pipeline_config.py
from typing import Optional

from embedchain.helper.json_serializable import register_deserializable

from .apps.base_app_config import BaseAppConfig


@register_deserializable
class PipelineConfig(BaseAppConfig):
    """
    Config to initialize an embedchain custom `App` instance, with extra config options.
    """

    def __init__(
        self,
        log_level: str = "WARNING",
        id: Optional[str] = None,
        name: Optional[str] = None,
        collect_metrics: Optional[bool] = True,
    ):
        """
        Initializes a configuration class instance for an App. This is the simplest form of an embedchain app.
        Most of the configuration is done in the `App` class itself.

        :param log_level: Debug level ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], defaults to "WARNING"
        :type log_level: str, optional
        :param id: ID of the app. Document metadata will have this id., defaults to None
        :type id: Optional[str], optional
        :param collect_metrics: Send anonymous telemetry to improve embedchain, defaults to True
        :type collect_metrics: Optional[bool], optional
        :param collection_name: Default collection name. It's recommended to use app.db.set_collection_name() instead,
        defaults to None
        :type collection_name: Optional[str], optional
        """
        self._setup_logging(log_level)
        self.id = id
        self.name = name
        self.collect_metrics = collect_metrics



# Source: /content/embedchain/embedchain/config/base_config.py
from typing import Any, Dict

from embedchain.helper.json_serializable import JSONSerializable


class BaseConfig(JSONSerializable):
    """
    Base config.
    """

    def __init__(self):
        """Initializes a configuration class for a class."""
        pass

    def as_dict(self) -> Dict[str, Any]:
        """Return config object as a dict

        :return: config object as dict
        :rtype: Dict[str, Any]
        """
        return vars(self)



# Source: /content/embedchain/embedchain/config/__init__.py
# flake8: noqa: F401

from .add_config import AddConfig, ChunkerConfig
from .apps.app_config import AppConfig
from .base_config import BaseConfig
from .embedder.base import BaseEmbedderConfig
from .embedder.base import BaseEmbedderConfig as EmbedderConfig
from .llm.base import BaseLlmConfig
from .pipeline_config import PipelineConfig
from .vectordb.chroma import ChromaDbConfig
from .vectordb.elasticsearch import ElasticsearchDBConfig
from .vectordb.opensearch import OpenSearchDBConfig
from .vectordb.zilliz import ZillizDBConfig



# Source: /content/embedchain/embedchain/config/add_config.py
from typing import Callable, Optional

from embedchain.config.base_config import BaseConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class ChunkerConfig(BaseConfig):
    """
    Config for the chunker used in `add` method
    """

    def __init__(
        self,
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None,
        length_function: Optional[Callable[[str], int]] = None,
    ):
        self.chunk_size = chunk_size if chunk_size else 2000
        self.chunk_overlap = chunk_overlap if chunk_overlap else 0
        self.length_function = length_function if length_function else len


@register_deserializable
class LoaderConfig(BaseConfig):
    """
    Config for the chunker used in `add` method
    """

    def __init__(self):
        pass


@register_deserializable
class AddConfig(BaseConfig):
    """
    Config for the `add` method.
    """

    def __init__(
        self,
        chunker: Optional[ChunkerConfig] = None,
        loader: Optional[LoaderConfig] = None,
    ):
        """
        Initializes a configuration class instance for the `add` method.

        :param chunker: Chunker config, defaults to None
        :type chunker: Optional[ChunkerConfig], optional
        :param loader: Loader config, defaults to None
        :type loader: Optional[LoaderConfig], optional
        """
        self.loader = loader
        self.chunker = chunker



# Source: /content/embedchain/embedchain/config/embedder/base.py
from typing import Optional

from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class BaseEmbedderConfig:
    def __init__(self, model: Optional[str] = None, deployment_name: Optional[str] = None):
        """
        Initialize a new instance of an embedder config class.

        :param model: model name of the llm embedding model (not applicable to all providers), defaults to None
        :type model: Optional[str], optional
        :param deployment_name: deployment name for llm embedding model, defaults to None
        :type deployment_name: Optional[str], optional
        """
        self.model = model
        self.deployment_name = deployment_name



# Source: /content/embedchain/embedchain/config/embedder/__init__.py



# Source: /content/embedchain/embedchain/config/apps/__init__.py



# Source: /content/embedchain/embedchain/config/apps/base_app_config.py
import logging
from typing import Optional

from embedchain.config.base_config import BaseConfig
from embedchain.helper.json_serializable import JSONSerializable
from embedchain.vectordb.base import BaseVectorDB


class BaseAppConfig(BaseConfig, JSONSerializable):
    """
    Parent config to initialize an instance of `App`.
    """

    def __init__(
        self,
        log_level: str = "WARNING",
        db: Optional[BaseVectorDB] = None,
        id: Optional[str] = None,
        collect_metrics: bool = True,
        collection_name: Optional[str] = None,
    ):
        """
        Initializes a configuration class instance for an App.
        Most of the configuration is done in the `App` class itself.

        :param log_level: Debug level ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], defaults to "WARNING"
        :type log_level: str, optional
        :param db: A database class. It is recommended to set this directly in the `App` class, not this config,
        defaults to None
        :type db: Optional[BaseVectorDB], optional
        :param id: ID of the app. Document metadata will have this id., defaults to None
        :type id: Optional[str], optional
        :param collect_metrics: Send anonymous telemetry to improve embedchain, defaults to True
        :type collect_metrics: Optional[bool], optional
        :param collection_name: Default collection name. It's recommended to use app.db.set_collection_name() instead,
        defaults to None
        :type collection_name: Optional[str], optional
        """
        self._setup_logging(log_level)
        self.id = id
        self.collect_metrics = True if (collect_metrics is True or collect_metrics is None) else False
        self.collection_name = collection_name

        if db:
            self._db = db
            logging.warning(
                "DEPRECATION WARNING: Please supply the database as the second parameter during app init. "
                "Such as `app(config=config, db=db)`."
            )

        if collection_name:
            logging.warning("DEPRECATION WARNING: Please supply the collection name to the database config.")
        return

    def _setup_logging(self, debug_level):
        level = logging.WARNING  # Default level
        if debug_level is not None:
            level = getattr(logging, debug_level.upper(), None)
            if not isinstance(level, int):
                raise ValueError(f"Invalid log level: {debug_level}")

        logging.basicConfig(format="%(asctime)s [%(name)s] [%(levelname)s] %(message)s", level=level)
        self.logger = logging.getLogger(__name__)
        return



# Source: /content/embedchain/embedchain/config/apps/app_config.py
from typing import Optional

from embedchain.helper.json_serializable import register_deserializable

from .base_app_config import BaseAppConfig


@register_deserializable
class AppConfig(BaseAppConfig):
    """
    Config to initialize an embedchain custom `App` instance, with extra config options.
    """

    def __init__(
        self,
        log_level: str = "WARNING",
        id: Optional[str] = None,
        collect_metrics: Optional[bool] = True,
        collection_name: Optional[str] = None,
    ):
        """
        Initializes a configuration class instance for an App. This is the simplest form of an embedchain app.
        Most of the configuration is done in the `App` class itself.

        :param log_level: Debug level ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], defaults to "WARNING"
        :type log_level: str, optional
        :param id: ID of the app. Document metadata will have this id., defaults to None
        :type id: Optional[str], optional
        :param collect_metrics: Send anonymous telemetry to improve embedchain, defaults to True
        :type collect_metrics: Optional[bool], optional
        :param collection_name: Default collection name. It's recommended to use app.db.set_collection_name() instead,
        defaults to None
        :type collection_name: Optional[str], optional
        """
        super().__init__(log_level=log_level, id=id, collect_metrics=collect_metrics, collection_name=collection_name)



# Source: /content/embedchain/embedchain/config/vectordb/base.py
from typing import Optional

from embedchain.config.base_config import BaseConfig


class BaseVectorDbConfig(BaseConfig):
    def __init__(
        self,
        collection_name: Optional[str] = None,
        dir: str = "db",
        host: Optional[str] = None,
        port: Optional[str] = None,
        **kwargs,
    ):
        """
        Initializes a configuration class instance for the vector database.

        :param collection_name: Default name for the collection, defaults to None
        :type collection_name: Optional[str], optional
        :param dir: Path to the database directory, where the database is stored, defaults to "db"
        :type dir: str, optional
        :param host: Database connection remote host. Use this if you run Embedchain as a client, defaults to None
        :type host: Optional[str], optional
        :param host: Database connection remote port. Use this if you run Embedchain as a client, defaults to None
        :type port: Optional[str], optional
        :param kwargs: Additional keyword arguments
        :type kwargs: dict
        """
        self.collection_name = collection_name or "embedchain_store"
        self.dir = dir
        self.host = host
        self.port = port
        # Assign additional keyword arguments
        if kwargs:
            for key, value in kwargs.items():
                setattr(self, key, value)



# Source: /content/embedchain/embedchain/config/vectordb/opensearch.py
from typing import Dict, Optional, Tuple

from embedchain.config.vectordb.base import BaseVectorDbConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class OpenSearchDBConfig(BaseVectorDbConfig):
    def __init__(
        self,
        opensearch_url: str,
        http_auth: Tuple[str, str],
        vector_dimension: int = 1536,
        collection_name: Optional[str] = None,
        dir: Optional[str] = None,
        **extra_params: Dict[str, any],
    ):
        """
        Initializes a configuration class instance for an OpenSearch client.

        :param collection_name: Default name for the collection, defaults to None
        :type collection_name: Optional[str], optional
        :param opensearch_url: URL of the OpenSearch domain
        :type opensearch_url: str, Eg, "http://localhost:9200"
        :param http_auth: Tuple of username and password
        :type http_auth: Tuple[str, str], Eg, ("username", "password")
        :param vector_dimension: Dimension of  the vector, defaults to 1536 (openai embedding model)
        :type vector_dimension: int, optional
        :param dir: Path to the database directory, where the database is stored, defaults to None
        :type dir: Optional[str], optional
        """
        self.opensearch_url = opensearch_url
        self.http_auth = http_auth
        self.vector_dimension = vector_dimension
        self.extra_params = extra_params

        super().__init__(collection_name=collection_name, dir=dir)



# Source: /content/embedchain/embedchain/config/vectordb/pinecone.py
from typing import Dict, Optional

from embedchain.config.vectordb.base import BaseVectorDbConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class PineconeDBConfig(BaseVectorDbConfig):
    def __init__(
        self,
        collection_name: Optional[str] = None,
        dir: Optional[str] = None,
        vector_dimension: int = 1536,
        metric: Optional[str] = "cosine",
        **extra_params: Dict[str, any],
    ):
        self.metric = metric
        self.vector_dimension = vector_dimension
        self.extra_params = extra_params
        super().__init__(collection_name=collection_name, dir=dir)



# Source: /content/embedchain/embedchain/config/vectordb/__init__.py



# Source: /content/embedchain/embedchain/config/vectordb/qdrant.py
from typing import Dict, Optional

from embedchain.config.vectordb.base import BaseVectorDbConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class QdrantDBConfig(BaseVectorDbConfig):
    """
    Config to initialize an qdrant client.
    :param url. qdrant url or list of nodes url to be used for connection
    """

    def __init__(
        self,
        collection_name: Optional[str] = None,
        dir: Optional[str] = None,
        hnsw_config: Optional[Dict[str, any]] = None,
        quantization_config: Optional[Dict[str, any]] = None,
        on_disk: Optional[bool] = None,
        **extra_params: Dict[str, any],
    ):
        """
        Initializes a configuration class instance for a qdrant client.

        :param collection_name: Default name for the collection, defaults to None
        :type collection_name: Optional[str], optional
        :param dir: Path to the database directory, where the database is stored, defaults to None
        :type dir: Optional[str], optional
        :param hnsw_config: Params for HNSW index
        :type hnsw_config: Optional[Dict[str, any]], defaults to None
        :param quantization_config: Params for quantization, if None - quantization will be disabled
        :type quantization_config: Optional[Dict[str, any]], defaults to None
        :param on_disk: If true - point`s payload will not be stored in memory.
                It will be read from the disk every time it is requested.
                This setting saves RAM by (slightly) increasing the response time.
                Note: those payload values that are involved in filtering and are indexed - remain in RAM.
        :type on_disk: bool, optional, defaults to None
        """
        self.hnsw_config = hnsw_config
        self.quantization_config = quantization_config
        self.on_disk = on_disk
        self.extra_params = extra_params
        super().__init__(collection_name=collection_name, dir=dir)



# Source: /content/embedchain/embedchain/config/vectordb/weaviate.py
from typing import Dict, Optional

from embedchain.config.vectordb.base import BaseVectorDbConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class WeaviateDBConfig(BaseVectorDbConfig):
    def __init__(
        self,
        collection_name: Optional[str] = None,
        dir: Optional[str] = None,
        **extra_params: Dict[str, any],
    ):
        self.extra_params = extra_params
        super().__init__(collection_name=collection_name, dir=dir)



# Source: /content/embedchain/embedchain/config/vectordb/elasticsearch.py
import os
from typing import Dict, List, Optional, Union

from embedchain.config.vectordb.base import BaseVectorDbConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class ElasticsearchDBConfig(BaseVectorDbConfig):
    def __init__(
        self,
        collection_name: Optional[str] = None,
        dir: Optional[str] = None,
        es_url: Union[str, List[str]] = None,
        **ES_EXTRA_PARAMS: Dict[str, any],
    ):
        """
        Initializes a configuration class instance for an Elasticsearch client.

        :param collection_name: Default name for the collection, defaults to None
        :type collection_name: Optional[str], optional
        :param dir: Path to the database directory, where the database is stored, defaults to None
        :type dir: Optional[str], optional
        :param es_url: elasticsearch url or list of nodes url to be used for connection, defaults to None
        :type es_url: Union[str, List[str]], optional
        :param ES_EXTRA_PARAMS: extra params dict that can be passed to elasticsearch.
        :type ES_EXTRA_PARAMS: Dict[str, Any], optional
        """
        # self, es_url: Union[str, List[str]] = None, **ES_EXTRA_PARAMS: Dict[str, any]):
        self.ES_URL = es_url or os.environ.get("ELASTICSEARCH_URL")
        if not self.ES_URL:
            raise AttributeError(
                "Elasticsearch needs a URL attribute, "
                "this can either be passed to `ElasticsearchDBConfig` or as `ELASTICSEARCH_URL` in `.env`"
            )
        self.ES_EXTRA_PARAMS = ES_EXTRA_PARAMS
        # Load API key from .env if it's not explicitly passed.
        # Can only set one of 'api_key', 'basic_auth', and 'bearer_auth'
        if (
            not self.ES_EXTRA_PARAMS.get("api_key")
            and not self.ES_EXTRA_PARAMS.get("basic_auth")
            and not self.ES_EXTRA_PARAMS.get("bearer_auth")
            and not self.ES_EXTRA_PARAMS.get("http_auth")
        ):
            self.ES_EXTRA_PARAMS["api_key"] = os.environ.get("ELASTICSEARCH_API_KEY")
        super().__init__(collection_name=collection_name, dir=dir)



# Source: /content/embedchain/embedchain/config/vectordb/chroma.py
from typing import Optional

from embedchain.config.vectordb.base import BaseVectorDbConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class ChromaDbConfig(BaseVectorDbConfig):
    def __init__(
        self,
        collection_name: Optional[str] = None,
        dir: Optional[str] = None,
        host: Optional[str] = None,
        port: Optional[str] = None,
        allow_reset=False,
        chroma_settings: Optional[dict] = None,
    ):
        """
        Initializes a configuration class instance for ChromaDB.

        :param collection_name: Default name for the collection, defaults to None
        :type collection_name: Optional[str], optional
        :param dir: Path to the database directory, where the database is stored, defaults to None
        :type dir: Optional[str], optional
        :param host: Database connection remote host. Use this if you run Embedchain as a client, defaults to None
        :type host: Optional[str], optional
        :param port: Database connection remote port. Use this if you run Embedchain as a client, defaults to None
        :type port: Optional[str], optional
        :param allow_reset: Resets the database. defaults to False
        :type allow_reset: bool
        :param chroma_settings: Chroma settings dict, defaults to None
        :type chroma_settings: Optional[dict], optional
        """

        self.chroma_settings = chroma_settings
        self.allow_reset = allow_reset
        super().__init__(collection_name=collection_name, dir=dir, host=host, port=port)



# Source: /content/embedchain/embedchain/config/vectordb/zilliz.py
import os
from typing import Optional

from embedchain.config.vectordb.base import BaseVectorDbConfig
from embedchain.helper.json_serializable import register_deserializable


@register_deserializable
class ZillizDBConfig(BaseVectorDbConfig):
    def __init__(
        self,
        collection_name: Optional[str] = None,
        dir: Optional[str] = None,
        uri: Optional[str] = None,
        token: Optional[str] = None,
        vector_dim: Optional[str] = None,
        metric_type: Optional[str] = None,
    ):
        """
        Initializes a configuration class instance for the vector database.

        :param collection_name: Default name for the collection, defaults to None
        :type collection_name: Optional[str], optional
        :param dir: Path to the database directory, where the database is stored, defaults to "db"
        :type dir: str, optional
        :param uri: Cluster endpoint obtained from the Zilliz Console, defaults to None
        :type uri: Optional[str], optional
        :param token: API Key, if a Serverless Cluster, username:password, if a Dedicated Cluster, defaults to None
        :type port: Optional[str], optional
        """
        self.uri = uri or os.environ.get("ZILLIZ_CLOUD_URI")
        if not self.uri:
            raise AttributeError(
                "Zilliz needs a URI attribute, "
                "this can either be passed to `ZILLIZ_CLOUD_URI` or as `ZILLIZ_CLOUD_URI` in `.env`"
            )

        self.token = token or os.environ.get("ZILLIZ_CLOUD_TOKEN")
        if not self.token:
            raise AttributeError(
                "Zilliz needs a token attribute, "
                "this can either be passed to `ZILLIZ_CLOUD_TOKEN` or as `ZILLIZ_CLOUD_TOKEN` in `.env`,"
                "if having a username and password, pass it in the form 'username:password' to `ZILLIZ_CLOUD_TOKEN`"
            )

        self.metric_type = metric_type if metric_type else "L2"

        self.vector_dim = vector_dim
        super().__init__(collection_name=collection_name, dir=dir)



# Source: /content/embedchain/embedchain/config/llm/base.py
import re
from string import Template
from typing import Any, Dict, Optional

from embedchain.config.base_config import BaseConfig
from embedchain.helper.json_serializable import register_deserializable

DEFAULT_PROMPT = """
  Use the following pieces of context to answer the query at the end.
  If you don't know the answer, just say that you don't know, don't try to make up an answer.

  $context

  Query: $query

  Helpful Answer:
"""  # noqa:E501

DEFAULT_PROMPT_WITH_HISTORY = """
  Use the following pieces of context to answer the query at the end.
  If you don't know the answer, just say that you don't know, don't try to make up an answer.
  I will provide you with our conversation history.

  $context

  History: $history

  Query: $query

  Helpful Answer:
"""  # noqa:E501

DOCS_SITE_DEFAULT_PROMPT = """
  Use the following pieces of context to answer the query at the end.
  If you don't know the answer, just say that you don't know, don't try to make up an answer. Wherever possible, give complete code snippet. Dont make up any code snippet on your own.

  $context

  Query: $query

  Helpful Answer:
"""  # noqa:E501

DEFAULT_PROMPT_TEMPLATE = Template(DEFAULT_PROMPT)
DEFAULT_PROMPT_WITH_HISTORY_TEMPLATE = Template(DEFAULT_PROMPT_WITH_HISTORY)
DOCS_SITE_PROMPT_TEMPLATE = Template(DOCS_SITE_DEFAULT_PROMPT)
query_re = re.compile(r"\$\{*query\}*")
context_re = re.compile(r"\$\{*context\}*")
history_re = re.compile(r"\$\{*history\}*")


@register_deserializable
class BaseLlmConfig(BaseConfig):
    """
    Config for the `query` method.
    """

    def __init__(
        self,
        number_documents: int = 1,
        template: Optional[Template] = None,
        model: Optional[str] = None,
        temperature: float = 0,
        max_tokens: int = 1000,
        top_p: float = 1,
        stream: bool = False,
        deployment_name: Optional[str] = None,
        system_prompt: Optional[str] = None,
        where: Dict[str, Any] = None,
        query_type: Optional[str] = None,
    ):
        """
        Initializes a configuration class instance for the LLM.

        Takes the place of the former `QueryConfig` or `ChatConfig`.

        :param number_documents:  Number of documents to pull from the database as
        context, defaults to 1
        :type number_documents: int, optional
        :param template:  The `Template` instance to use as a template for
        prompt, defaults to None
        :type template: Optional[Template], optional
        :param model: Controls the OpenAI model used, defaults to None
        :type model: Optional[str], optional
        :param temperature:  Controls the randomness of the model's output.
        Higher values (closer to 1) make output more random, lower values make it more deterministic, defaults to 0
        :type temperature: float, optional
        :param max_tokens: Controls how many tokens are generated, defaults to 1000
        :type max_tokens: int, optional
        :param top_p: Controls the diversity of words. Higher values (closer to 1) make word selection more diverse,
        defaults to 1
        :type top_p: float, optional
        :param stream: Control if response is streamed back to user, defaults to False
        :type stream: bool, optional
        :param deployment_name: t.b.a., defaults to None
        :type deployment_name: Optional[str], optional
        :param system_prompt: System prompt string, defaults to None
        :type system_prompt: Optional[str], optional
        :param where: A dictionary of key-value pairs to filter the database results., defaults to None
        :type where: Dict[str, Any], optional
        :raises ValueError: If the template is not valid as template should
        contain $context and $query (and optionally $history)
        :raises ValueError: Stream is not boolean
        """
        if template is None:
            template = DEFAULT_PROMPT_TEMPLATE

        self.number_documents = number_documents
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.model = model
        self.top_p = top_p
        self.deployment_name = deployment_name
        self.system_prompt = system_prompt
        self.query_type = query_type

        if type(template) is str:
            template = Template(template)

        if self.validate_template(template):
            self.template = template
        else:
            raise ValueError("`template` should have `query` and `context` keys and potentially `history` (if used).")

        if not isinstance(stream, bool):
            raise ValueError("`stream` should be bool")
        self.stream = stream
        self.where = where

    def validate_template(self, template: Template) -> bool:
        """
        validate the template

        :param template: the template to validate
        :type template: Template
        :return: valid (true) or invalid (false)
        :rtype: bool
        """
        return re.search(query_re, template.template) and re.search(context_re, template.template)

    def _validate_template_history(self, template: Template) -> bool:
        """
        validate the template with history

        :param template: the template to validate
        :type template: Template
        :return: valid (true) or invalid (false)
        :rtype: bool
        """
        return re.search(history_re, template.template)



# Source: /content/embedchain/embedchain/config/llm/__init__.py



# Source: /content/embedchain/tests/test_factory.py
import os

import pytest

import embedchain
import embedchain.embedder.gpt4all
import embedchain.embedder.huggingface
import embedchain.embedder.openai
import embedchain.embedder.vertexai
import embedchain.llm.anthropic
import embedchain.llm.openai
import embedchain.vectordb.chroma
import embedchain.vectordb.elasticsearch
import embedchain.vectordb.opensearch
from embedchain.factory import EmbedderFactory, LlmFactory, VectorDBFactory


class TestFactories:
    @pytest.mark.parametrize(
        "provider_name, config_data, expected_class",
        [
            ("openai", {}, embedchain.llm.openai.OpenAILlm),
            ("anthropic", {}, embedchain.llm.anthropic.AnthropicLlm),
        ],
    )
    def test_llm_factory_create(self, provider_name, config_data, expected_class):
        os.environ["ANTHROPIC_API_KEY"] = "test_api_key"
        os.environ["OPENAI_API_KEY"] = "test_api_key"
        llm_instance = LlmFactory.create(provider_name, config_data)
        assert isinstance(llm_instance, expected_class)

    @pytest.mark.parametrize(
        "provider_name, config_data, expected_class",
        [
            ("gpt4all", {}, embedchain.embedder.gpt4all.GPT4AllEmbedder),
            (
                "huggingface",
                {"model": "sentence-transformers/all-mpnet-base-v2"},
                embedchain.embedder.huggingface.HuggingFaceEmbedder,
            ),
            ("vertexai", {"model": "textembedding-gecko"}, embedchain.embedder.vertexai.VertexAIEmbedder),
            ("openai", {}, embedchain.embedder.openai.OpenAIEmbedder),
        ],
    )
    def test_embedder_factory_create(self, mocker, provider_name, config_data, expected_class):
        mocker.patch("embedchain.embedder.vertexai.VertexAIEmbedder", autospec=True)
        embedder_instance = EmbedderFactory.create(provider_name, config_data)
        assert isinstance(embedder_instance, expected_class)

    @pytest.mark.parametrize(
        "provider_name, config_data, expected_class",
        [
            ("chroma", {}, embedchain.vectordb.chroma.ChromaDB),
            (
                "opensearch",
                {"opensearch_url": "http://localhost:9200", "http_auth": ("admin", "admin")},
                embedchain.vectordb.opensearch.OpenSearchDB,
            ),
            ("elasticsearch", {"es_url": "http://localhost:9200"}, embedchain.vectordb.elasticsearch.ElasticsearchDB),
        ],
    )
    def test_vectordb_factory_create(self, mocker, provider_name, config_data, expected_class):
        mocker.patch("embedchain.vectordb.opensearch.OpenSearchDB", autospec=True)
        vectordb_instance = VectorDBFactory.create(provider_name, config_data)
        assert isinstance(vectordb_instance, expected_class)



# Source: /content/embedchain/tests/__init__.py



# Source: /content/embedchain/tests/conftest.py
import os

import pytest


def clean_db():
    db_path = os.path.expanduser("~/.embedchain/embedchain.db")
    if os.path.exists(db_path):
        os.remove(db_path)


@pytest.fixture
def setup():
    clean_db()
    yield
    clean_db()


@pytest.fixture(autouse=True)
def disable_telemetry():
    os.environ["EC_TELEMETRY"] = "false"
    yield
    del os.environ["EC_TELEMETRY"]



# Source: /content/embedchain/tests/test_client.py
import pytest

from embedchain import Client


class TestClient:
    @pytest.fixture
    def mock_requests_post(self, mocker):
        return mocker.patch("embedchain.client.requests.post")

    def test_valid_api_key(self, mock_requests_post):
        mock_requests_post.return_value.status_code = 200
        client = Client(api_key="valid_api_key")
        assert client.check("valid_api_key") is True

    def test_invalid_api_key(self, mock_requests_post):
        mock_requests_post.return_value.status_code = 401
        with pytest.raises(ValueError):
            Client(api_key="invalid_api_key")

    def test_update_valid_api_key(self, mock_requests_post):
        mock_requests_post.return_value.status_code = 200
        client = Client(api_key="valid_api_key")
        client.update("new_valid_api_key")
        assert client.get() == "new_valid_api_key"

    def test_clear_api_key(self, mock_requests_post):
        mock_requests_post.return_value.status_code = 200
        client = Client(api_key="valid_api_key")
        client.clear()
        assert client.get() is None

    def test_save_api_key(self, mock_requests_post):
        mock_requests_post.return_value.status_code = 200
        api_key_to_save = "valid_api_key"
        client = Client(api_key=api_key_to_save)
        client.save()
        assert client.get() == api_key_to_save

    def test_load_api_key_from_config(self, mocker):
        mocker.patch("embedchain.Client.load_config", return_value={"api_key": "test_api_key"})
        client = Client()
        assert client.get() == "test_api_key"

    def test_load_invalid_api_key_from_config(self, mocker):
        mocker.patch("embedchain.Client.load_config", return_value={})
        with pytest.raises(ValueError):
            Client()

    def test_load_missing_api_key_from_config(self, mocker):
        mocker.patch("embedchain.Client.load_config", return_value={})
        with pytest.raises(ValueError):
            Client()



# Source: /content/embedchain/tests/models/test_data_type.py
from embedchain.models.data_type import (DataType, DirectDataType,
                                         IndirectDataType, SpecialDataType)


def test_subclass_types_in_data_type():
    """Test that all data type category subclasses are contained in the composite data type"""
    # Check if DirectDataType values are in DataType
    for data_type in DirectDataType:
        assert data_type.value in DataType._value2member_map_

    # Check if IndirectDataType values are in DataType
    for data_type in IndirectDataType:
        assert data_type.value in DataType._value2member_map_

    # Check if SpecialDataType values are in DataType
    for data_type in SpecialDataType:
        assert data_type.value in DataType._value2member_map_


def test_data_type_in_subclasses():
    """Test that all data types in the composite data type are categorized in a subclass"""
    for data_type in DataType:
        if data_type.value in DirectDataType._value2member_map_:
            assert data_type.value in DirectDataType._value2member_map_
        elif data_type.value in IndirectDataType._value2member_map_:
            assert data_type.value in IndirectDataType._value2member_map_
        elif data_type.value in SpecialDataType._value2member_map_:
            assert data_type.value in SpecialDataType._value2member_map_
        else:
            assert False, f"{data_type.value} not found in any subclass enums"



# Source: /content/embedchain/tests/models/test_clip_processor.py
import os
import tempfile
import urllib

from PIL import Image

from embedchain.models.clip_processor import ClipProcessor


class TestClipProcessor:
    def test_load_model(self):
        # Test that the `load_model()` method loads the CLIP model and image preprocessing correctly.
        model = ClipProcessor.load_model()
        assert model is not None

    def test_get_image_features(self):
        # Clone the image to a temporary folder.
        with tempfile.TemporaryDirectory() as tmp_dir:
            urllib.request.urlretrieve("https://upload.wikimedia.org/wikipedia/en/a/a9/Example.jpg", "image.jpg")

            image = Image.open("image.jpg")
            image.save(os.path.join(tmp_dir, "image.jpg"))

            # Get the image features.
            model = ClipProcessor.load_model()
            ClipProcessor.get_image_features(os.path.join(tmp_dir, "image.jpg"), model)

            # Delete the temporary file.
            os.remove(os.path.join(tmp_dir, "image.jpg"))
            os.remove("image.jpg")

    def test_get_text_features(self):
        # Test that the `get_text_features()` method returns a list containing the text embedding.
        query = "This is a text query."
        text_features = ClipProcessor.get_text_features(query)

        # Assert that the text embedding is not None.
        assert text_features is not None

        # Assert that the text embedding is a list of floats.
        assert isinstance(text_features, list)

        # Assert that the text embedding has the correct length.
        assert len(text_features) == 512



# Source: /content/embedchain/tests/helper_classes/test_json_serializable.py
import random
import unittest
from string import Template

from embedchain import App
from embedchain.config import AppConfig, BaseLlmConfig
from embedchain.helper.json_serializable import (JSONSerializable,
                                                 register_deserializable)


class TestJsonSerializable(unittest.TestCase):
    """Test that the datatype detection is working, based on the input."""

    def test_base_function(self):
        """Test that the base premise of serialization and deserealization is working"""

        @register_deserializable
        class TestClass(JSONSerializable):
            def __init__(self):
                self.rng = random.random()

        original_class = TestClass()
        serial = original_class.serialize()

        # Negative test to show that a new class does not have the same random number.
        negative_test_class = TestClass()
        self.assertNotEqual(original_class.rng, negative_test_class.rng)

        # Test to show that a deserialized class has the same random number.
        positive_test_class: TestClass = TestClass().deserialize(serial)
        self.assertEqual(original_class.rng, positive_test_class.rng)
        self.assertTrue(isinstance(positive_test_class, TestClass))

        # Test that it works as a static method too.
        positive_test_class: TestClass = TestClass.deserialize(serial)
        self.assertEqual(original_class.rng, positive_test_class.rng)

    # TODO: There's no reason it shouldn't work, but serialization to and from file should be tested too.

    def test_registration_required(self):
        """Test that registration is required, and that without registration the default class is returned."""

        class SecondTestClass(JSONSerializable):
            def __init__(self):
                self.default = True

        app = SecondTestClass()
        # Make not default
        app.default = False
        # Serialize
        serial = app.serialize()
        # Deserialize. Due to the way errors are handled, it will not fail but return a default class.
        app: SecondTestClass = SecondTestClass().deserialize(serial)
        self.assertTrue(app.default)
        # If we register and try again with the same serial, it should work
        SecondTestClass.register_class_as_deserializable(SecondTestClass)
        app: SecondTestClass = SecondTestClass().deserialize(serial)
        self.assertFalse(app.default)

    def test_recursive(self):
        """Test recursiveness with the real app"""
        random_id = str(random.random())
        config = AppConfig(id=random_id, collect_metrics=False)
        # config class is set under app.config.
        app = App(config=config)
        s = app.serialize()
        new_app: App = App.deserialize(s)
        # The id of the new app is the same as the first one.
        self.assertEqual(random_id, new_app.config.id)
        # We have proven that a nested class (app.config) can be serialized and deserialized just the same.
        # TODO: test deeper recursion

    def test_special_subclasses(self):
        """Test special subclasses that are not serializable by default."""
        # Template
        config = BaseLlmConfig(template=Template("My custom template with $query, $context and $history."))
        s = config.serialize()
        new_config: BaseLlmConfig = BaseLlmConfig.deserialize(s)
        self.assertEqual(config.template.template, new_config.template.template)



# Source: /content/embedchain/tests/bots/test_poe.py
import argparse

import pytest
from fastapi_poe.types import ProtocolMessage, QueryRequest

from embedchain.bots.poe import PoeBot, start_command


@pytest.fixture
def poe_bot(mocker):
    bot = PoeBot()
    mocker.patch("fastapi_poe.run")
    return bot


@pytest.mark.asyncio
async def test_poe_bot_get_response(poe_bot, mocker):
    query = QueryRequest(
        version="test",
        type="query",
        query=[ProtocolMessage(role="system", content="Test content")],
        user_id="test_user_id",
        conversation_id="test_conversation_id",
        message_id="test_message_id",
    )

    mocker.patch.object(poe_bot.app.llm, "set_history")

    response_generator = poe_bot.get_response(query)

    await response_generator.__anext__()
    poe_bot.app.llm.set_history.assert_called_once()


def test_poe_bot_handle_message(poe_bot, mocker):
    mocker.patch.object(poe_bot, "ask_bot", return_value="Answer from the bot")

    response_ask = poe_bot.handle_message("What is the answer?")
    assert response_ask == "Answer from the bot"

    # TODO: This test will fail because the add_data method is commented out.
    # mocker.patch.object(poe_bot, 'add_data', return_value="Added data from: some_data")
    # response_add = poe_bot.handle_message("/add some_data")
    # assert response_add == "Added data from: some_data"


def test_start_command(mocker):
    mocker.patch("argparse.ArgumentParser.parse_args", return_value=argparse.Namespace(api_key="test_api_key"))
    mocker.patch("embedchain.bots.poe.run")

    start_command()



# Source: /content/embedchain/tests/bots/test_base.py
import os
from unittest.mock import patch

import pytest

from embedchain.bots.base import BaseBot
from embedchain.config import AddConfig, BaseLlmConfig


@pytest.fixture
def base_bot():
    os.environ["OPENAI_API_KEY"] = "test_api_key"  # needed by App
    return BaseBot()


def test_add(base_bot):
    data = "Test data"
    config = AddConfig()

    with patch.object(base_bot.app, "add") as mock_add:
        base_bot.add(data, config)
        mock_add.assert_called_with(data, config=config)


def test_query(base_bot):
    query = "Test query"
    config = BaseLlmConfig()

    with patch.object(base_bot.app, "query") as mock_query:
        mock_query.return_value = "Query result"

        result = base_bot.query(query, config)

    assert isinstance(result, str)
    assert result == "Query result"


def test_start():
    class TestBot(BaseBot):
        def start(self):
            return "Bot started"

    bot = TestBot()
    result = bot.start()
    assert result == "Bot started"


def test_start_not_implemented():
    bot = BaseBot()
    with pytest.raises(NotImplementedError):
        bot.start()



# Source: /content/embedchain/tests/embedder/test_embedder.py
from unittest.mock import MagicMock

import pytest
from chromadb.api.types import Documents, Embeddings

from embedchain.config.embedder.base import BaseEmbedderConfig
from embedchain.embedder.base import BaseEmbedder


@pytest.fixture
def base_embedder():
    return BaseEmbedder()


def test_initialization(base_embedder):
    assert isinstance(base_embedder.config, BaseEmbedderConfig)
    # not initialized
    assert not hasattr(base_embedder, "embedding_fn")
    assert not hasattr(base_embedder, "vector_dimension")


def test_set_embedding_fn(base_embedder):
    def embedding_function(texts: Documents) -> Embeddings:
        return [f"Embedding for {text}" for text in texts]

    base_embedder.set_embedding_fn(embedding_function)
    assert hasattr(base_embedder, "embedding_fn")
    assert callable(base_embedder.embedding_fn)
    embeddings = base_embedder.embedding_fn(["text1", "text2"])
    assert embeddings == ["Embedding for text1", "Embedding for text2"]


def test_set_embedding_fn_when_not_a_function(base_embedder):
    with pytest.raises(ValueError):
        base_embedder.set_embedding_fn(None)


def test_set_vector_dimension(base_embedder):
    base_embedder.set_vector_dimension(256)
    assert hasattr(base_embedder, "vector_dimension")
    assert base_embedder.vector_dimension == 256


def test_set_vector_dimension_type_error(base_embedder):
    with pytest.raises(TypeError):
        base_embedder.set_vector_dimension(None)


def test_langchain_default_concept():
    embeddings = MagicMock()
    embeddings.embed_documents.return_value = ["Embedding1", "Embedding2"]
    embed_function = BaseEmbedder._langchain_default_concept(embeddings)
    result = embed_function(["text1", "text2"])
    assert result == ["Embedding1", "Embedding2"]


def test_embedder_with_config():
    embedder = BaseEmbedder(BaseEmbedderConfig())
    assert isinstance(embedder.config, BaseEmbedderConfig)



# Source: /content/embedchain/tests/telemetry/test_posthog.py
import os
import logging
from embedchain.telemetry.posthog import AnonymousTelemetry


class TestAnonymousTelemetry:
    def test_init(self, mocker):
        # Enable telemetry specifically for this test
        os.environ["EC_TELEMETRY"] = "true"
        mock_posthog = mocker.patch("embedchain.telemetry.posthog.Posthog")
        telemetry = AnonymousTelemetry()
        assert telemetry.project_api_key == "phc_XnMmNHzwxE7PVHX4mD2r8K6nfxVM48a2sq2U3N1p2lO"
        assert telemetry.host == "https://app.posthog.com"
        assert telemetry.enabled is True
        assert telemetry.user_id
        mock_posthog.assert_called_once_with(project_api_key=telemetry.project_api_key, host=telemetry.host)

    def test_init_with_disabled_telemetry(self, mocker, monkeypatch):
        mocker.patch("embedchain.telemetry.posthog.Posthog")
        telemetry = AnonymousTelemetry()
        assert telemetry.enabled is False
        assert telemetry.posthog.disabled is True

    def test_get_user_id(self, mocker, tmpdir):
        mock_uuid = mocker.patch("embedchain.telemetry.posthog.uuid.uuid4")
        mock_uuid.return_value = "unique_user_id"
        config_file = tmpdir.join("config.json")
        mocker.patch("embedchain.telemetry.posthog.CONFIG_FILE", str(config_file))
        telemetry = AnonymousTelemetry()

        user_id = telemetry.get_user_id()
        assert user_id == "unique_user_id"
        assert config_file.read() == '{"user_id": "unique_user_id"}'

    def test_capture(self, mocker):
        # Enable telemetry specifically for this test
        os.environ["EC_TELEMETRY"] = "true"
        mock_posthog = mocker.patch("embedchain.telemetry.posthog.Posthog")
        telemetry = AnonymousTelemetry()
        event_name = "test_event"
        properties = {"key": "value"}
        telemetry.capture(event_name, properties)

        mock_posthog.assert_called_once_with(
            project_api_key=telemetry.project_api_key,
            host=telemetry.host,
        )
        mock_posthog.return_value.capture.assert_called_once_with(
            telemetry.user_id,
            event_name,
            properties,
        )

    def test_capture_with_exception(self, mocker, caplog):
        mock_posthog = mocker.patch("embedchain.telemetry.posthog.Posthog")
        mock_posthog.return_value.capture.side_effect = Exception("Test Exception")
        telemetry = AnonymousTelemetry()
        event_name = "test_event"
        properties = {"key": "value"}
        with caplog.at_level(logging.ERROR):
            telemetry.capture(event_name, properties)
        assert "Failed to send telemetry event" in caplog.text



# Source: /content/embedchain/tests/embedchain/test_embedchain.py
import os
import unittest
from unittest.mock import patch

from embedchain import App
from embedchain.config import AppConfig, ChromaDbConfig


class TestChromaDbHostsLoglevel(unittest.TestCase):
    os.environ["OPENAI_API_KEY"] = "test_key"

    @patch("chromadb.api.models.Collection.Collection.add")
    @patch("embedchain.embedchain.EmbedChain.retrieve_from_database")
    @patch("embedchain.llm.base.BaseLlm.get_answer_from_llm")
    @patch("embedchain.llm.base.BaseLlm.get_llm_model_answer")
    def test_whole_app(
        self,
        _mock_add,
        _mock_ec_retrieve_from_database,
        _mock_get_answer_from_llm,
        mock_ec_get_llm_model_answer,
    ):
        """
        Test if the `App` instance is initialized without a config that does not contain default hosts and ports.
        """
        config = AppConfig(log_level="DEBUG", collect_metrics=False)

        app = App(config)

        knowledge = "lorem ipsum dolor sit amet, consectetur adipiscing"

        app.add(knowledge, data_type="text")

        app.query("What text did I give you?")
        app.chat("What text did I give you?")

        self.assertEqual(mock_ec_get_llm_model_answer.call_args[1]["documents"], [knowledge])

    def test_add_after_reset(self):
        """
        Test if the `App` instance is correctly reconstructed after a reset.
        """
        config = AppConfig(log_level="DEBUG", collect_metrics=False)
        chroma_config = {"allow_reset": True}
        app = App(config=config, db_config=ChromaDbConfig(**chroma_config))
        app.reset()

        # Make sure the client is still healthy
        app.db.client.heartbeat()
        # Make sure the collection exists, and can be added to
        app.db.collection.add(
            embeddings=[[1.1, 2.3, 3.2], [4.5, 6.9, 4.4], [1.1, 2.3, 3.2]],
            metadatas=[
                {"chapter": "3", "verse": "16"},
                {"chapter": "3", "verse": "5"},
                {"chapter": "29", "verse": "11"},
            ],
            ids=["id1", "id2", "id3"],
        )

        app.reset()



# Source: /content/embedchain/tests/embedchain/test_add.py
import os

import pytest

from embedchain import App
from embedchain.config import AddConfig, AppConfig, ChunkerConfig
from embedchain.models.data_type import DataType

os.environ["OPENAI_API_KEY"] = "test_key"


@pytest.fixture
def app(mocker):
    mocker.patch("chromadb.api.models.Collection.Collection.add")
    return App(config=AppConfig(collect_metrics=False))


def test_add(app):
    app.add("https://example.com", metadata={"foo": "bar"})
    assert app.user_asks == [["https://example.com", "web_page", {"foo": "bar"}]]


# TODO: Make this test faster by generating a sitemap locally rather than using a remote one
# def test_add_sitemap(app):
#     app.add("https://www.google.com/sitemap.xml", metadata={"foo": "bar"})
#     assert app.user_asks == [["https://www.google.com/sitemap.xml", "sitemap", {"foo": "bar"}]]


def test_add_forced_type(app):
    data_type = "text"
    app.add("https://example.com", data_type=data_type, metadata={"foo": "bar"})
    assert app.user_asks == [["https://example.com", data_type, {"foo": "bar"}]]


def test_dry_run(app):
    chunker_config = ChunkerConfig(chunk_size=1, chunk_overlap=0)
    text = """0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"""

    result = app.add(source=text, config=AddConfig(chunker=chunker_config), dry_run=True)

    chunks = result["chunks"]
    metadata = result["metadata"]
    count = result["count"]
    data_type = result["type"]

    assert len(chunks) == len(text)
    assert count == len(text)
    assert data_type == DataType.TEXT
    for item in metadata:
        assert isinstance(item, dict)
        assert "local" in item["url"]
        assert "text" in item["data_type"]



# Source: /content/embedchain/tests/embedchain/test_utils.py
import tempfile
import unittest
from unittest.mock import patch

from embedchain.models.data_type import DataType
from embedchain.utils import detect_datatype


class TestApp(unittest.TestCase):
    """Test that the datatype detection is working, based on the input."""

    def test_detect_datatype_youtube(self):
        self.assertEqual(detect_datatype("https://www.youtube.com/watch?v=dQw4w9WgXcQ"), DataType.YOUTUBE_VIDEO)
        self.assertEqual(detect_datatype("https://m.youtube.com/watch?v=dQw4w9WgXcQ"), DataType.YOUTUBE_VIDEO)
        self.assertEqual(
            detect_datatype("https://www.youtube-nocookie.com/watch?v=dQw4w9WgXcQ"), DataType.YOUTUBE_VIDEO
        )
        self.assertEqual(detect_datatype("https://vid.plus/watch?v=dQw4w9WgXcQ"), DataType.YOUTUBE_VIDEO)
        self.assertEqual(detect_datatype("https://youtu.be/dQw4w9WgXcQ"), DataType.YOUTUBE_VIDEO)

    def test_detect_datatype_local_file(self):
        self.assertEqual(detect_datatype("file:///home/user/file.txt"), DataType.WEB_PAGE)

    def test_detect_datatype_pdf(self):
        self.assertEqual(detect_datatype("https://www.example.com/document.pdf"), DataType.PDF_FILE)

    def test_detect_datatype_local_pdf(self):
        self.assertEqual(detect_datatype("file:///home/user/document.pdf"), DataType.PDF_FILE)

    def test_detect_datatype_xml(self):
        self.assertEqual(detect_datatype("https://www.example.com/sitemap.xml"), DataType.SITEMAP)

    def test_detect_datatype_local_xml(self):
        self.assertEqual(detect_datatype("file:///home/user/sitemap.xml"), DataType.SITEMAP)

    def test_detect_datatype_docx(self):
        self.assertEqual(detect_datatype("https://www.example.com/document.docx"), DataType.DOCX)

    def test_detect_datatype_local_docx(self):
        self.assertEqual(detect_datatype("file:///home/user/document.docx"), DataType.DOCX)

    def test_detect_data_type_json(self):
        self.assertEqual(detect_datatype("https://www.example.com/data.json"), DataType.JSON)

    def test_detect_data_type_local_json(self):
        self.assertEqual(detect_datatype("file:///home/user/data.json"), DataType.JSON)

    @patch("os.path.isfile")
    def test_detect_datatype_regular_filesystem_docx(self, mock_isfile):
        with tempfile.NamedTemporaryFile(suffix=".docx", delete=True) as tmp:
            mock_isfile.return_value = True
            self.assertEqual(detect_datatype(tmp.name), DataType.DOCX)

    def test_detect_datatype_docs_site(self):
        self.assertEqual(detect_datatype("https://docs.example.com"), DataType.DOCS_SITE)

    def test_detect_datatype_docs_sitein_path(self):
        self.assertEqual(detect_datatype("https://www.example.com/docs/index.html"), DataType.DOCS_SITE)
        self.assertNotEqual(detect_datatype("file:///var/www/docs/index.html"), DataType.DOCS_SITE)  # NOT equal

    def test_detect_datatype_web_page(self):
        self.assertEqual(detect_datatype("https://nav.al/agi"), DataType.WEB_PAGE)

    def test_detect_datatype_invalid_url(self):
        self.assertEqual(detect_datatype("not a url"), DataType.TEXT)

    def test_detect_datatype_qna_pair(self):
        self.assertEqual(
            detect_datatype(("Question?", "Answer. Content of the string is irrelevant.")), DataType.QNA_PAIR
        )  #

    def test_detect_datatype_qna_pair_types(self):
        """Test that a QnA pair needs to be a tuple of length two, and both items have to be strings."""
        with self.assertRaises(TypeError):
            self.assertNotEqual(
                detect_datatype(("How many planets are in our solar system?", 8)), DataType.QNA_PAIR
            )  # NOT equal

    def test_detect_datatype_text(self):
        self.assertEqual(detect_datatype("Just some text."), DataType.TEXT)

    def test_detect_datatype_non_string_error(self):
        """Test type error if the value passed is not a string, and not a valid non-string data_type"""
        with self.assertRaises(TypeError):
            detect_datatype(["foo", "bar"])

    @patch("os.path.isfile")
    def test_detect_datatype_regular_filesystem_file_not_detected(self, mock_isfile):
        """Test error if a valid file is referenced, but it isn't a valid data_type"""
        with tempfile.NamedTemporaryFile(suffix=".txt", delete=True) as tmp:
            mock_isfile.return_value = True
            with self.assertRaises(ValueError):
                detect_datatype(tmp.name)

    def test_detect_datatype_regular_filesystem_no_file(self):
        """Test that if a filepath is not actually an existing file, it is not handled as a file path."""
        self.assertEqual(detect_datatype("/var/not-an-existing-file.txt"), DataType.TEXT)

    def test_doc_examples_quickstart(self):
        """Test examples used in the documentation."""
        self.assertEqual(detect_datatype("https://en.wikipedia.org/wiki/Elon_Musk"), DataType.WEB_PAGE)
        self.assertEqual(detect_datatype("https://www.tesla.com/elon-musk"), DataType.WEB_PAGE)

    def test_doc_examples_introduction(self):
        """Test examples used in the documentation."""
        self.assertEqual(detect_datatype("https://www.youtube.com/watch?v=3qHkcs3kG44"), DataType.YOUTUBE_VIDEO)
        self.assertEqual(
            detect_datatype(
                "https://navalmanack.s3.amazonaws.com/Eric-Jorgenson_The-Almanack-of-Naval-Ravikant_Final.pdf"
            ),
            DataType.PDF_FILE,
        )
        self.assertEqual(detect_datatype("https://nav.al/feedback"), DataType.WEB_PAGE)

    def test_doc_examples_app_types(self):
        """Test examples used in the documentation."""
        self.assertEqual(detect_datatype("https://www.youtube.com/watch?v=Ff4fRgnuFgQ"), DataType.YOUTUBE_VIDEO)
        self.assertEqual(detect_datatype("https://en.wikipedia.org/wiki/Mark_Zuckerberg"), DataType.WEB_PAGE)

    def test_doc_examples_configuration(self):
        """Test examples used in the documentation."""
        import subprocess
        import sys

        subprocess.check_call([sys.executable, "-m", "pip", "install", "wikipedia"])
        import wikipedia

        page = wikipedia.page("Albert Einstein")
        # TODO: Add a wikipedia type, so wikipedia is a dependency and we don't need this slow test.
        # (timings: import: 1.4s, fetch wiki: 0.7s)
        self.assertEqual(detect_datatype(page.content), DataType.TEXT)


if __name__ == "__main__":
    unittest.main()



# Source: /content/embedchain/tests/loaders/test_docx_file.py
import hashlib
from unittest.mock import MagicMock, patch

import pytest

from embedchain.loaders.docx_file import DocxFileLoader


@pytest.fixture
def mock_docx2txt_loader():
    with patch("embedchain.loaders.docx_file.Docx2txtLoader") as mock_loader:
        yield mock_loader


@pytest.fixture
def docx_file_loader():
    return DocxFileLoader()


def test_load_data(mock_docx2txt_loader, docx_file_loader):
    mock_url = "mock_docx_file.docx"

    mock_loader = MagicMock()
    mock_loader.load.return_value = [MagicMock(page_content="Sample Docx Content", metadata={"url": "local"})]

    mock_docx2txt_loader.return_value = mock_loader

    result = docx_file_loader.load_data(mock_url)

    assert "doc_id" in result
    assert "data" in result

    expected_content = "Sample Docx Content"
    assert result["data"][0]["content"] == expected_content

    assert result["data"][0]["meta_data"]["url"] == "local"

    expected_doc_id = hashlib.sha256((expected_content + mock_url).encode()).hexdigest()
    assert result["doc_id"] == expected_doc_id



# Source: /content/embedchain/tests/loaders/test_local_text.py
import hashlib

import pytest

from embedchain.loaders.local_text import LocalTextLoader


@pytest.fixture
def text_loader():
    return LocalTextLoader()


def test_load_data(text_loader):
    mock_content = "This is a sample text content."

    result = text_loader.load_data(mock_content)

    assert "doc_id" in result
    assert "data" in result

    url = "local"
    assert result["data"][0]["content"] == mock_content

    assert result["data"][0]["meta_data"]["url"] == url

    expected_doc_id = hashlib.sha256((mock_content + url).encode()).hexdigest()
    assert result["doc_id"] == expected_doc_id



# Source: /content/embedchain/tests/loaders/test_mdx.py
import hashlib
from unittest.mock import mock_open, patch

import pytest

from embedchain.loaders.mdx import MdxLoader


@pytest.fixture
def mdx_loader():
    return MdxLoader()


def test_load_data(mdx_loader):
    mock_content = "Sample MDX Content"

    # Mock open function to simulate file reading
    with patch("builtins.open", mock_open(read_data=mock_content)):
        url = "mock_file.mdx"
        result = mdx_loader.load_data(url)

        assert "doc_id" in result
        assert "data" in result

        assert result["data"][0]["content"] == mock_content

        assert result["data"][0]["meta_data"]["url"] == url

        expected_doc_id = hashlib.sha256((mock_content + url).encode()).hexdigest()
        assert result["doc_id"] == expected_doc_id



# Source: /content/embedchain/tests/loaders/test_pdf_file.py
import pytest
from langchain.schema import Document


def test_load_data(loader, mocker):
    mocked_pypdfloader = mocker.patch("embedchain.loaders.pdf_file.PyPDFLoader")
    mocked_pypdfloader.return_value.load_and_split.return_value = [
        Document(page_content="Page 0 Content", metadata={"source": "example.pdf", "page": 0}),
        Document(page_content="Page 1 Content", metadata={"source": "example.pdf", "page": 1}),
    ]

    mock_sha256 = mocker.patch("embedchain.loaders.docs_site_loader.hashlib.sha256")
    doc_id = "mocked_hash"
    mock_sha256.return_value.hexdigest.return_value = doc_id

    result = loader.load_data("dummy_url")
    assert result["doc_id"] is doc_id
    assert result["data"] == [
        {"content": "Page 0 Content", "meta_data": {"source": "example.pdf", "page": 0, "url": "dummy_url"}},
        {"content": "Page 1 Content", "meta_data": {"source": "example.pdf", "page": 1, "url": "dummy_url"}},
    ]


def test_load_data_fails_to_find_data(loader, mocker):
    mocked_pypdfloader = mocker.patch("embedchain.loaders.pdf_file.PyPDFLoader")
    mocked_pypdfloader.return_value.load_and_split.return_value = []

    with pytest.raises(ValueError):
        loader.load_data("dummy_url")


@pytest.fixture
def loader():
    from embedchain.loaders.pdf_file import PdfFileLoader

    return PdfFileLoader()



# Source: /content/embedchain/tests/loaders/test_youtube_video.py
import hashlib
from unittest.mock import MagicMock, Mock, patch

import pytest

from embedchain.loaders.youtube_video import YoutubeVideoLoader


@pytest.fixture
def youtube_video_loader():
    return YoutubeVideoLoader()


def test_load_data(youtube_video_loader):
    video_url = "https://www.youtube.com/watch?v=VIDEO_ID"
    mock_loader = Mock()
    mock_page_content = "This is a YouTube video content."
    mock_loader.load.return_value = [
        MagicMock(
            page_content=mock_page_content,
            metadata={"url": video_url, "title": "Test Video"},
        )
    ]

    with patch("embedchain.loaders.youtube_video.YoutubeLoader.from_youtube_url", return_value=mock_loader):
        result = youtube_video_loader.load_data(video_url)

    expected_doc_id = hashlib.sha256((mock_page_content + video_url).encode()).hexdigest()

    assert result["doc_id"] == expected_doc_id

    expected_data = [
        {
            "content": "This is a YouTube video content.",
            "meta_data": {"url": video_url, "title": "Test Video"},
        }
    ]

    assert result["data"] == expected_data


def test_load_data_with_empty_doc(youtube_video_loader):
    video_url = "https://www.youtube.com/watch?v=VIDEO_ID"
    mock_loader = Mock()
    mock_loader.load.return_value = []

    with patch("embedchain.loaders.youtube_video.YoutubeLoader.from_youtube_url", return_value=mock_loader):
        with pytest.raises(ValueError):
            youtube_video_loader.load_data(video_url)



# Source: /content/embedchain/tests/loaders/test_xml.py
import tempfile

import pytest

from embedchain.loaders.xml import XmlLoader

# Taken from https://github.com/langchain-ai/langchain/blob/master/libs/langchain/tests/integration_tests/examples/factbook.xml
SAMPLE_XML = """<?xml version="1.0" encoding="UTF-8"?>
<factbook>
  <country>
    <name>United States</name>
    <capital>Washington, DC</capital>
    <leader>Joe Biden</leader>
    <sport>Baseball</sport>
  </country>
  <country>
    <name>Canada</name>
    <capital>Ottawa</capital>
    <leader>Justin Trudeau</leader>
    <sport>Hockey</sport>
  </country>
  <country>
    <name>France</name>
    <capital>Paris</capital>
    <leader>Emmanuel Macron</leader>
    <sport>Soccer</sport>
  </country>
  <country>
    <name>Trinidad &amp; Tobado</name>
    <capital>Port of Spain</capital>
    <leader>Keith Rowley</leader>
    <sport>Track &amp; Field</sport>
  </country>
</factbook>"""


@pytest.mark.parametrize("xml", [SAMPLE_XML])
def test_load_data(xml: str):
    """
    Test XML loader

    Tests that XML file is loaded, metadata is correct and content is correct
    """
    # Creating temporary XML file
    with tempfile.NamedTemporaryFile(mode="w+") as tmpfile:
        tmpfile.write(xml)

        tmpfile.seek(0)
        filename = tmpfile.name

        # Loading CSV using XmlLoader
        loader = XmlLoader()
        result = loader.load_data(filename)
        data = result["data"]

        # Assertions
        assert len(data) == 1
        assert "United States Washington, DC Joe Biden" in data[0]["content"]
        assert "Canada Ottawa Justin Trudeau" in data[0]["content"]
        assert "France Paris Emmanuel Macron" in data[0]["content"]
        assert "Trinidad & Tobado Port of Spain Keith Rowley" in data[0]["content"]
        assert data[0]["meta_data"]["url"] == filename



# Source: /content/embedchain/tests/loaders/test_notion.py
import hashlib
import os
from unittest.mock import Mock, patch

import pytest

from embedchain.loaders.notion import NotionLoader


@pytest.fixture
def notion_loader():
    with patch.dict(os.environ, {"NOTION_INTEGRATION_TOKEN": "test_notion_token"}):
        yield NotionLoader()


def test_load_data(notion_loader):
    source = "https://www.notion.so/Test-Page-1234567890abcdef1234567890abcdef"
    mock_text = "This is a test page."
    expected_doc_id = hashlib.sha256((mock_text + source).encode()).hexdigest()
    expected_data = [
        {
            "content": mock_text,
            "meta_data": {"url": "notion-12345678-90ab-cdef-1234-567890abcdef"},  # formatted_id
        }
    ]

    mock_page = Mock()
    mock_page.text = mock_text
    mock_documents = [mock_page]

    with patch("embedchain.loaders.notion.NotionPageReader") as mock_reader:
        mock_reader.return_value.load_data.return_value = mock_documents
        result = notion_loader.load_data(source)

    assert result["doc_id"] == expected_doc_id
    assert result["data"] == expected_data



# Source: /content/embedchain/tests/loaders/test_docs_site.py
import hashlib
from unittest.mock import Mock, patch

import pytest
from requests import Response

from embedchain.loaders.docs_site_loader import DocsSiteLoader


@pytest.fixture
def mock_requests_get():
    with patch("requests.get") as mock_get:
        yield mock_get


@pytest.fixture
def docs_site_loader():
    return DocsSiteLoader()


def test_get_child_links_recursive(mock_requests_get, docs_site_loader):
    mock_response = Mock()
    mock_response.status_code = 200
    mock_response.text = """
        <html>
            <a href="/page1">Page 1</a>
            <a href="/page2">Page 2</a>
        </html>
    """
    mock_requests_get.return_value = mock_response

    docs_site_loader._get_child_links_recursive("https://example.com")

    assert len(docs_site_loader.visited_links) == 2
    assert "https://example.com/page1" in docs_site_loader.visited_links
    assert "https://example.com/page2" in docs_site_loader.visited_links


def test_get_child_links_recursive_status_not_200(mock_requests_get, docs_site_loader):
    mock_response = Mock()
    mock_response.status_code = 404
    mock_requests_get.return_value = mock_response

    docs_site_loader._get_child_links_recursive("https://example.com")

    assert len(docs_site_loader.visited_links) == 0


def test_get_all_urls(mock_requests_get, docs_site_loader):
    mock_response = Mock()
    mock_response.status_code = 200
    mock_response.text = """
        <html>
            <a href="/page1">Page 1</a>
            <a href="/page2">Page 2</a>
            <a href="https://example.com/external">External</a>
        </html>
    """
    mock_requests_get.return_value = mock_response

    all_urls = docs_site_loader._get_all_urls("https://example.com")

    assert len(all_urls) == 3
    assert "https://example.com/page1" in all_urls
    assert "https://example.com/page2" in all_urls
    assert "https://example.com/external" in all_urls


def test_load_data_from_url(mock_requests_get, docs_site_loader):
    mock_response = Mock()
    mock_response.status_code = 200
    mock_response.content = """
        <html>
            <nav>
                <h1>Navigation</h1>
            </nav>
            <article class="bd-article">
                <p>Article Content</p>
            </article>
        </html>
    """.encode()
    mock_requests_get.return_value = mock_response

    data = docs_site_loader._load_data_from_url("https://example.com/page1")

    assert len(data) == 1
    assert data[0]["content"] == "Article Content"
    assert data[0]["meta_data"]["url"] == "https://example.com/page1"


def test_load_data_from_url_status_not_200(mock_requests_get, docs_site_loader):
    mock_response = Mock()
    mock_response.status_code = 404
    mock_requests_get.return_value = mock_response

    data = docs_site_loader._load_data_from_url("https://example.com/page1")

    assert data == []
    assert len(data) == 0


def test_load_data(mock_requests_get, docs_site_loader):
    mock_response = Response()
    mock_response.status_code = 200
    mock_response._content = """
        <html>
            <a href="/page1">Page 1</a>
            <a href="/page2">Page 2</a>
        """.encode()
    mock_requests_get.return_value = mock_response

    url = "https://example.com"
    data = docs_site_loader.load_data(url)
    expected_doc_id = hashlib.sha256((" ".join(docs_site_loader.visited_links) + url).encode()).hexdigest()

    assert len(data["data"]) == 2
    assert data["doc_id"] == expected_doc_id


def test_if_response_status_not_200(mock_requests_get, docs_site_loader):
    mock_response = Response()
    mock_response.status_code = 404
    mock_requests_get.return_value = mock_response

    url = "https://example.com"
    data = docs_site_loader.load_data(url)
    expected_doc_id = hashlib.sha256((" ".join(docs_site_loader.visited_links) + url).encode()).hexdigest()

    assert len(data["data"]) == 0
    assert data["doc_id"] == expected_doc_id



# Source: /content/embedchain/tests/loaders/test_gmail.py
import pytest
from llama_hub.readwise.base import Document

from embedchain.loaders.gmail import GmailLoader


@pytest.fixture
def mock_quopri(mocker):
    return mocker.patch("embedchain.loaders.gmail.quopri.decodestring", return_value=b"your_test_decoded_string")


@pytest.fixture
def mock_beautifulsoup(mocker):
    return mocker.patch("embedchain.loaders.gmail.BeautifulSoup", return_value=mocker.MagicMock())


@pytest.fixture
def gmail_loader(mock_quopri, mock_beautifulsoup):
    return GmailLoader()


def test_load_data_file_not_found(gmail_loader, mocker):
    with pytest.raises(FileNotFoundError):
        with mocker.patch("os.path.isfile", return_value=False):
            gmail_loader.load_data("your_query")


@pytest.mark.skip(reason="TODO: Fix this test. Failing due to some googleapiclient import issue.")
def test_load_data(gmail_loader, mocker):
    mock_gmail_reader_instance = mocker.MagicMock()
    text = "your_test_email_text"
    metadata = {
        "id": "your_test_id",
        "snippet": "your_test_snippet",
    }
    mock_gmail_reader_instance.load_data.return_value = [Document(text=text, extra_info=metadata)]

    with mocker.patch("os.path.isfile", return_value=True):
        response_data = gmail_loader.load_data("your_query")

    assert "doc_id" in response_data
    assert "data" in response_data
    assert isinstance(response_data["doc_id"], str)
    assert isinstance(response_data["data"], list)



# Source: /content/embedchain/tests/loaders/test_docs_site_loader.py
import pytest
import responses
from bs4 import BeautifulSoup


@pytest.mark.parametrize(
    "ignored_tag",
    [
        "<nav>This is a navigation bar.</nav>",
        "<aside>This is an aside.</aside>",
        "<form>This is a form.</form>",
        "<header>This is a header.</header>",
        "<noscript>This is a noscript.</noscript>",
        "<svg>This is an SVG.</svg>",
        "<canvas>This is a canvas.</canvas>",
        "<footer>This is a footer.</footer>",
        "<script>This is a script.</script>",
        "<style>This is a style.</style>",
    ],
    ids=["nav", "aside", "form", "header", "noscript", "svg", "canvas", "footer", "script", "style"],
)
@pytest.mark.parametrize(
    "selectee",
    [
        """
<article class="bd-article">
    <h2>Article Title</h2>
    <p>Article content goes here.</p>
    {ignored_tag}
</article>
""",
        """
<article role="main">
    <h2>Main Article Title</h2>
    <p>Main article content goes here.</p>
    {ignored_tag}
</article>
""",
        """
<div class="md-content">
    <h2>Markdown Content</h2>
    <p>Markdown content goes here.</p>
    {ignored_tag}
</div>
""",
        """
<div role="main">
    <h2>Main Content</h2>
    <p>Main content goes here.</p>
    {ignored_tag}
</div>
""",
        """
<div class="container">
    <h2>Container</h2>
    <p>Container content goes here.</p>
    {ignored_tag}
</div>
        """,
        """
<div class="section">
    <h2>Section</h2>
    <p>Section content goes here.</p>
    {ignored_tag}
</div>
        """,
        """
<article>
    <h2>Generic Article</h2>
    <p>Generic article content goes here.</p>
    {ignored_tag}
</article>
        """,
        """
<main>
    <h2>Main Content</h2>
    <p>Main content goes here.</p>
    {ignored_tag}
</main>
""",
    ],
    ids=[
        "article.bd-article",
        'article[role="main"]',
        "div.md-content",
        'div[role="main"]',
        "div.container",
        "div.section",
        "article",
        "main",
    ],
)
def test_load_data_gets_by_selectors_and_ignored_tags(selectee, ignored_tag, loader, mocked_responses, mocker):
    child_url = "https://docs.embedchain.ai/quickstart"
    selectee = selectee.format(ignored_tag=ignored_tag)
    html_body = """
<!DOCTYPE html>
<html lang="en">
<body>
    {selectee}
</body>
</html>
"""
    html_body = html_body.format(selectee=selectee)
    mocked_responses.get(child_url, body=html_body, status=200, content_type="text/html")

    url = "https://docs.embedchain.ai/"
    html_body = """
<!DOCTYPE html>
<html lang="en">
<body>
    <li><a href="/quickstart">Quickstart</a></li>
</body>
</html>
"""
    mocked_responses.get(url, body=html_body, status=200, content_type="text/html")

    mock_sha256 = mocker.patch("embedchain.loaders.docs_site_loader.hashlib.sha256")
    doc_id = "mocked_hash"
    mock_sha256.return_value.hexdigest.return_value = doc_id

    result = loader.load_data(url)
    selector_soup = BeautifulSoup(selectee, "html.parser")
    expected_content = " ".join((selector_soup.select_one("h2").get_text(), selector_soup.select_one("p").get_text()))
    assert result["doc_id"] == doc_id
    assert result["data"] == [
        {
            "content": expected_content,
            "meta_data": {"url": "https://docs.embedchain.ai/quickstart"},
        }
    ]


def test_load_data_gets_child_links_recursively(loader, mocked_responses, mocker):
    child_url = "https://docs.embedchain.ai/quickstart"
    html_body = """
<!DOCTYPE html>
<html lang="en">
<body>
    <li><a href="/">..</a></li>
    <li><a href="/quickstart">.</a></li>
</body>
</html>
"""
    mocked_responses.get(child_url, body=html_body, status=200, content_type="text/html")

    child_url = "https://docs.embedchain.ai/introduction"
    html_body = """
<!DOCTYPE html>
<html lang="en">
<body>
    <li><a href="/">..</a></li>
    <li><a href="/introduction">.</a></li>
</body>
</html>
"""
    mocked_responses.get(child_url, body=html_body, status=200, content_type="text/html")

    url = "https://docs.embedchain.ai/"
    html_body = """
<!DOCTYPE html>
<html lang="en">
<body>
    <li><a href="/quickstart">Quickstart</a></li>
    <li><a href="/introduction">Introduction</a></li>
</body>
</html>
"""
    mocked_responses.get(url, body=html_body, status=200, content_type="text/html")

    mock_sha256 = mocker.patch("embedchain.loaders.docs_site_loader.hashlib.sha256")
    doc_id = "mocked_hash"
    mock_sha256.return_value.hexdigest.return_value = doc_id

    result = loader.load_data(url)
    assert result["doc_id"] == doc_id
    expected_data = [
        {"content": "..\n.", "meta_data": {"url": "https://docs.embedchain.ai/quickstart"}},
        {"content": "..\n.", "meta_data": {"url": "https://docs.embedchain.ai/introduction"}},
    ]
    assert all(item in expected_data for item in result["data"])


def test_load_data_fails_to_fetch_website(loader, mocked_responses, mocker):
    child_url = "https://docs.embedchain.ai/introduction"
    mocked_responses.get(child_url, status=404)

    url = "https://docs.embedchain.ai/"
    html_body = """
<!DOCTYPE html>
<html lang="en">
<body>
    <li><a href="/introduction">Introduction</a></li>
</body>
</html>
"""
    mocked_responses.get(url, body=html_body, status=200, content_type="text/html")

    mock_sha256 = mocker.patch("embedchain.loaders.docs_site_loader.hashlib.sha256")
    doc_id = "mocked_hash"
    mock_sha256.return_value.hexdigest.return_value = doc_id

    result = loader.load_data(url)
    assert result["doc_id"] is doc_id
    assert result["data"] == []


@pytest.fixture
def loader():
    from embedchain.loaders.docs_site_loader import DocsSiteLoader

    return DocsSiteLoader()


@pytest.fixture
def mocked_responses():
    with responses.RequestsMock() as rsps:
        yield rsps



# Source: /content/embedchain/tests/loaders/test_web_page.py
import hashlib
from unittest.mock import Mock, patch

import pytest

from embedchain.loaders.web_page import WebPageLoader


@pytest.fixture
def web_page_loader():
    return WebPageLoader()


def test_load_data(web_page_loader):
    page_url = "https://example.com/page"
    mock_response = Mock()
    mock_response.status_code = 200
    mock_response.content = """
        <html>
            <head>
                <title>Test Page</title>
            </head>
            <body>
                <div id="content">
                    <p>This is some test content.</p>
                </div>
            </body>
        </html>
    """
    with patch("embedchain.loaders.web_page.requests.get", return_value=mock_response):
        result = web_page_loader.load_data(page_url)

    content = web_page_loader._get_clean_content(mock_response.content, page_url)
    expected_doc_id = hashlib.sha256((content + page_url).encode()).hexdigest()
    assert result["doc_id"] == expected_doc_id

    expected_data = [
        {
            "content": content,
            "meta_data": {
                "url": page_url,
            },
        }
    ]

    assert result["data"] == expected_data


def test_get_clean_content_excludes_unnecessary_info(web_page_loader):
    mock_html = """
        <html>
        <head>
            <title>Sample HTML</title>
            <style>
                /* Stylesheet to be excluded */
                .elementor-location-header {
                    background-color: #f0f0f0;
                }
            </style>
        </head>
        <body>
            <header id="header">Header Content</header>
            <nav class="nav">Nav Content</nav>
            <aside>Aside Content</aside>
            <form>Form Content</form>
            <main>Main Content</main>
            <footer class="footer">Footer Content</footer>
            <script>Some Script</script>
            <noscript>NoScript Content</noscript>
            <svg>SVG Content</svg>
            <canvas>Canvas Content</canvas>
            
            <div id="sidebar">Sidebar Content</div>
            <div id="main-navigation">Main Navigation Content</div>
            <div id="menu-main-menu">Menu Main Menu Content</div>
            
            <div class="header-sidebar-wrapper">Header Sidebar Wrapper Content</div>
            <div class="blog-sidebar-wrapper">Blog Sidebar Wrapper Content</div>
            <div class="related-posts">Related Posts Content</div>
        </body>
        </html>
    """

    tags_to_exclude = [
        "nav",
        "aside",
        "form",
        "header",
        "noscript",
        "svg",
        "canvas",
        "footer",
        "script",
        "style",
    ]
    ids_to_exclude = ["sidebar", "main-navigation", "menu-main-menu"]
    classes_to_exclude = [
        "elementor-location-header",
        "navbar-header",
        "nav",
        "header-sidebar-wrapper",
        "blog-sidebar-wrapper",
        "related-posts",
    ]

    content = web_page_loader._get_clean_content(mock_html, "https://example.com/page")

    for tag in tags_to_exclude:
        assert tag not in content

    for id in ids_to_exclude:
        assert id not in content

    for class_name in classes_to_exclude:
        assert class_name not in content

    assert len(content) > 0



# Source: /content/embedchain/tests/loaders/test_openapi.py
import pytest

from embedchain.loaders.openapi import OpenAPILoader


@pytest.fixture
def openapi_loader():
    return OpenAPILoader()


def test_load_data(openapi_loader, mocker):
    mocker.patch("builtins.open", mocker.mock_open(read_data="key1: value1\nkey2: value2"))

    mocker.patch("hashlib.sha256", return_value=mocker.Mock(hexdigest=lambda: "mock_hash"))

    file_path = "configs/openai_openapi.yaml"
    result = openapi_loader.load_data(file_path)

    expected_doc_id = "mock_hash"
    expected_data = [
        {"content": "key1: value1", "meta_data": {"url": file_path, "row": 1}},
        {"content": "key2: value2", "meta_data": {"url": file_path, "row": 2}},
    ]

    assert result["doc_id"] == expected_doc_id
    assert result["data"] == expected_data



# Source: /content/embedchain/tests/loaders/test_json.py
import hashlib
from unittest.mock import patch

from langchain.docstore.document import Document
from langchain.document_loaders.json_loader import \
    JSONLoader as LangchainJSONLoader

from embedchain.loaders.json import JSONLoader


def test_load_data():
    mock_document = [
        Document(page_content="content1", metadata={"seq_num": 1}),
        Document(page_content="content2", metadata={"seq_num": 2}),
    ]
    with patch.object(LangchainJSONLoader, "load", return_value=mock_document):
        content = "temp.json"

        result = JSONLoader.load_data(content)

        assert "doc_id" in result
        assert "data" in result

        expected_data = [
            {"content": "content1", "meta_data": {"url": content, "row": 1}},
            {"content": "content2", "meta_data": {"url": content, "row": 2}},
        ]

        assert result["data"] == expected_data

        expected_doc_id = hashlib.sha256((content + ", ".join(["content1", "content2"])).encode()).hexdigest()
        assert result["doc_id"] == expected_doc_id



# Source: /content/embedchain/tests/loaders/test_local_qna_pair.py
import hashlib

import pytest

from embedchain.loaders.local_qna_pair import LocalQnaPairLoader


@pytest.fixture
def qna_pair_loader():
    return LocalQnaPairLoader()


def test_load_data(qna_pair_loader):
    question = "What is the capital of France?"
    answer = "The capital of France is Paris."

    content = (question, answer)
    result = qna_pair_loader.load_data(content)

    assert "doc_id" in result
    assert "data" in result
    url = "local"

    expected_content = f"Q: {question}\nA: {answer}"
    assert result["data"][0]["content"] == expected_content

    assert result["data"][0]["meta_data"]["url"] == url

    assert result["data"][0]["meta_data"]["question"] == question

    expected_doc_id = hashlib.sha256((expected_content + url).encode()).hexdigest()
    assert result["doc_id"] == expected_doc_id



# Source: /content/embedchain/tests/loaders/test_csv.py
import csv
import os
import pathlib
import tempfile
from unittest.mock import MagicMock, patch

import pytest

from embedchain.loaders.csv import CsvLoader


@pytest.mark.parametrize("delimiter", [",", "\t", ";", "|"])
def test_load_data(delimiter):
    """
    Test csv loader

    Tests that file is loaded, metadata is correct and content is correct
    """
    # Creating temporary CSV file
    with tempfile.NamedTemporaryFile(mode="w+", newline="", delete=False) as tmpfile:
        writer = csv.writer(tmpfile, delimiter=delimiter)
        writer.writerow(["Name", "Age", "Occupation"])
        writer.writerow(["Alice", "28", "Engineer"])
        writer.writerow(["Bob", "35", "Doctor"])
        writer.writerow(["Charlie", "22", "Student"])

        tmpfile.seek(0)
        filename = tmpfile.name

        # Loading CSV using CsvLoader
        loader = CsvLoader()
        result = loader.load_data(filename)
        data = result["data"]

        # Assertions
        assert len(data) == 3
        assert data[0]["content"] == "Name: Alice, Age: 28, Occupation: Engineer"
        assert data[0]["meta_data"]["url"] == filename
        assert data[0]["meta_data"]["row"] == 1
        assert data[1]["content"] == "Name: Bob, Age: 35, Occupation: Doctor"
        assert data[1]["meta_data"]["url"] == filename
        assert data[1]["meta_data"]["row"] == 2
        assert data[2]["content"] == "Name: Charlie, Age: 22, Occupation: Student"
        assert data[2]["meta_data"]["url"] == filename
        assert data[2]["meta_data"]["row"] == 3

        # Cleaning up the temporary file
        os.unlink(filename)


@pytest.mark.parametrize("delimiter", [",", "\t", ";", "|"])
def test_load_data_with_file_uri(delimiter):
    """
    Test csv loader with file URI

    Tests that file is loaded, metadata is correct and content is correct
    """
    # Creating temporary CSV file
    with tempfile.NamedTemporaryFile(mode="w+", newline="", delete=False) as tmpfile:
        writer = csv.writer(tmpfile, delimiter=delimiter)
        writer.writerow(["Name", "Age", "Occupation"])
        writer.writerow(["Alice", "28", "Engineer"])
        writer.writerow(["Bob", "35", "Doctor"])
        writer.writerow(["Charlie", "22", "Student"])

        tmpfile.seek(0)
        filename = pathlib.Path(tmpfile.name).as_uri()  # Convert path to file URI

        # Loading CSV using CsvLoader
        loader = CsvLoader()
        result = loader.load_data(filename)
        data = result["data"]

        # Assertions
        assert len(data) == 3
        assert data[0]["content"] == "Name: Alice, Age: 28, Occupation: Engineer"
        assert data[0]["meta_data"]["url"] == filename
        assert data[0]["meta_data"]["row"] == 1
        assert data[1]["content"] == "Name: Bob, Age: 35, Occupation: Doctor"
        assert data[1]["meta_data"]["url"] == filename
        assert data[1]["meta_data"]["row"] == 2
        assert data[2]["content"] == "Name: Charlie, Age: 22, Occupation: Student"
        assert data[2]["meta_data"]["url"] == filename
        assert data[2]["meta_data"]["row"] == 3

        # Cleaning up the temporary file
        os.unlink(tmpfile.name)


@pytest.mark.parametrize("content", ["ftp://example.com", "sftp://example.com", "mailto://example.com"])
def test_get_file_content(content):
    with pytest.raises(ValueError):
        loader = CsvLoader()
        loader._get_file_content(content)


@pytest.mark.parametrize("content", ["http://example.com", "https://example.com"])
def test_get_file_content_http(content):
    """
    Test _get_file_content method of CsvLoader for http and https URLs
    """

    with patch("requests.get") as mock_get:
        mock_response = MagicMock()
        mock_response.text = "Name,Age,Occupation\nAlice,28,Engineer\nBob,35,Doctor\nCharlie,22,Student"
        mock_get.return_value = mock_response

        loader = CsvLoader()
        file_content = loader._get_file_content(content)

        mock_get.assert_called_once_with(content)
        mock_response.raise_for_status.assert_called_once()
        assert file_content.read() == mock_response.text



# Source: /content/embedchain/tests/apps/test_apps.py
import os

import pytest
import yaml

from embedchain import App
from embedchain.config import (AddConfig, AppConfig, BaseEmbedderConfig,
                               BaseLlmConfig, ChromaDbConfig)
from embedchain.embedder.base import BaseEmbedder
from embedchain.llm.base import BaseLlm
from embedchain.vectordb.base import BaseVectorDB, BaseVectorDbConfig
from embedchain.vectordb.chroma import ChromaDB


@pytest.fixture
def app():
    os.environ["OPENAI_API_KEY"] = "test_api_key"
    return App()


def test_app(app):
    assert isinstance(app.llm, BaseLlm)
    assert isinstance(app.db, BaseVectorDB)
    assert isinstance(app.embedder, BaseEmbedder)


class TestConfigForAppComponents:
    def test_constructor_config(self):
        collection_name = "my-test-collection"
        app = App(db_config=ChromaDbConfig(collection_name=collection_name))
        assert app.db.config.collection_name == collection_name

    def test_component_config(self):
        collection_name = "my-test-collection"
        database = ChromaDB(config=ChromaDbConfig(collection_name=collection_name))
        app = App(db=database)
        assert app.db.config.collection_name == collection_name

    def test_different_configs_are_proper_instances(self):
        app_config = AppConfig()
        wrong_config = AddConfig()
        with pytest.raises(TypeError):
            App(config=wrong_config)

        assert isinstance(app_config, AppConfig)

        llm_config = BaseLlmConfig()
        wrong_llm_config = "wrong_llm_config"

        with pytest.raises(TypeError):
            App(llm_config=wrong_llm_config)

        assert isinstance(llm_config, BaseLlmConfig)

        db_config = BaseVectorDbConfig()
        wrong_db_config = "wrong_db_config"

        with pytest.raises(TypeError):
            App(db_config=wrong_db_config)

        assert isinstance(db_config, BaseVectorDbConfig)

        embedder_config = BaseEmbedderConfig()
        wrong_embedder_config = "wrong_embedder_config"
        with pytest.raises(TypeError):
            App(embedder_config=wrong_embedder_config)

        assert isinstance(embedder_config, BaseEmbedderConfig)

    def test_components_raises_type_error_if_not_proper_instances(self):
        wrong_llm = "wrong_llm"
        with pytest.raises(TypeError):
            App(llm=wrong_llm)

        wrong_db = "wrong_db"
        with pytest.raises(TypeError):
            App(db=wrong_db)

        wrong_embedder = "wrong_embedder"
        with pytest.raises(TypeError):
            App(embedder=wrong_embedder)


class TestAppFromConfig:
    def load_config_data(self, yaml_path):
        with open(yaml_path, "r") as file:
            return yaml.safe_load(file)

    def test_from_chroma_config(self):
        yaml_path = "configs/chroma.yaml"
        config_data = self.load_config_data(yaml_path)

        app = App.from_config(yaml_path)

        # Check if the App instance and its components were created correctly
        assert isinstance(app, App)

        # Validate the AppConfig values
        assert app.config.id == config_data["app"]["config"]["id"]
        assert app.config.collection_name == config_data["app"]["config"]["collection_name"]
        # Even though not present in the config, the default value is used
        assert app.config.collect_metrics is True

        # Validate the LLM config values
        llm_config = config_data["llm"]["config"]
        assert app.llm.config.temperature == llm_config["temperature"]
        assert app.llm.config.max_tokens == llm_config["max_tokens"]
        assert app.llm.config.top_p == llm_config["top_p"]
        assert app.llm.config.stream == llm_config["stream"]

        # Validate the VectorDB config values
        db_config = config_data["vectordb"]["config"]
        assert app.db.config.collection_name == db_config["collection_name"]
        assert app.db.config.dir == db_config["dir"]
        assert app.db.config.allow_reset == db_config["allow_reset"]

        # Validate the Embedder config values
        embedder_config = config_data["embedder"]["config"]
        assert app.embedder.config.model == embedder_config["model"]
        assert app.embedder.config.deployment_name == embedder_config["deployment_name"]

    def test_from_opensource_config(self):
        yaml_path = "configs/opensource.yaml"
        config_data = self.load_config_data(yaml_path)

        app = App.from_config(yaml_path)

        # Check if the App instance and its components were created correctly
        assert isinstance(app, App)

        # Validate the AppConfig values
        assert app.config.id == config_data["app"]["config"]["id"]
        assert app.config.collection_name == config_data["app"]["config"]["collection_name"]
        assert app.config.collect_metrics == config_data["app"]["config"]["collect_metrics"]

        # Validate the LLM config values
        llm_config = config_data["llm"]["config"]
        assert app.llm.config.model == llm_config["model"]
        assert app.llm.config.temperature == llm_config["temperature"]
        assert app.llm.config.max_tokens == llm_config["max_tokens"]
        assert app.llm.config.top_p == llm_config["top_p"]
        assert app.llm.config.stream == llm_config["stream"]

        # Validate the VectorDB config values
        db_config = config_data["vectordb"]["config"]
        assert app.db.config.collection_name == db_config["collection_name"]
        assert app.db.config.dir == db_config["dir"]
        assert app.db.config.allow_reset == db_config["allow_reset"]

        # Validate the Embedder config values
        embedder_config = config_data["embedder"]["config"]
        assert app.embedder.config.deployment_name == embedder_config["deployment_name"]



# Source: /content/embedchain/tests/chunkers/test_image_chunker.py
import unittest

from embedchain.chunkers.images import ImagesChunker
from embedchain.config import ChunkerConfig
from embedchain.models.data_type import DataType


class TestImageChunker(unittest.TestCase):
    def test_chunks(self):
        """
        Test the chunks generated by TextChunker.
        # TODO: Not a very precise test.
        """
        chunker_config = ChunkerConfig(chunk_size=1, chunk_overlap=0, length_function=len)
        chunker = ImagesChunker(config=chunker_config)
        # Data type must be set manually in the test
        chunker.set_data_type(DataType.IMAGES)

        image_path = "./tmp/image.jpeg"
        app_id = "app1"
        result = chunker.create_chunks(MockLoader(), image_path, app_id=app_id)

        expected_chunks = {
            "doc_id": f"{app_id}--123",
            "documents": [image_path],
            "embeddings": ["embedding"],
            "ids": ["140bedbf9c3f6d56a9846d2ba7088798683f4da0c248231336e6a05679e4fdfe"],
            "metadatas": [{"data_type": "images", "doc_id": f"{app_id}--123", "url": "none"}],
        }
        self.assertEqual(expected_chunks, result)

    def test_chunks_with_default_config(self):
        """
        Test the chunks generated by ImageChunker with default config.
        """
        chunker = ImagesChunker()
        # Data type must be set manually in the test
        chunker.set_data_type(DataType.IMAGES)

        image_path = "./tmp/image.jpeg"
        app_id = "app1"
        result = chunker.create_chunks(MockLoader(), image_path, app_id=app_id)

        expected_chunks = {
            "doc_id": f"{app_id}--123",
            "documents": [image_path],
            "embeddings": ["embedding"],
            "ids": ["140bedbf9c3f6d56a9846d2ba7088798683f4da0c248231336e6a05679e4fdfe"],
            "metadatas": [{"data_type": "images", "doc_id": f"{app_id}--123", "url": "none"}],
        }
        self.assertEqual(expected_chunks, result)

    def test_word_count(self):
        chunker_config = ChunkerConfig(chunk_size=1, chunk_overlap=0, length_function=len)
        chunker = ImagesChunker(config=chunker_config)
        chunker.set_data_type(DataType.IMAGES)

        document = [["ab cd", "ef gh"], ["ij kl", "mn op"]]
        result = chunker.get_word_count(document)
        self.assertEqual(result, 1)


class MockLoader:
    def load_data(self, src):
        """
        Mock loader that returns a list of data dictionaries.
        Adjust this method to return different data for testing.
        """
        return {
            "doc_id": "123",
            "data": [
                {
                    "content": src,
                    "embedding": "embedding",
                    "meta_data": {"url": "none"},
                }
            ],
        }



# Source: /content/embedchain/tests/chunkers/test_base_chunker.py
import hashlib
from unittest.mock import MagicMock

import pytest

from embedchain.chunkers.base_chunker import BaseChunker
from embedchain.models.data_type import DataType


@pytest.fixture
def text_splitter_mock():
    return MagicMock()


@pytest.fixture
def loader_mock():
    return MagicMock()


@pytest.fixture
def app_id():
    return "test_app"


@pytest.fixture
def data_type():
    return DataType.TEXT


@pytest.fixture
def chunker(text_splitter_mock, data_type):
    text_splitter = text_splitter_mock
    chunker = BaseChunker(text_splitter)
    chunker.set_data_type(data_type)
    return chunker


def test_create_chunks(chunker, text_splitter_mock, loader_mock, app_id, data_type):
    text_splitter_mock.split_text.return_value = ["Chunk 1", "Chunk 2"]
    loader_mock.load_data.return_value = {
        "data": [{"content": "Content 1", "meta_data": {"url": "URL 1"}}],
        "doc_id": "DocID",
    }

    result = chunker.create_chunks(loader_mock, "test_src", app_id)
    expected_ids = [
        f"{app_id}--" + hashlib.sha256(("Chunk 1" + "URL 1").encode()).hexdigest(),
        f"{app_id}--" + hashlib.sha256(("Chunk 2" + "URL 1").encode()).hexdigest(),
    ]

    assert result["documents"] == ["Chunk 1", "Chunk 2"]
    assert result["ids"] == expected_ids
    assert result["metadatas"] == [
        {
            "url": "URL 1",
            "data_type": data_type.value,
            "doc_id": f"{app_id}--DocID",
        },
        {
            "url": "URL 1",
            "data_type": data_type.value,
            "doc_id": f"{app_id}--DocID",
        },
    ]
    assert result["doc_id"] == f"{app_id}--DocID"


def test_get_chunks(chunker, text_splitter_mock):
    text_splitter_mock.split_text.return_value = ["Chunk 1", "Chunk 2"]

    content = "This is a test content."
    result = chunker.get_chunks(content)

    assert len(result) == 2
    assert result == ["Chunk 1", "Chunk 2"]


def test_set_data_type(chunker):
    chunker.set_data_type(DataType.MDX)
    assert chunker.data_type == DataType.MDX


def test_get_word_count(chunker):
    documents = ["This is a test.", "Another test."]
    result = chunker.get_word_count(documents)
    assert result == 6



# Source: /content/embedchain/tests/chunkers/test_chunkers.py
from embedchain.chunkers.docs_site import DocsSiteChunker
from embedchain.chunkers.docx_file import DocxFileChunker
from embedchain.chunkers.gmail import GmailChunker
from embedchain.chunkers.json import JSONChunker
from embedchain.chunkers.mdx import MdxChunker
from embedchain.chunkers.notion import NotionChunker
from embedchain.chunkers.openapi import OpenAPIChunker
from embedchain.chunkers.pdf_file import PdfFileChunker
from embedchain.chunkers.qna_pair import QnaPairChunker
from embedchain.chunkers.sitemap import SitemapChunker
from embedchain.chunkers.table import TableChunker
from embedchain.chunkers.text import TextChunker
from embedchain.chunkers.web_page import WebPageChunker
from embedchain.chunkers.xml import XmlChunker
from embedchain.chunkers.youtube_video import YoutubeVideoChunker
from embedchain.config.add_config import ChunkerConfig

chunker_config = ChunkerConfig(chunk_size=500, chunk_overlap=0, length_function=len)

chunker_common_config = {
    DocsSiteChunker: {"chunk_size": 500, "chunk_overlap": 50, "length_function": len},
    DocxFileChunker: {"chunk_size": 1000, "chunk_overlap": 0, "length_function": len},
    PdfFileChunker: {"chunk_size": 1000, "chunk_overlap": 0, "length_function": len},
    TextChunker: {"chunk_size": 300, "chunk_overlap": 0, "length_function": len},
    MdxChunker: {"chunk_size": 1000, "chunk_overlap": 0, "length_function": len},
    NotionChunker: {"chunk_size": 300, "chunk_overlap": 0, "length_function": len},
    QnaPairChunker: {"chunk_size": 300, "chunk_overlap": 0, "length_function": len},
    TableChunker: {"chunk_size": 300, "chunk_overlap": 0, "length_function": len},
    SitemapChunker: {"chunk_size": 500, "chunk_overlap": 0, "length_function": len},
    WebPageChunker: {"chunk_size": 500, "chunk_overlap": 0, "length_function": len},
    XmlChunker: {"chunk_size": 500, "chunk_overlap": 50, "length_function": len},
    YoutubeVideoChunker: {"chunk_size": 2000, "chunk_overlap": 0, "length_function": len},
    JSONChunker: {"chunk_size": 1000, "chunk_overlap": 0, "length_function": len},
    OpenAPIChunker: {"chunk_size": 1000, "chunk_overlap": 0, "length_function": len},
    GmailChunker: {"chunk_size": 1000, "chunk_overlap": 0, "length_function": len},
}


def test_default_config_values():
    for chunker_class, config in chunker_common_config.items():
        chunker = chunker_class()
        assert chunker.text_splitter._chunk_size == config["chunk_size"]
        assert chunker.text_splitter._chunk_overlap == config["chunk_overlap"]
        assert chunker.text_splitter._length_function == config["length_function"]


def test_custom_config_values():
    for chunker_class, _ in chunker_common_config.items():
        chunker = chunker_class(config=chunker_config)
        assert chunker.text_splitter._chunk_size == 500
        assert chunker.text_splitter._chunk_overlap == 0
        assert chunker.text_splitter._length_function == len



# Source: /content/embedchain/tests/chunkers/test_text.py
# ruff: noqa: E501

from embedchain.chunkers.text import TextChunker
from embedchain.config import ChunkerConfig
from embedchain.models.data_type import DataType


class TestTextChunker:
    def test_chunks_without_app_id(self):
        """
        Test the chunks generated by TextChunker.
        """
        chunker_config = ChunkerConfig(chunk_size=10, chunk_overlap=0, length_function=len)
        chunker = TextChunker(config=chunker_config)
        text = "Lorem ipsum dolor sit amet, consectetur adipiscing elit."
        # Data type must be set manually in the test
        chunker.set_data_type(DataType.TEXT)
        result = chunker.create_chunks(MockLoader(), text)
        documents = result["documents"]
        assert len(documents) > 5

    def test_chunks_with_app_id(self):
        """
        Test the chunks generated by TextChunker with app_id
        """
        chunker_config = ChunkerConfig(chunk_size=10, chunk_overlap=0, length_function=len)
        chunker = TextChunker(config=chunker_config)
        text = "Lorem ipsum dolor sit amet, consectetur adipiscing elit."
        chunker.set_data_type(DataType.TEXT)
        result = chunker.create_chunks(MockLoader(), text)
        documents = result["documents"]
        assert len(documents) > 5

    def test_big_chunksize(self):
        """
        Test that if an infinitely high chunk size is used, only one chunk is returned.
        """
        chunker_config = ChunkerConfig(chunk_size=9999999999, chunk_overlap=0, length_function=len)
        chunker = TextChunker(config=chunker_config)
        text = "Lorem ipsum dolor sit amet, consectetur adipiscing elit."
        # Data type must be set manually in the test
        chunker.set_data_type(DataType.TEXT)
        result = chunker.create_chunks(MockLoader(), text)
        documents = result["documents"]
        assert len(documents) == 1

    def test_small_chunksize(self):
        """
        Test that if a chunk size of one is used, every character is a chunk.
        """
        chunker_config = ChunkerConfig(chunk_size=1, chunk_overlap=0, length_function=len)
        chunker = TextChunker(config=chunker_config)
        # We can't test with lorem ipsum because chunks are deduped, so would be recurring characters.
        text = """0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n\r\x0b\x0c"""
        # Data type must be set manually in the test
        chunker.set_data_type(DataType.TEXT)
        result = chunker.create_chunks(MockLoader(), text)
        documents = result["documents"]
        assert len(documents) == len(text)

    def test_word_count(self):
        chunker_config = ChunkerConfig(chunk_size=1, chunk_overlap=0, length_function=len)
        chunker = TextChunker(config=chunker_config)
        chunker.set_data_type(DataType.TEXT)

        document = ["ab cd", "ef gh"]
        result = chunker.get_word_count(document)
        assert result == 4


class MockLoader:
    def load_data(self, src):
        """
        Mock loader that returns a list of data dictionaries.
        Adjust this method to return different data for testing.
        """
        return {
            "doc_id": "123",
            "data": [
                {
                    "content": src,
                    "meta_data": {"url": "none"},
                }
            ],
        }



# Source: /content/embedchain/tests/vectordb/test_zilliz_db.py
# ruff: noqa: E501

import os
from unittest import mock
from unittest.mock import Mock, patch

import pytest

from embedchain.config import ZillizDBConfig
from embedchain.vectordb.zilliz import ZillizVectorDB


# to run tests, provide the URI and TOKEN in .env file
class TestZillizVectorDBConfig:
    @mock.patch.dict(os.environ, {"ZILLIZ_CLOUD_URI": "mocked_uri", "ZILLIZ_CLOUD_TOKEN": "mocked_token"})
    def test_init_with_uri_and_token(self):
        """
        Test if the `ZillizVectorDBConfig` instance is initialized with the correct uri and token values.
        """
        # Create a ZillizDBConfig instance with mocked values
        expected_uri = "mocked_uri"
        expected_token = "mocked_token"
        db_config = ZillizDBConfig()

        # Assert that the values in the ZillizVectorDB instance match the mocked values
        assert db_config.uri == expected_uri
        assert db_config.token == expected_token

    @mock.patch.dict(os.environ, {"ZILLIZ_CLOUD_URI": "mocked_uri", "ZILLIZ_CLOUD_TOKEN": "mocked_token"})
    def test_init_without_uri(self):
        """
        Test if the `ZillizVectorDBConfig` instance throws an error when no URI found.
        """
        try:
            del os.environ["ZILLIZ_CLOUD_URI"]
        except KeyError:
            pass

        with pytest.raises(AttributeError):
            ZillizDBConfig()

    @mock.patch.dict(os.environ, {"ZILLIZ_CLOUD_URI": "mocked_uri", "ZILLIZ_CLOUD_TOKEN": "mocked_token"})
    def test_init_without_token(self):
        """
        Test if the `ZillizVectorDBConfig` instance throws an error when no Token found.
        """
        try:
            del os.environ["ZILLIZ_CLOUD_TOKEN"]
        except KeyError:
            pass
        # Test if an exception is raised when ZILLIZ_CLOUD_TOKEN is missing
        with pytest.raises(AttributeError):
            ZillizDBConfig()


class TestZillizVectorDB:
    @pytest.fixture
    @mock.patch.dict(os.environ, {"ZILLIZ_CLOUD_URI": "mocked_uri", "ZILLIZ_CLOUD_TOKEN": "mocked_token"})
    def mock_config(self, mocker):
        return mocker.Mock(spec=ZillizDBConfig())

    @patch("embedchain.vectordb.zilliz.MilvusClient", autospec=True)
    @patch("embedchain.vectordb.zilliz.connections.connect", autospec=True)
    def test_zilliz_vector_db_setup(self, mock_connect, mock_client, mock_config):
        """
        Test if the `ZillizVectorDB` instance is initialized with the correct uri and token values.
        """
        # Create an instance of ZillizVectorDB with the mock config
        # zilliz_db = ZillizVectorDB(config=mock_config)
        ZillizVectorDB(config=mock_config)

        # Assert that the MilvusClient and connections.connect were called
        mock_client.assert_called_once_with(uri=mock_config.uri, token=mock_config.token)
        mock_connect.assert_called_once_with(uri=mock_config.uri, token=mock_config.token)


class TestZillizDBCollection:
    @pytest.fixture
    @mock.patch.dict(os.environ, {"ZILLIZ_CLOUD_URI": "mocked_uri", "ZILLIZ_CLOUD_TOKEN": "mocked_token"})
    def mock_config(self, mocker):
        return mocker.Mock(spec=ZillizDBConfig())

    @pytest.fixture
    def mock_embedder(self, mocker):
        return mocker.Mock()

    @mock.patch.dict(os.environ, {"ZILLIZ_CLOUD_URI": "mocked_uri", "ZILLIZ_CLOUD_TOKEN": "mocked_token"})
    def test_init_with_default_collection(self):
        """
        Test if the `ZillizVectorDB` instance is initialized with the correct default collection name.
        """
        # Create a ZillizDBConfig instance
        db_config = ZillizDBConfig()

        assert db_config.collection_name == "embedchain_store"

    @mock.patch.dict(os.environ, {"ZILLIZ_CLOUD_URI": "mocked_uri", "ZILLIZ_CLOUD_TOKEN": "mocked_token"})
    def test_init_with_custom_collection(self):
        """
        Test if the `ZillizVectorDB` instance is initialized with the correct custom collection name.
        """
        # Create a ZillizDBConfig instance with mocked values

        expected_collection = "test_collection"
        db_config = ZillizDBConfig(collection_name="test_collection")

        assert db_config.collection_name == expected_collection

    @patch("embedchain.vectordb.zilliz.MilvusClient", autospec=True)
    @patch("embedchain.vectordb.zilliz.connections", autospec=True)
    def test_query_with_skip_embedding(self, mock_connect, mock_client, mock_config):
        """
        Test if the `ZillizVectorDB` instance is takes in the query with skip_embeddings.
        """
        # Create an instance of ZillizVectorDB with mock config
        zilliz_db = ZillizVectorDB(config=mock_config)

        # Add a 'collection' attribute to the ZillizVectorDB instance for testing
        zilliz_db.collection = Mock(is_empty=False)  # Mock the 'collection' object

        assert zilliz_db.client == mock_client()

        # Mock the MilvusClient search method
        with patch.object(zilliz_db.client, "search") as mock_search:
            # Mock the search result
            mock_search.return_value = [[{"entity": {"text": "result_doc", "url": "url_1", "doc_id": "doc_id_1"}}]]

            # Call the query method with skip_embedding=True
            query_result = zilliz_db.query(input_query=["query_text"], n_results=1, where={}, skip_embedding=True)

            # Assert that MilvusClient.search was called with the correct parameters
            mock_search.assert_called_once_with(
                collection_name=mock_config.collection_name,
                data=["query_text"],
                limit=1,
                output_fields=["text", "url", "doc_id"],
            )

            # Assert that the query result matches the expected result
            assert query_result == [("result_doc", "url_1", "doc_id_1")]

    @patch("embedchain.vectordb.zilliz.MilvusClient", autospec=True)
    @patch("embedchain.vectordb.zilliz.connections", autospec=True)
    def test_query_without_skip_embedding(self, mock_connect, mock_client, mock_embedder, mock_config):
        """
        Test if the `ZillizVectorDB` instance is takes in the query without skip_embeddings.
        """
        # Create an instance of ZillizVectorDB with mock config
        zilliz_db = ZillizVectorDB(config=mock_config)

        # Add a 'embedder' attribute to the ZillizVectorDB instance for testing
        zilliz_db.embedder = mock_embedder  # Mock the 'collection' object

        # Add a 'collection' attribute to the ZillizVectorDB instance for testing
        zilliz_db.collection = Mock(is_empty=False)  # Mock the 'collection' object

        assert zilliz_db.client == mock_client()

        # Mock the MilvusClient search method
        with patch.object(zilliz_db.client, "search") as mock_search:
            # Mock the embedding function
            mock_embedder.embedding_fn.return_value = ["query_vector"]

            # Mock the search result
            mock_search.return_value = [[{"entity": {"text": "result_doc", "url": "url_1", "doc_id": "doc_id_1"}}]]

            # Call the query method with skip_embedding=False
            query_result = zilliz_db.query(input_query=["query_text"], n_results=1, where={}, skip_embedding=False)

            # Assert that MilvusClient.search was called with the correct parameters
            mock_search.assert_called_once_with(
                collection_name=mock_config.collection_name,
                data=["query_vector"],
                limit=1,
                output_fields=["text", "url", "doc_id"],
            )

            # Assert that the query result matches the expected result
            assert query_result == [("result_doc", "url_1", "doc_id_1")]



# Source: /content/embedchain/tests/vectordb/test_weaviate.py
import unittest
from unittest.mock import patch

from embedchain import App
from embedchain.config import AppConfig
from embedchain.config.vectordb.pinecone import PineconeDBConfig
from embedchain.embedder.base import BaseEmbedder
from embedchain.vectordb.weaviate import WeaviateDB


class TestWeaviateDb(unittest.TestCase):
    def test_incorrect_config_throws_error(self):
        """Test the init method of the WeaviateDb class throws error for incorrect config"""
        with self.assertRaises(TypeError):
            WeaviateDB(config=PineconeDBConfig())

    @patch("embedchain.vectordb.weaviate.weaviate")
    def test_initialize(self, weaviate_mock):
        """Test the init method of the WeaviateDb class."""
        weaviate_client_mock = weaviate_mock.Client.return_value
        weaviate_client_schema_mock = weaviate_client_mock.schema

        # Mock that schema doesn't already exist so that a new schema is created
        weaviate_client_schema_mock.exists.return_value = False
        # Set the embedder
        embedder = BaseEmbedder()
        embedder.set_vector_dimension(1526)

        # Create a Weaviate instance
        db = WeaviateDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=embedder)

        expected_class_obj = {
            "classes": [
                {
                    "class": "Embedchain_store_1526",
                    "vectorizer": "none",
                    "properties": [
                        {
                            "name": "identifier",
                            "dataType": ["text"],
                        },
                        {
                            "name": "text",
                            "dataType": ["text"],
                        },
                        {
                            "name": "metadata",
                            "dataType": ["Embedchain_store_1526_metadata"],
                        },
                    ],
                },
                {
                    "class": "Embedchain_store_1526_metadata",
                    "vectorizer": "none",
                    "properties": [
                        {
                            "name": "data_type",
                            "dataType": ["text"],
                        },
                        {
                            "name": "doc_id",
                            "dataType": ["text"],
                        },
                        {
                            "name": "url",
                            "dataType": ["text"],
                        },
                        {
                            "name": "hash",
                            "dataType": ["text"],
                        },
                        {
                            "name": "app_id",
                            "dataType": ["text"],
                        },
                        {
                            "name": "text",
                            "dataType": ["text"],
                        },
                    ],
                },
            ]
        }

        # Assert that the Weaviate client was initialized
        weaviate_mock.Client.assert_called_once()
        self.assertEqual(db.index_name, "Embedchain_store_1526")
        weaviate_client_schema_mock.create.assert_called_once_with(expected_class_obj)

    @patch("embedchain.vectordb.weaviate.weaviate")
    def test_get_or_create_db(self, weaviate_mock):
        """Test the _get_or_create_db method of the WeaviateDb class."""
        weaviate_client_mock = weaviate_mock.Client.return_value

        embedder = BaseEmbedder()
        embedder.set_vector_dimension(1526)

        # Create a Weaviate instance
        db = WeaviateDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=embedder)

        expected_client = db._get_or_create_db()
        self.assertEqual(expected_client, weaviate_client_mock)

    @patch("embedchain.vectordb.weaviate.weaviate")
    def test_add(self, weaviate_mock):
        """Test the add method of the WeaviateDb class."""
        weaviate_client_mock = weaviate_mock.Client.return_value
        weaviate_client_batch_mock = weaviate_client_mock.batch
        weaviate_client_batch_enter_mock = weaviate_client_mock.batch.__enter__.return_value

        # Set the embedder
        embedder = BaseEmbedder()
        embedder.set_vector_dimension(1526)

        # Create a Weaviate instance
        db = WeaviateDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=embedder)
        db.BATCH_SIZE = 1

        embeddings = [[1, 2, 3], [4, 5, 6]]
        documents = ["This is a test document.", "This is another test document."]
        metadatas = [None, None]
        ids = ["123", "456"]
        skip_embedding = True
        db.add(embeddings, documents, metadatas, ids, skip_embedding)

        # Check if the document was added to the database.
        weaviate_client_batch_mock.configure.assert_called_once_with(batch_size=1, timeout_retries=3)
        weaviate_client_batch_enter_mock.add_data_object.assert_any_call(
            data_object={"text": documents[0]}, class_name="Embedchain_store_1526_metadata", vector=embeddings[0]
        )
        weaviate_client_batch_enter_mock.add_data_object.assert_any_call(
            data_object={"text": documents[1]}, class_name="Embedchain_store_1526_metadata", vector=embeddings[1]
        )

        weaviate_client_batch_enter_mock.add_data_object.assert_any_call(
            data_object={"identifier": ids[0], "text": documents[0]},
            class_name="Embedchain_store_1526",
            vector=embeddings[0],
        )
        weaviate_client_batch_enter_mock.add_data_object.assert_any_call(
            data_object={"identifier": ids[1], "text": documents[1]},
            class_name="Embedchain_store_1526",
            vector=embeddings[1],
        )

    @patch("embedchain.vectordb.weaviate.weaviate")
    def test_query_without_where(self, weaviate_mock):
        """Test the query method of the WeaviateDb class."""
        weaviate_client_mock = weaviate_mock.Client.return_value
        weaviate_client_query_mock = weaviate_client_mock.query
        weaviate_client_query_get_mock = weaviate_client_query_mock.get.return_value

        # Set the embedder
        embedder = BaseEmbedder()
        embedder.set_vector_dimension(1526)

        # Create a Weaviate instance
        db = WeaviateDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=embedder)

        # Query for the document.
        db.query(input_query=["This is a test document."], n_results=1, where={}, skip_embedding=True)

        weaviate_client_query_mock.get.assert_called_once_with("Embedchain_store_1526", ["text"])
        weaviate_client_query_get_mock.with_near_vector.assert_called_once_with(
            {"vector": ["This is a test document."]}
        )

    @patch("embedchain.vectordb.weaviate.weaviate")
    def test_query_with_where(self, weaviate_mock):
        """Test the query method of the WeaviateDb class."""
        weaviate_client_mock = weaviate_mock.Client.return_value
        weaviate_client_query_mock = weaviate_client_mock.query
        weaviate_client_query_get_mock = weaviate_client_query_mock.get.return_value
        weaviate_client_query_get_where_mock = weaviate_client_query_get_mock.with_where.return_value

        # Set the embedder
        embedder = BaseEmbedder()
        embedder.set_vector_dimension(1526)

        # Create a Weaviate instance
        db = WeaviateDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=embedder)

        # Query for the document.
        db.query(input_query=["This is a test document."], n_results=1, where={"doc_id": "123"}, skip_embedding=True)

        weaviate_client_query_mock.get.assert_called_once_with("Embedchain_store_1526", ["text"])
        weaviate_client_query_get_mock.with_where.assert_called_once_with(
            {"operator": "Equal", "path": ["metadata", "Embedchain_store_1526_metadata", "doc_id"], "valueText": "123"}
        )
        weaviate_client_query_get_where_mock.with_near_vector.assert_called_once_with(
            {"vector": ["This is a test document."]}
        )

    @patch("embedchain.vectordb.weaviate.weaviate")
    def test_reset(self, weaviate_mock):
        """Test the reset method of the WeaviateDb class."""
        weaviate_client_mock = weaviate_mock.Client.return_value
        weaviate_client_batch_mock = weaviate_client_mock.batch

        # Set the embedder
        embedder = BaseEmbedder()
        embedder.set_vector_dimension(1526)

        # Create a Weaviate instance
        db = WeaviateDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=embedder)

        # Reset the database.
        db.reset()

        weaviate_client_batch_mock.delete_objects.assert_called_once_with(
            "Embedchain_store_1526", where={"path": ["identifier"], "operator": "Like", "valueText": ".*"}
        )

    @patch("embedchain.vectordb.weaviate.weaviate")
    def test_count(self, weaviate_mock):
        """Test the reset method of the WeaviateDb class."""
        weaviate_client_mock = weaviate_mock.Client.return_value
        weaviate_client_query = weaviate_client_mock.query

        # Set the embedder
        embedder = BaseEmbedder()
        embedder.set_vector_dimension(1526)

        # Create a Weaviate instance
        db = WeaviateDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=embedder)

        # Reset the database.
        db.count()

        weaviate_client_query.aggregate.assert_called_once_with("Embedchain_store_1526")



# Source: /content/embedchain/tests/vectordb/test_elasticsearch_db.py
import os
import unittest
from unittest.mock import patch

from embedchain import App
from embedchain.config import AppConfig, ElasticsearchDBConfig
from embedchain.embedder.gpt4all import GPT4AllEmbedder
from embedchain.vectordb.elasticsearch import ElasticsearchDB


class TestEsDB(unittest.TestCase):
    @patch("embedchain.vectordb.elasticsearch.Elasticsearch")
    def test_setUp(self, mock_client):
        self.db = ElasticsearchDB(config=ElasticsearchDBConfig(es_url="https://localhost:9200"))
        self.vector_dim = 384
        app_config = AppConfig(collection_name=False, collect_metrics=False)
        self.app = App(config=app_config, db=self.db)

        # Assert that the Elasticsearch client is stored in the ElasticsearchDB class.
        self.assertEqual(self.db.client, mock_client.return_value)

    @patch("embedchain.vectordb.elasticsearch.Elasticsearch")
    def test_query(self, mock_client):
        self.db = ElasticsearchDB(config=ElasticsearchDBConfig(es_url="https://localhost:9200"))
        app_config = AppConfig(collection_name=False, collect_metrics=False)
        self.app = App(config=app_config, db=self.db, embedder=GPT4AllEmbedder())

        # Assert that the Elasticsearch client is stored in the ElasticsearchDB class.
        self.assertEqual(self.db.client, mock_client.return_value)

        # Create some dummy data.
        embeddings = [[1, 2, 3], [4, 5, 6]]
        documents = ["This is a document.", "This is another document."]
        metadatas = [{"url": "url_1", "doc_id": "doc_id_1"}, {"url": "url_2", "doc_id": "doc_id_2"}]
        ids = ["doc_1", "doc_2"]

        # Add the data to the database.
        self.db.add(embeddings, documents, metadatas, ids, skip_embedding=False)

        search_response = {
            "hits": {
                "hits": [
                    {
                        "_source": {"text": "This is a document.", "metadata": {"url": "url_1", "doc_id": "doc_id_1"}},
                        "_score": 0.9,
                    },
                    {
                        "_source": {
                            "text": "This is another document.",
                            "metadata": {"url": "url_2", "doc_id": "doc_id_2"},
                        },
                        "_score": 0.8,
                    },
                ]
            }
        }

        # Configure the mock client to return the mocked response.
        mock_client.return_value.search.return_value = search_response

        # Query the database for the documents that are most similar to the query "This is a document".
        query = ["This is a document"]
        results = self.db.query(query, n_results=2, where={}, skip_embedding=False)

        # Assert that the results are correct.
        self.assertEqual(
            results, [("This is a document.", "url_1", "doc_id_1"), ("This is another document.", "url_2", "doc_id_2")]
        )

    @patch("embedchain.vectordb.elasticsearch.Elasticsearch")
    def test_query_with_skip_embedding(self, mock_client):
        self.db = ElasticsearchDB(config=ElasticsearchDBConfig(es_url="https://localhost:9200"))
        app_config = AppConfig(collection_name=False, collect_metrics=False)
        self.app = App(config=app_config, db=self.db)

        # Assert that the Elasticsearch client is stored in the ElasticsearchDB class.
        self.assertEqual(self.db.client, mock_client.return_value)

        # Create some dummy data.
        embeddings = [[1, 2, 3], [4, 5, 6]]
        documents = ["This is a document.", "This is another document."]
        metadatas = [{"url": "url_1", "doc_id": "doc_id_1"}, {"url": "url_2", "doc_id": "doc_id_2"}]
        ids = ["doc_1", "doc_2"]

        # Add the data to the database.
        self.db.add(embeddings, documents, metadatas, ids, skip_embedding=True)

        search_response = {
            "hits": {
                "hits": [
                    {
                        "_source": {"text": "This is a document.", "metadata": {"url": "url_1", "doc_id": "doc_id_1"}},
                        "_score": 0.9,
                    },
                    {
                        "_source": {
                            "text": "This is another document.",
                            "metadata": {"url": "url_2", "doc_id": "doc_id_2"},
                        },
                        "_score": 0.8,
                    },
                ]
            }
        }

        # Configure the mock client to return the mocked response.
        mock_client.return_value.search.return_value = search_response

        # Query the database for the documents that are most similar to the query "This is a document".
        query = ["This is a document"]
        results = self.db.query(query, n_results=2, where={}, skip_embedding=True)

        # Assert that the results are correct.
        self.assertEqual(
            results, [("This is a document.", "url_1", "doc_id_1"), ("This is another document.", "url_2", "doc_id_2")]
        )

    def test_init_without_url(self):
        # Make sure it's not loaded from env
        try:
            del os.environ["ELASTICSEARCH_URL"]
        except KeyError:
            pass
        # Test if an exception is raised when an invalid es_config is provided
        with self.assertRaises(AttributeError):
            ElasticsearchDB()

    def test_init_with_invalid_es_config(self):
        # Test if an exception is raised when an invalid es_config is provided
        with self.assertRaises(TypeError):
            ElasticsearchDB(es_config={"ES_URL": "some_url", "valid es_config": False})



# Source: /content/embedchain/tests/vectordb/test_qdrant.py
import unittest
import uuid

from mock import patch
from qdrant_client.http import models
from qdrant_client.http.models import Batch

from embedchain import App
from embedchain.config import AppConfig
from embedchain.config.vectordb.pinecone import PineconeDBConfig
from embedchain.embedder.base import BaseEmbedder
from embedchain.vectordb.qdrant import QdrantDB


class TestQdrantDB(unittest.TestCase):
    TEST_UUIDS = ["abc", "def", "ghi"]

    def test_incorrect_config_throws_error(self):
        """Test the init method of the Qdrant class throws error for incorrect config"""
        with self.assertRaises(TypeError):
            QdrantDB(config=PineconeDBConfig())

    @patch("embedchain.vectordb.qdrant.QdrantClient")
    def test_initialize(self, qdrant_client_mock):
        # Set the embedder
        embedder = BaseEmbedder()
        embedder.set_vector_dimension(1526)

        # Create a Qdrant instance
        db = QdrantDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=embedder)

        self.assertEqual(db.collection_name, "embedchain-store-1526")
        self.assertEqual(db.client, qdrant_client_mock.return_value)
        qdrant_client_mock.return_value.get_collections.assert_called_once()

    @patch("embedchain.vectordb.qdrant.QdrantClient")
    def test_get(self, qdrant_client_mock):
        qdrant_client_mock.return_value.scroll.return_value = ([], None)

        # Set the embedder
        embedder = BaseEmbedder()
        embedder.set_vector_dimension(1526)

        # Create a Qdrant instance
        db = QdrantDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=embedder)

        resp = db.get(ids=[], where={})
        self.assertEqual(resp, {"ids": []})
        resp2 = db.get(ids=["123", "456"], where={"url": "https://ai.ai"})
        self.assertEqual(resp2, {"ids": []})

    @patch("embedchain.vectordb.qdrant.QdrantClient")
    @patch.object(uuid, "uuid4", side_effect=TEST_UUIDS)
    def test_add(self, uuid_mock, qdrant_client_mock):
        qdrant_client_mock.return_value.scroll.return_value = ([], None)

        # Set the embedder
        embedder = BaseEmbedder()
        embedder.set_vector_dimension(1526)

        # Create a Qdrant instance
        db = QdrantDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=embedder)

        embeddings = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]
        documents = ["This is a test document.", "This is another test document."]
        metadatas = [{}, {}]
        ids = ["123", "456"]
        skip_embedding = True
        db.add(embeddings, documents, metadatas, ids, skip_embedding)
        qdrant_client_mock.return_value.upsert.assert_called_once_with(
            collection_name="embedchain-store-1526",
            points=Batch(
                ids=["abc", "def"],
                payloads=[
                    {
                        "identifier": "123",
                        "text": "This is a test document.",
                        "metadata": {"text": "This is a test document."},
                    },
                    {
                        "identifier": "456",
                        "text": "This is another test document.",
                        "metadata": {"text": "This is another test document."},
                    },
                ],
                vectors=embeddings,
            ),
        )

    @patch("embedchain.vectordb.qdrant.QdrantClient")
    def test_query(self, qdrant_client_mock):
        # Set the embedder
        embedder = BaseEmbedder()
        embedder.set_vector_dimension(1526)

        # Create a Qdrant instance
        db = QdrantDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=embedder)

        # Query for the document.
        db.query(input_query=["This is a test document."], n_results=1, where={"doc_id": "123"}, skip_embedding=True)

        qdrant_client_mock.return_value.search.assert_called_once_with(
            collection_name="embedchain-store-1526",
            query_filter=models.Filter(
                must=[
                    models.FieldCondition(
                        key="payload.metadata.doc_id",
                        match=models.MatchValue(
                            value="123",
                        ),
                    )
                ]
            ),
            query_vector=["This is a test document."],
            limit=1,
        )

    @patch("embedchain.vectordb.qdrant.QdrantClient")
    def test_count(self, qdrant_client_mock):
        # Set the embedder
        embedder = BaseEmbedder()
        embedder.set_vector_dimension(1526)

        # Create a Qdrant instance
        db = QdrantDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=embedder)

        db.count()
        qdrant_client_mock.return_value.get_collection.assert_called_once_with(collection_name="embedchain-store-1526")

    @patch("embedchain.vectordb.qdrant.QdrantClient")
    def test_reset(self, qdrant_client_mock):
        # Set the embedder
        embedder = BaseEmbedder()
        embedder.set_vector_dimension(1526)

        # Create a Qdrant instance
        db = QdrantDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=embedder)

        db.reset()
        qdrant_client_mock.return_value.delete_collection.assert_called_once_with(
            collection_name="embedchain-store-1526"
        )


if __name__ == "__main__":
    unittest.main()



# Source: /content/embedchain/tests/vectordb/test_chroma_db.py
import os
import shutil
from unittest.mock import patch

import pytest
from chromadb.config import Settings

from embedchain import App
from embedchain.config import AppConfig, ChromaDbConfig
from embedchain.vectordb.chroma import ChromaDB

os.environ["OPENAI_API_KEY"] = "test-api-key"


@pytest.fixture
def chroma_db():
    return ChromaDB(config=ChromaDbConfig(host="test-host", port="1234"))


@pytest.fixture
def app_with_settings():
    chroma_config = ChromaDbConfig(allow_reset=True, dir="test-db")
    app_config = AppConfig(collect_metrics=False)
    return App(config=app_config, db_config=chroma_config)


@pytest.fixture(scope="session", autouse=True)
def cleanup_db():
    yield
    try:
        shutil.rmtree("test-db")
    except OSError as e:
        print("Error: %s - %s." % (e.filename, e.strerror))


def test_chroma_db_init_with_host_and_port(chroma_db):
    settings = chroma_db.client.get_settings()
    assert settings.chroma_server_host == "test-host"
    assert settings.chroma_server_http_port == "1234"


def test_chroma_db_init_with_basic_auth():
    chroma_config = {
        "host": "test-host",
        "port": "1234",
        "chroma_settings": {
            "chroma_client_auth_provider": "chromadb.auth.basic.BasicAuthClientProvider",
            "chroma_client_auth_credentials": "admin:admin",
        },
    }

    db = ChromaDB(config=ChromaDbConfig(**chroma_config))
    settings = db.client.get_settings()
    assert settings.chroma_server_host == "test-host"
    assert settings.chroma_server_http_port == "1234"
    assert settings.chroma_client_auth_provider == chroma_config["chroma_settings"]["chroma_client_auth_provider"]
    assert settings.chroma_client_auth_credentials == chroma_config["chroma_settings"]["chroma_client_auth_credentials"]


@patch("embedchain.vectordb.chroma.chromadb.Client")
def test_app_init_with_host_and_port(mock_client):
    host = "test-host"
    port = "1234"
    config = AppConfig(collect_metrics=False)
    db_config = ChromaDbConfig(host=host, port=port)
    _app = App(config, db_config=db_config)

    called_settings: Settings = mock_client.call_args[0][0]
    assert called_settings.chroma_server_host == host
    assert called_settings.chroma_server_http_port == port


@patch("embedchain.vectordb.chroma.chromadb.Client")
def test_app_init_with_host_and_port_none(mock_client):
    _app = App(config=AppConfig(collect_metrics=False), db_config=ChromaDbConfig(allow_reset=True, dir="test-db"))

    called_settings: Settings = mock_client.call_args[0][0]
    assert called_settings.chroma_server_host is None
    assert called_settings.chroma_server_http_port is None


def test_chroma_db_duplicates_throw_warning(caplog):
    app = App(config=AppConfig(collect_metrics=False), db_config=ChromaDbConfig(allow_reset=True, dir="test-db"))
    app.db.collection.add(embeddings=[[0, 0, 0]], ids=["0"])
    app.db.collection.add(embeddings=[[0, 0, 0]], ids=["0"])
    assert "Insert of existing embedding ID: 0" in caplog.text
    assert "Add of existing embedding ID: 0" in caplog.text
    app.db.reset()


def test_chroma_db_duplicates_collections_no_warning(caplog):
    app = App(config=AppConfig(collect_metrics=False), db_config=ChromaDbConfig(allow_reset=True, dir="test-db"))
    app.set_collection_name("test_collection_1")
    app.db.collection.add(embeddings=[[0, 0, 0]], ids=["0"])
    app.set_collection_name("test_collection_2")
    app.db.collection.add(embeddings=[[0, 0, 0]], ids=["0"])
    assert "Insert of existing embedding ID: 0" not in caplog.text
    assert "Add of existing embedding ID: 0" not in caplog.text
    app.db.reset()
    app.set_collection_name("test_collection_1")
    app.db.reset()


def test_chroma_db_collection_init_with_default_collection():
    app = App(config=AppConfig(collect_metrics=False), db_config=ChromaDbConfig(allow_reset=True, dir="test-db"))
    assert app.db.collection.name == "embedchain_store"


def test_chroma_db_collection_init_with_custom_collection():
    app = App(config=AppConfig(collect_metrics=False), db_config=ChromaDbConfig(allow_reset=True, dir="test-db"))
    app.set_collection_name(name="test_collection")
    assert app.db.collection.name == "test_collection"


def test_chroma_db_collection_set_collection_name():
    app = App(config=AppConfig(collect_metrics=False), db_config=ChromaDbConfig(allow_reset=True, dir="test-db"))
    app.set_collection_name("test_collection")
    assert app.db.collection.name == "test_collection"


def test_chroma_db_collection_changes_encapsulated():
    app = App(config=AppConfig(collect_metrics=False), db_config=ChromaDbConfig(allow_reset=True, dir="test-db"))
    app.set_collection_name("test_collection_1")
    assert app.db.count() == 0

    app.db.collection.add(embeddings=[0, 0, 0], ids=["0"])
    assert app.db.count() == 1

    app.set_collection_name("test_collection_2")
    assert app.db.count() == 0

    app.db.collection.add(embeddings=[0, 0, 0], ids=["0"])
    app.set_collection_name("test_collection_1")
    assert app.db.count() == 1
    app.db.reset()
    app.set_collection_name("test_collection_2")
    app.db.reset()


def test_chroma_db_collection_add_with_skip_embedding(app_with_settings):
    # Start with a clean app
    app_with_settings.db.reset()

    assert app_with_settings.db.count() == 0

    app_with_settings.db.add(
        embeddings=[[0, 0, 0]],
        documents=["document"],
        metadatas=[{"url": "url_1", "doc_id": "doc_id_1"}],
        ids=["id"],
        skip_embedding=True,
    )

    assert app_with_settings.db.count() == 1

    data = app_with_settings.db.get(["id"], limit=1)
    expected_value = {
        "documents": ["document"],
        "embeddings": None,
        "ids": ["id"],
        "metadatas": [{"url": "url_1", "doc_id": "doc_id_1"}],
    }

    assert data == expected_value

    data = app_with_settings.db.query(input_query=[0, 0, 0], where={}, n_results=1, skip_embedding=True)
    expected_value = [("document", "url_1", "doc_id_1")]

    assert data == expected_value
    app_with_settings.db.reset()


def test_chroma_db_collection_add_with_invalid_inputs(app_with_settings):
    # Start with a clean app
    app_with_settings.db.reset()

    assert app_with_settings.db.count() == 0

    with pytest.raises(ValueError):
        app_with_settings.db.add(
            embeddings=[[0, 0, 0]],
            documents=["document", "document2"],
            metadatas=[{"value": "somevalue"}],
            ids=["id"],
            skip_embedding=True,
        )

    assert app_with_settings.db.count() == 0

    with pytest.raises(ValueError):
        app_with_settings.db.add(
            embeddings=None,
            documents=["document", "document2"],
            metadatas=[{"value": "somevalue"}],
            ids=["id"],
            skip_embedding=True,
        )

    assert app_with_settings.db.count() == 0
    app_with_settings.db.reset()


def test_chroma_db_collection_collections_are_persistent():
    app = App(config=AppConfig(collect_metrics=False), db_config=ChromaDbConfig(allow_reset=True, dir="test-db"))
    app.set_collection_name("test_collection_1")
    app.db.collection.add(embeddings=[[0, 0, 0]], ids=["0"])
    del app

    app = App(config=AppConfig(collect_metrics=False), db_config=ChromaDbConfig(allow_reset=True, dir="test-db"))
    app.set_collection_name("test_collection_1")
    assert app.db.count() == 1

    app.db.reset()


def test_chroma_db_collection_parallel_collections():
    app1 = App(
        AppConfig(collection_name="test_collection_1", collect_metrics=False),
        db_config=ChromaDbConfig(allow_reset=True, dir="test-db"),
    )
    app2 = App(
        AppConfig(collection_name="test_collection_2", collect_metrics=False),
        db_config=ChromaDbConfig(allow_reset=True, dir="test-db"),
    )

    # cleanup if any previous tests failed or were interrupted
    app1.db.reset()
    app2.db.reset()

    app1.db.collection.add(embeddings=[0, 0, 0], ids=["0"])
    assert app1.db.count() == 1
    assert app2.db.count() == 0

    app1.db.collection.add(embeddings=[[0, 0, 0], [1, 1, 1]], ids=["1", "2"])
    app2.db.collection.add(embeddings=[0, 0, 0], ids=["0"])

    app1.set_collection_name("test_collection_2")
    assert app1.db.count() == 1
    app2.set_collection_name("test_collection_1")
    assert app2.db.count() == 3

    # cleanup
    app1.db.reset()
    app2.db.reset()


def test_chroma_db_collection_ids_share_collections():
    app1 = App(
        AppConfig(id="new_app_id_1", collect_metrics=False), db_config=ChromaDbConfig(allow_reset=True, dir="test-db")
    )
    app1.set_collection_name("one_collection")
    app2 = App(
        AppConfig(id="new_app_id_2", collect_metrics=False), db_config=ChromaDbConfig(allow_reset=True, dir="test-db")
    )
    app2.set_collection_name("one_collection")

    app1.db.collection.add(embeddings=[[0, 0, 0], [1, 1, 1]], ids=["0", "1"])
    app2.db.collection.add(embeddings=[0, 0, 0], ids=["2"])

    assert app1.db.count() == 3
    assert app2.db.count() == 3

    # cleanup
    app1.db.reset()
    app2.db.reset()


def test_chroma_db_collection_reset():
    app1 = App(
        AppConfig(id="new_app_id_1", collect_metrics=False), db_config=ChromaDbConfig(allow_reset=True, dir="test-db")
    )
    app1.set_collection_name("one_collection")
    app2 = App(
        AppConfig(id="new_app_id_2", collect_metrics=False), db_config=ChromaDbConfig(allow_reset=True, dir="test-db")
    )
    app2.set_collection_name("two_collection")
    app3 = App(
        AppConfig(id="new_app_id_1", collect_metrics=False), db_config=ChromaDbConfig(allow_reset=True, dir="test-db")
    )
    app3.set_collection_name("three_collection")
    app4 = App(
        AppConfig(id="new_app_id_4", collect_metrics=False), db_config=ChromaDbConfig(allow_reset=True, dir="test-db")
    )
    app4.set_collection_name("four_collection")

    app1.db.collection.add(embeddings=[0, 0, 0], ids=["1"])
    app2.db.collection.add(embeddings=[0, 0, 0], ids=["2"])
    app3.db.collection.add(embeddings=[0, 0, 0], ids=["3"])
    app4.db.collection.add(embeddings=[0, 0, 0], ids=["4"])

    app1.db.reset()

    assert app1.db.count() == 0
    assert app2.db.count() == 1
    assert app3.db.count() == 1
    assert app4.db.count() == 1

    # cleanup
    app2.db.reset()
    app3.db.reset()
    app4.db.reset()


def test_chroma_db_collection_query(app_with_settings):
    app_with_settings.db.reset()

    assert app_with_settings.db.count() == 0

    app_with_settings.db.add(
        embeddings=[[0, 0, 0]],
        documents=["document"],
        metadatas=[{"url": "url_1", "doc_id": "doc_id_1"}],
        ids=["id"],
        skip_embedding=True,
    )

    assert app_with_settings.db.count() == 1

    app_with_settings.db.add(
        embeddings=[[0, 1, 0]],
        documents=["document2"],
        metadatas=[{"url": "url_2", "doc_id": "doc_id_2"}],
        ids=["id2"],
        skip_embedding=True,
    )

    assert app_with_settings.db.count() == 2

    data = app_with_settings.db.query(input_query=[0, 0, 0], where={}, n_results=2, skip_embedding=True)
    expected_value = [("document", "url_1", "doc_id_1"), ("document2", "url_2", "doc_id_2")]

    assert data == expected_value
    app_with_settings.db.reset()



# Source: /content/embedchain/tests/vectordb/test_pinecone.py
from unittest import mock
from unittest.mock import patch

from embedchain import App
from embedchain.config import AppConfig
from embedchain.embedder.base import BaseEmbedder
from embedchain.vectordb.pinecone import PineconeDB


class TestPinecone:
    @patch("embedchain.vectordb.pinecone.pinecone")
    def test_init(self, pinecone_mock):
        """Test that the PineconeDB can be initialized."""
        # Create a PineconeDB instance
        PineconeDB()

        # Assert that the Pinecone client was initialized
        pinecone_mock.init.assert_called_once()
        pinecone_mock.list_indexes.assert_called_once()
        pinecone_mock.Index.assert_called_once()

    @patch("embedchain.vectordb.pinecone.pinecone")
    def test_set_embedder(self, pinecone_mock):
        """Test that the embedder can be set."""

        # Set the embedder
        embedder = BaseEmbedder()

        # Create a PineconeDB instance
        db = PineconeDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=embedder)

        # Assert that the embedder was set
        assert db.embedder == embedder
        pinecone_mock.init.assert_called_once()

    @patch("embedchain.vectordb.pinecone.pinecone")
    def test_add_documents(self, pinecone_mock):
        """Test that documents can be added to the database."""
        pinecone_client_mock = pinecone_mock.Index.return_value

        embedding_function = mock.Mock()
        base_embedder = BaseEmbedder()
        base_embedder.set_embedding_fn(embedding_function)
        vectors = [[0, 0, 0], [1, 1, 1]]
        embedding_function.return_value = vectors
        # Create a PineconeDb instance
        db = PineconeDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=base_embedder)

        # Add some documents to the database
        documents = ["This is a document.", "This is another document."]
        metadatas = [{}, {}]
        ids = ["doc1", "doc2"]
        db.add(vectors, documents, metadatas, ids, True)

        expected_pinecone_upsert_args = [
            {"id": "doc1", "metadata": {"text": "This is a document."}, "values": [0, 0, 0]},
            {"id": "doc2", "metadata": {"text": "This is another document."}, "values": [1, 1, 1]},
        ]
        # Assert that the Pinecone client was called to upsert the documents
        pinecone_client_mock.upsert.assert_called_once_with(expected_pinecone_upsert_args)

    @patch("embedchain.vectordb.pinecone.pinecone")
    def test_query_documents(self, pinecone_mock):
        """Test that documents can be queried from the database."""
        pinecone_client_mock = pinecone_mock.Index.return_value

        embedding_function = mock.Mock()
        base_embedder = BaseEmbedder()
        base_embedder.set_embedding_fn(embedding_function)
        vectors = [[0, 0, 0]]
        embedding_function.return_value = vectors
        # Create a PineconeDB instance
        db = PineconeDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=base_embedder)

        # Query the database for documents that are similar to "document"
        input_query = ["document"]
        n_results = 1
        db.query(input_query, n_results, where={}, skip_embedding=False)

        # Assert that the Pinecone client was called to query the database
        pinecone_client_mock.query.assert_called_once_with(
            vector=db.embedder.embedding_fn(input_query)[0], top_k=n_results, filter={}, include_metadata=True
        )

    @patch("embedchain.vectordb.pinecone.pinecone")
    def test_reset(self, pinecone_mock):
        """Test that the database can be reset."""
        # Create a PineconeDb instance
        db = PineconeDB()
        app_config = AppConfig(collect_metrics=False)
        App(config=app_config, db=db, embedder=BaseEmbedder())

        # Reset the database
        db.reset()

        # Assert that the Pinecone client was called to delete the index
        pinecone_mock.delete_index.assert_called_once_with(db.index_name)

        # Assert that the index is recreated
        pinecone_mock.Index.assert_called_with(db.index_name)



# Source: /content/embedchain/tests/llm/test_base_llm.py
from string import Template

import pytest

from embedchain.llm.base import BaseLlm, BaseLlmConfig


@pytest.fixture
def base_llm():
    config = BaseLlmConfig()
    return BaseLlm(config=config)


def test_is_get_llm_model_answer_not_implemented(base_llm):
    with pytest.raises(NotImplementedError):
        base_llm.get_llm_model_answer()


def test_is_stream_bool():
    with pytest.raises(ValueError):
        config = BaseLlmConfig(stream="test value")
        BaseLlm(config=config)


def test_template_string_gets_converted_to_Template_instance():
    config = BaseLlmConfig(template="test value $query $context")
    llm = BaseLlm(config=config)
    assert isinstance(llm.config.template, Template)


def test_is_get_llm_model_answer_implemented():
    class TestLlm(BaseLlm):
        def get_llm_model_answer(self):
            return "Implemented"

    config = BaseLlmConfig()
    llm = TestLlm(config=config)
    assert llm.get_llm_model_answer() == "Implemented"


def test_stream_query_response(base_llm):
    answer = ["Chunk1", "Chunk2", "Chunk3"]
    result = list(base_llm._stream_query_response(answer))
    assert result == answer


def test_stream_chat_response(base_llm):
    answer = ["Chunk1", "Chunk2", "Chunk3"]
    result = list(base_llm._stream_chat_response(answer))
    assert result == answer


def test_append_search_and_context(base_llm):
    context = "Context"
    web_search_result = "Web Search Result"
    result = base_llm._append_search_and_context(context, web_search_result)
    expected_result = "Context\nWeb Search Result: Web Search Result"
    assert result == expected_result


def test_access_search_and_get_results(base_llm, mocker):
    base_llm.access_search_and_get_results = mocker.patch.object(
        base_llm, "access_search_and_get_results", return_value="Search Results"
    )
    input_query = "Test query"
    result = base_llm.access_search_and_get_results(input_query)
    assert result == "Search Results"



# Source: /content/embedchain/tests/llm/test_chat.py
import os
import unittest
from unittest.mock import MagicMock, patch

from embedchain import App
from embedchain.config import AppConfig, BaseLlmConfig
from embedchain.llm.base import BaseLlm


class TestApp(unittest.TestCase):
    def setUp(self):
        os.environ["OPENAI_API_KEY"] = "test_key"
        self.app = App(config=AppConfig(collect_metrics=False))

    @patch.object(App, "retrieve_from_database", return_value=["Test context"])
    @patch.object(BaseLlm, "get_answer_from_llm", return_value="Test answer")
    def test_chat_with_memory(self, mock_get_answer, mock_retrieve):
        """
        This test checks the functionality of the 'chat' method in the App class with respect to the chat history
        memory.
        The 'chat' method is called twice. The first call initializes the chat history memory.
        The second call is expected to use the chat history from the first call.

        Key assumptions tested:
            called with correct arguments, adding the correct chat history.
        - After the first call, 'memory.chat_memory.add_user_message' and 'memory.chat_memory.add_ai_message' are
        - During the second call, the 'chat' method uses the chat history from the first call.

        The test isolates the 'chat' method behavior by mocking out 'retrieve_from_database', 'get_answer_from_llm' and
        'memory' methods.
        """
        config = AppConfig(collect_metrics=False)
        app = App(config=config)
        first_answer = app.chat("Test query 1")
        self.assertEqual(first_answer, "Test answer")
        self.assertEqual(len(app.llm.memory.chat_memory.messages), 2)
        self.assertEqual(len(app.llm.history.splitlines()), 2)
        second_answer = app.chat("Test query 2")
        self.assertEqual(second_answer, "Test answer")
        self.assertEqual(len(app.llm.memory.chat_memory.messages), 4)
        self.assertEqual(len(app.llm.history.splitlines()), 4)

    @patch.object(App, "retrieve_from_database", return_value=["Test context"])
    @patch.object(BaseLlm, "get_answer_from_llm", return_value="Test answer")
    def test_template_replacement(self, mock_get_answer, mock_retrieve):
        """
        Tests that if a default template is used and it doesn't contain history,
        the default template is swapped in.

        Also tests that a dry run does not change the history
        """
        config = AppConfig(collect_metrics=False)
        app = App(config=config)
        first_answer = app.chat("Test query 1")
        self.assertEqual(first_answer, "Test answer")
        self.assertEqual(len(app.llm.history.splitlines()), 2)
        history = app.llm.history
        dry_run = app.chat("Test query 2", dry_run=True)
        self.assertIn("History:", dry_run)
        self.assertEqual(history, app.llm.history)
        self.assertEqual(len(app.llm.history.splitlines()), 2)

    @patch("chromadb.api.models.Collection.Collection.add", MagicMock)
    def test_chat_with_where_in_params(self):
        """
        Test where filter
        """
        with patch.object(self.app, "retrieve_from_database") as mock_retrieve:
            mock_retrieve.return_value = ["Test context"]
            with patch.object(self.app.llm, "get_llm_model_answer") as mock_answer:
                mock_answer.return_value = "Test answer"
                answer = self.app.chat("Test query", where={"attribute": "value"})

        self.assertEqual(answer, "Test answer")
        _args, kwargs = mock_retrieve.call_args
        self.assertEqual(kwargs.get("input_query"), "Test query")
        self.assertEqual(kwargs.get("where"), {"attribute": "value"})
        mock_answer.assert_called_once()

    @patch("chromadb.api.models.Collection.Collection.add", MagicMock)
    def test_chat_with_where_in_chat_config(self):
        """
        This test checks the functionality of the 'chat' method in the App class.
        It simulates a scenario where the 'retrieve_from_database' method returns a context list based on
        a where filter and 'get_llm_model_answer' returns an expected answer string.

        The 'chat' method is expected to call 'retrieve_from_database' with the where filter specified
        in the BaseLlmConfig and 'get_llm_model_answer' methods appropriately and return the right answer.

        Key assumptions tested:
        - 'retrieve_from_database' method is called exactly once with arguments: "Test query" and an instance of
            BaseLlmConfig.
        - 'get_llm_model_answer' is called exactly once. The specific arguments are not checked in this test.
        - 'chat' method returns the value it received from 'get_llm_model_answer'.

        The test isolates the 'chat' method behavior by mocking out 'retrieve_from_database' and
        'get_llm_model_answer' methods.
        """
        with patch.object(self.app.llm, "get_llm_model_answer") as mock_answer:
            mock_answer.return_value = "Test answer"
            with patch.object(self.app.db, "query") as mock_database_query:
                mock_database_query.return_value = ["Test context"]
                llm_config = BaseLlmConfig(where={"attribute": "value"})
                answer = self.app.chat("Test query", llm_config)

        self.assertEqual(answer, "Test answer")
        _args, kwargs = mock_database_query.call_args
        self.assertEqual(kwargs.get("input_query"), "Test query")
        self.assertEqual(kwargs.get("where"), {"attribute": "value"})
        mock_answer.assert_called_once()



# Source: /content/embedchain/tests/llm/test_anthrophic.py
import os
from unittest.mock import MagicMock, patch

import pytest
from langchain.schema import HumanMessage, SystemMessage

from embedchain.config import BaseLlmConfig
from embedchain.llm.anthropic import AnthropicLlm


@pytest.fixture
def anthropic_llm():
    os.environ["ANTHROPIC_API_KEY"] = "test_api_key"
    config = BaseLlmConfig(temperature=0.5, model="gpt2")
    return AnthropicLlm(config)


def test_get_llm_model_answer(anthropic_llm):
    with patch.object(AnthropicLlm, "_get_answer", return_value="Test Response") as mock_method:
        prompt = "Test Prompt"
        response = anthropic_llm.get_llm_model_answer(prompt)
        assert response == "Test Response"
        mock_method.assert_called_once_with(prompt=prompt, config=anthropic_llm.config)


def test_get_answer(anthropic_llm):
    with patch("langchain.chat_models.ChatAnthropic") as mock_chat:
        mock_chat_instance = mock_chat.return_value
        mock_chat_instance.return_value = MagicMock(content="Test Response")

        prompt = "Test Prompt"
        response = anthropic_llm._get_answer(prompt, anthropic_llm.config)

        assert response == "Test Response"
        mock_chat.assert_called_once_with(
            anthropic_api_key="test_api_key",
            temperature=anthropic_llm.config.temperature,
            model=anthropic_llm.config.model,
        )
        mock_chat_instance.assert_called_once_with(
            anthropic_llm._get_messages(prompt, system_prompt=anthropic_llm.config.system_prompt)
        )


def test_get_messages(anthropic_llm):
    prompt = "Test Prompt"
    system_prompt = "Test System Prompt"
    messages = anthropic_llm._get_messages(prompt, system_prompt)
    assert messages == [
        SystemMessage(content="Test System Prompt", additional_kwargs={}),
        HumanMessage(content="Test Prompt", additional_kwargs={}, example=False),
    ]


def test_get_answer_max_tokens_is_provided(anthropic_llm, caplog):
    with patch("langchain.chat_models.ChatAnthropic") as mock_chat:
        mock_chat_instance = mock_chat.return_value
        mock_chat_instance.return_value = MagicMock(content="Test Response")

        prompt = "Test Prompt"
        config = anthropic_llm.config
        config.max_tokens = 500

        response = anthropic_llm._get_answer(prompt, config)

        assert response == "Test Response"
        mock_chat.assert_called_once_with(
            anthropic_api_key="test_api_key", temperature=config.temperature, model=config.model
        )

        assert "Config option `max_tokens` is not supported by this model." in caplog.text



# Source: /content/embedchain/tests/llm/test_azure_openai.py
from unittest.mock import MagicMock, patch

import pytest
from langchain.schema import HumanMessage, SystemMessage

from embedchain.config import BaseLlmConfig
from embedchain.llm.azure_openai import AzureOpenAILlm


@pytest.fixture
def azure_openai_llm():
    config = BaseLlmConfig(
        deployment_name="azure_deployment",
        temperature=0.7,
        model="gpt-3.5-turbo",
        max_tokens=50,
        system_prompt="System Prompt",
    )
    return AzureOpenAILlm(config)


def test_get_llm_model_answer(azure_openai_llm):
    with patch.object(AzureOpenAILlm, "_get_answer", return_value="Test Response") as mock_method:
        prompt = "Test Prompt"
        response = azure_openai_llm.get_llm_model_answer(prompt)
        assert response == "Test Response"
        mock_method.assert_called_once_with(prompt=prompt, config=azure_openai_llm.config)


def test_get_answer(azure_openai_llm):
    with patch("langchain.chat_models.AzureChatOpenAI") as mock_chat:
        mock_chat_instance = mock_chat.return_value
        mock_chat_instance.return_value = MagicMock(content="Test Response")

        prompt = "Test Prompt"
        response = azure_openai_llm._get_answer(prompt, azure_openai_llm.config)

        assert response == "Test Response"
        mock_chat.assert_called_once_with(
            deployment_name=azure_openai_llm.config.deployment_name,
            openai_api_version="2023-05-15",
            model_name=azure_openai_llm.config.model or "gpt-3.5-turbo",
            temperature=azure_openai_llm.config.temperature,
            max_tokens=azure_openai_llm.config.max_tokens,
            streaming=azure_openai_llm.config.stream,
        )
        mock_chat_instance.assert_called_once_with(
            azure_openai_llm._get_messages(prompt, system_prompt=azure_openai_llm.config.system_prompt)
        )


def test_get_messages(azure_openai_llm):
    prompt = "Test Prompt"
    system_prompt = "Test System Prompt"
    messages = azure_openai_llm._get_messages(prompt, system_prompt)
    assert messages == [
        SystemMessage(content="Test System Prompt", additional_kwargs={}),
        HumanMessage(content="Test Prompt", additional_kwargs={}, example=False),
    ]


def test_get_answer_top_p_is_provided(azure_openai_llm, caplog):
    with patch("langchain.chat_models.AzureChatOpenAI") as mock_chat:
        mock_chat_instance = mock_chat.return_value
        mock_chat_instance.return_value = MagicMock(content="Test Response")

        prompt = "Test Prompt"
        config = azure_openai_llm.config
        config.top_p = 0.5

        response = azure_openai_llm._get_answer(prompt, config)

        assert response == "Test Response"
        mock_chat.assert_called_once_with(
            deployment_name=config.deployment_name,
            openai_api_version="2023-05-15",
            model_name=config.model or "gpt-3.5-turbo",
            temperature=config.temperature,
            max_tokens=config.max_tokens,
            streaming=config.stream,
        )
        mock_chat_instance.assert_called_once_with(
            azure_openai_llm._get_messages(prompt, system_prompt=config.system_prompt)
        )

        assert "Config option `top_p` is not supported by this model." in caplog.text


def test_when_no_deployment_name_provided():
    config = BaseLlmConfig(temperature=0.7, model="gpt-3.5-turbo", max_tokens=50, system_prompt="System Prompt")
    with pytest.raises(ValueError):
        llm = AzureOpenAILlm(config)
        llm.get_llm_model_answer("Test Prompt")



# Source: /content/embedchain/tests/llm/test_vertex_ai.py
from unittest.mock import MagicMock, patch

import pytest
from langchain.schema import HumanMessage, SystemMessage

from embedchain.config import BaseLlmConfig
from embedchain.llm.vertex_ai import VertexAILlm


@pytest.fixture
def vertexai_llm():
    config = BaseLlmConfig(temperature=0.6, model="vertexai_model", system_prompt="System Prompt")
    return VertexAILlm(config)


def test_get_llm_model_answer(vertexai_llm):
    with patch.object(VertexAILlm, "_get_answer", return_value="Test Response") as mock_method:
        prompt = "Test Prompt"
        response = vertexai_llm.get_llm_model_answer(prompt)
        assert response == "Test Response"
        mock_method.assert_called_once_with(prompt=prompt, config=vertexai_llm.config)


def test_get_answer_with_warning(vertexai_llm, caplog):
    with patch("langchain.chat_models.ChatVertexAI") as mock_chat:
        mock_chat_instance = mock_chat.return_value
        mock_chat_instance.return_value = MagicMock(content="Test Response")

        prompt = "Test Prompt"
        config = vertexai_llm.config
        config.top_p = 0.5

        response = vertexai_llm._get_answer(prompt, config)

        assert response == "Test Response"
        mock_chat.assert_called_once_with(temperature=config.temperature, model=config.model)

        assert "Config option `top_p` is not supported by this model." in caplog.text


def test_get_answer_no_warning(vertexai_llm, caplog):
    with patch("langchain.chat_models.ChatVertexAI") as mock_chat:
        mock_chat_instance = mock_chat.return_value
        mock_chat_instance.return_value = MagicMock(content="Test Response")

        prompt = "Test Prompt"
        config = vertexai_llm.config
        config.top_p = 1.0

        response = vertexai_llm._get_answer(prompt, config)

        assert response == "Test Response"
        mock_chat.assert_called_once_with(temperature=config.temperature, model=config.model)

        assert "Config option `top_p` is not supported by this model." not in caplog.text


def test_get_messages(vertexai_llm):
    prompt = "Test Prompt"
    system_prompt = "Test System Prompt"
    messages = vertexai_llm._get_messages(prompt, system_prompt)
    assert messages == [
        SystemMessage(content="Test System Prompt", additional_kwargs={}),
        HumanMessage(content="Test Prompt", additional_kwargs={}, example=False),
    ]



# Source: /content/embedchain/tests/llm/test_jina.py
import os

import pytest
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

from embedchain.config import BaseLlmConfig
from embedchain.llm.jina import JinaLlm


@pytest.fixture
def config():
    os.environ["JINACHAT_API_KEY"] = "test_api_key"
    config = BaseLlmConfig(temperature=0.7, max_tokens=50, top_p=0.8, stream=False, system_prompt="System prompt")
    yield config
    os.environ.pop("JINACHAT_API_KEY")


def test_init_raises_value_error_without_api_key(mocker):
    mocker.patch.dict(os.environ, clear=True)
    with pytest.raises(ValueError):
        JinaLlm()


def test_get_llm_model_answer(config, mocker):
    mocked_get_answer = mocker.patch("embedchain.llm.jina.JinaLlm._get_answer", return_value="Test answer")

    llm = JinaLlm(config)
    answer = llm.get_llm_model_answer("Test query")

    assert answer == "Test answer"
    mocked_get_answer.assert_called_once_with("Test query", config)


def test_get_llm_model_answer_with_system_prompt(config, mocker):
    config.system_prompt = "Custom system prompt"
    mocked_get_answer = mocker.patch("embedchain.llm.jina.JinaLlm._get_answer", return_value="Test answer")

    llm = JinaLlm(config)
    answer = llm.get_llm_model_answer("Test query")

    assert answer == "Test answer"
    mocked_get_answer.assert_called_once_with("Test query", config)


def test_get_llm_model_answer_empty_prompt(config, mocker):
    mocked_get_answer = mocker.patch("embedchain.llm.jina.JinaLlm._get_answer", return_value="Test answer")

    llm = JinaLlm(config)
    answer = llm.get_llm_model_answer("")

    assert answer == "Test answer"
    mocked_get_answer.assert_called_once_with("", config)


def test_get_llm_model_answer_with_streaming(config, mocker):
    config.stream = True
    mocked_jinachat = mocker.patch("embedchain.llm.jina.JinaChat")

    llm = JinaLlm(config)
    llm.get_llm_model_answer("Test query")

    mocked_jinachat.assert_called_once()
    callbacks = [callback[1]["callbacks"] for callback in mocked_jinachat.call_args_list]
    assert any(isinstance(callback[0], StreamingStdOutCallbackHandler) for callback in callbacks)


def test_get_llm_model_answer_without_system_prompt(config, mocker):
    config.system_prompt = None
    mocked_jinachat = mocker.patch("embedchain.llm.jina.JinaChat")

    llm = JinaLlm(config)
    llm.get_llm_model_answer("Test query")

    mocked_jinachat.assert_called_once_with(
        temperature=config.temperature,
        max_tokens=config.max_tokens,
        model_kwargs={"top_p": config.top_p},
    )



# Source: /content/embedchain/tests/llm/test_cohere.py
import os

import pytest

from embedchain.config import BaseLlmConfig
from embedchain.llm.cohere import CohereLlm


@pytest.fixture
def cohere_llm_config():
    os.environ["COHERE_API_KEY"] = "test_api_key"
    config = BaseLlmConfig(model="gptd-instruct-tft", max_tokens=50, temperature=0.7, top_p=0.8)
    yield config
    os.environ.pop("COHERE_API_KEY")


def test_init_raises_value_error_without_api_key(mocker):
    mocker.patch.dict(os.environ, clear=True)
    with pytest.raises(ValueError):
        CohereLlm()


def test_get_llm_model_answer_raises_value_error_for_system_prompt(cohere_llm_config):
    llm = CohereLlm(cohere_llm_config)
    llm.config.system_prompt = "system_prompt"
    with pytest.raises(ValueError):
        llm.get_llm_model_answer("prompt")


def test_get_llm_model_answer(cohere_llm_config, mocker):
    mocker.patch("embedchain.llm.cohere.CohereLlm._get_answer", return_value="Test answer")

    llm = CohereLlm(cohere_llm_config)
    answer = llm.get_llm_model_answer("Test query")

    assert answer == "Test answer"


def test_get_answer_mocked_cohere(cohere_llm_config, mocker):
    mocked_cohere = mocker.patch("embedchain.llm.cohere.Cohere")
    mock_instance = mocked_cohere.return_value
    mock_instance.return_value = "Mocked answer"

    llm = CohereLlm(cohere_llm_config)
    prompt = "Test query"
    answer = llm.get_llm_model_answer(prompt)

    assert answer == "Mocked answer"
    mocked_cohere.assert_called_once_with(
        cohere_api_key="test_api_key",
        model="gptd-instruct-tft",
        max_tokens=50,
        temperature=0.7,
        p=0.8,
    )
    mock_instance.assert_called_once_with(prompt)



# Source: /content/embedchain/tests/llm/test_openai.py
import os

import pytest
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

from embedchain.config import BaseLlmConfig
from embedchain.llm.openai import OpenAILlm


@pytest.fixture
def config():
    os.environ["OPENAI_API_KEY"] = "test_api_key"
    config = BaseLlmConfig(
        temperature=0.7, max_tokens=50, top_p=0.8, stream=False, system_prompt="System prompt", model="gpt-3.5-turbo"
    )
    yield config
    os.environ.pop("OPENAI_API_KEY")


def test_get_llm_model_answer(config, mocker):
    mocked_get_answer = mocker.patch("embedchain.llm.openai.OpenAILlm._get_answer", return_value="Test answer")

    llm = OpenAILlm(config)
    answer = llm.get_llm_model_answer("Test query")

    assert answer == "Test answer"
    mocked_get_answer.assert_called_once_with("Test query", config)


def test_get_llm_model_answer_with_system_prompt(config, mocker):
    config.system_prompt = "Custom system prompt"
    mocked_get_answer = mocker.patch("embedchain.llm.openai.OpenAILlm._get_answer", return_value="Test answer")

    llm = OpenAILlm(config)
    answer = llm.get_llm_model_answer("Test query")

    assert answer == "Test answer"
    mocked_get_answer.assert_called_once_with("Test query", config)


def test_get_llm_model_answer_empty_prompt(config, mocker):
    mocked_get_answer = mocker.patch("embedchain.llm.openai.OpenAILlm._get_answer", return_value="Test answer")

    llm = OpenAILlm(config)
    answer = llm.get_llm_model_answer("")

    assert answer == "Test answer"
    mocked_get_answer.assert_called_once_with("", config)


def test_get_llm_model_answer_with_streaming(config, mocker):
    config.stream = True
    mocked_jinachat = mocker.patch("embedchain.llm.openai.ChatOpenAI")

    llm = OpenAILlm(config)
    llm.get_llm_model_answer("Test query")

    mocked_jinachat.assert_called_once()
    callbacks = [callback[1]["callbacks"] for callback in mocked_jinachat.call_args_list]
    assert any(isinstance(callback[0], StreamingStdOutCallbackHandler) for callback in callbacks)


def test_get_llm_model_answer_without_system_prompt(config, mocker):
    config.system_prompt = None
    mocked_jinachat = mocker.patch("embedchain.llm.openai.ChatOpenAI")

    llm = OpenAILlm(config)
    llm.get_llm_model_answer("Test query")

    mocked_jinachat.assert_called_once_with(
        model=config.model,
        temperature=config.temperature,
        max_tokens=config.max_tokens,
        model_kwargs={"top_p": config.top_p},
    )



# Source: /content/embedchain/tests/llm/test_gpt4all.py
import pytest
from langchain.llms.gpt4all import GPT4All as LangchainGPT4All

from embedchain.config import BaseLlmConfig
from embedchain.llm.gpt4all import GPT4ALLLlm


@pytest.fixture
def config():
    config = BaseLlmConfig(
        temperature=0.7,
        max_tokens=50,
        top_p=0.8,
        stream=False,
        system_prompt="System prompt",
        model="orca-mini-3b.ggmlv3.q4_0.bin",
    )
    yield config


@pytest.fixture
def gpt4all_with_config(config):
    return GPT4ALLLlm(config=config)


@pytest.fixture
def gpt4all_without_config():
    return GPT4ALLLlm()


def test_gpt4all_init_with_config(config, gpt4all_with_config):
    assert gpt4all_with_config.config.temperature == config.temperature
    assert gpt4all_with_config.config.max_tokens == config.max_tokens
    assert gpt4all_with_config.config.top_p == config.top_p
    assert gpt4all_with_config.config.stream == config.stream
    assert gpt4all_with_config.config.system_prompt == config.system_prompt
    assert gpt4all_with_config.config.model == config.model

    assert isinstance(gpt4all_with_config.instance, LangchainGPT4All)


def test_gpt4all_init_without_config(gpt4all_without_config):
    assert gpt4all_without_config.config.model == "orca-mini-3b.ggmlv3.q4_0.bin"
    assert isinstance(gpt4all_without_config.instance, LangchainGPT4All)


def test_get_llm_model_answer(mocker, gpt4all_with_config):
    test_query = "Test query"
    test_answer = "Test answer"

    mocked_get_answer = mocker.patch("embedchain.llm.gpt4all.GPT4ALLLlm._get_answer", return_value=test_answer)
    answer = gpt4all_with_config.get_llm_model_answer(test_query)

    assert answer == test_answer
    mocked_get_answer.assert_called_once_with(prompt=test_query, config=gpt4all_with_config.config)


def test_gpt4all_model_switching(gpt4all_with_config):
    with pytest.raises(RuntimeError, match="GPT4ALLLlm does not support switching models at runtime."):
        gpt4all_with_config._get_answer("Test prompt", BaseLlmConfig(model="new_model"))



# Source: /content/embedchain/tests/llm/test_generate_prompt.py
import unittest
from string import Template

from embedchain import App
from embedchain.config import AppConfig, BaseLlmConfig


class TestGeneratePrompt(unittest.TestCase):
    def setUp(self):
        self.app = App(config=AppConfig(collect_metrics=False))

    def test_generate_prompt_with_template(self):
        """
        Tests that the generate_prompt method correctly formats the prompt using
        a custom template provided in the BaseLlmConfig instance.

        This test sets up a scenario with an input query and a list of contexts,
        and a custom template, and then calls generate_prompt. It checks that the
        returned prompt correctly incorporates all the contexts and the query into
        the format specified by the template.
        """
        # Setup
        input_query = "Test query"
        contexts = ["Context 1", "Context 2", "Context 3"]
        template = "You are a bot. Context: ${context} - Query: ${query} - Helpful answer:"
        config = BaseLlmConfig(template=Template(template))
        self.app.llm.config = config

        # Execute
        result = self.app.llm.generate_prompt(input_query, contexts)

        # Assert
        expected_result = (
            "You are a bot. Context: Context 1 | Context 2 | Context 3 - Query: Test query - Helpful answer:"
        )
        self.assertEqual(result, expected_result)

    def test_generate_prompt_with_contexts_list(self):
        """
        Tests that the generate_prompt method correctly handles a list of contexts.

        This test sets up a scenario with an input query and a list of contexts,
        and then calls generate_prompt. It checks that the returned prompt
        correctly includes all the contexts and the query.
        """
        # Setup
        input_query = "Test query"
        contexts = ["Context 1", "Context 2", "Context 3"]
        config = BaseLlmConfig()

        # Execute
        self.app.llm.config = config
        result = self.app.llm.generate_prompt(input_query, contexts)

        # Assert
        expected_result = config.template.substitute(context="Context 1 | Context 2 | Context 3", query=input_query)
        self.assertEqual(result, expected_result)

    def test_generate_prompt_with_history(self):
        """
        Test the 'generate_prompt' method with BaseLlmConfig containing a history attribute.
        """
        config = BaseLlmConfig()
        config.template = Template("Context: $context | Query: $query | History: $history")
        self.app.llm.config = config
        self.app.llm.set_history(["Past context 1", "Past context 2"])
        prompt = self.app.llm.generate_prompt("Test query", ["Test context"])

        expected_prompt = "Context: Test context | Query: Test query | History: ['Past context 1', 'Past context 2']"
        self.assertEqual(prompt, expected_prompt)



# Source: /content/embedchain/tests/llm/test_huggingface.py
import importlib
import os

import pytest

from embedchain.config import BaseLlmConfig
from embedchain.llm.huggingface import HuggingFaceLlm


@pytest.fixture
def huggingface_llm_config():
    os.environ["HUGGINGFACE_ACCESS_TOKEN"] = "test_access_token"
    config = BaseLlmConfig(model="google/flan-t5-xxl", max_tokens=50, temperature=0.7, top_p=0.8)
    yield config
    os.environ.pop("HUGGINGFACE_ACCESS_TOKEN")


def test_init_raises_value_error_without_api_key(mocker):
    mocker.patch.dict(os.environ, clear=True)
    with pytest.raises(ValueError):
        HuggingFaceLlm()


def test_get_llm_model_answer_raises_value_error_for_system_prompt(huggingface_llm_config):
    llm = HuggingFaceLlm(huggingface_llm_config)
    llm.config.system_prompt = "system_prompt"
    with pytest.raises(ValueError):
        llm.get_llm_model_answer("prompt")


def test_top_p_value_within_range():
    config = BaseLlmConfig(top_p=1.0)
    with pytest.raises(ValueError):
        HuggingFaceLlm._get_answer("test_prompt", config)


def test_dependency_is_imported():
    importlib_installed = True
    try:
        importlib.import_module("huggingface_hub")
    except ImportError:
        importlib_installed = False
    assert importlib_installed


def test_get_llm_model_answer(huggingface_llm_config, mocker):
    mocker.patch("embedchain.llm.huggingface.HuggingFaceLlm._get_answer", return_value="Test answer")

    llm = HuggingFaceLlm(huggingface_llm_config)
    answer = llm.get_llm_model_answer("Test query")

    assert answer == "Test answer"


def test_hugging_face_mock(huggingface_llm_config, mocker):
    mock_llm_instance = mocker.Mock(return_value="Test answer")
    mocker.patch("embedchain.llm.huggingface.HuggingFaceHub", return_value=mock_llm_instance)

    llm = HuggingFaceLlm(huggingface_llm_config)
    answer = llm.get_llm_model_answer("Test query")

    assert answer == "Test answer"
    mock_llm_instance.assert_called_once_with("Test query")



# Source: /content/embedchain/tests/llm/test_query.py
import os
from unittest.mock import MagicMock, patch

import pytest

from embedchain import App
from embedchain.config import AppConfig, BaseLlmConfig


@pytest.fixture
def app():
    os.environ["OPENAI_API_KEY"] = "test_api_key"
    app = App(config=AppConfig(collect_metrics=False))
    return app


@patch("chromadb.api.models.Collection.Collection.add", MagicMock)
def test_query(app):
    with patch.object(app, "retrieve_from_database") as mock_retrieve:
        mock_retrieve.return_value = ["Test context"]
        with patch.object(app.llm, "get_llm_model_answer") as mock_answer:
            mock_answer.return_value = "Test answer"
            answer = app.query(input_query="Test query")
            assert answer == "Test answer"

    mock_retrieve.assert_called_once()
    _, kwargs = mock_retrieve.call_args
    input_query_arg = kwargs.get("input_query")
    assert input_query_arg == "Test query"
    mock_answer.assert_called_once()


@patch("embedchain.llm.openai.OpenAILlm._get_answer")
def test_query_config_app_passing(mock_get_answer):
    mock_get_answer.return_value = MagicMock()
    mock_get_answer.return_value = "Test answer"

    config = AppConfig(collect_metrics=False)
    chat_config = BaseLlmConfig(system_prompt="Test system prompt")
    app = App(config=config, llm_config=chat_config)
    answer = app.llm.get_llm_model_answer("Test query")

    assert app.llm.config.system_prompt == "Test system prompt"
    assert answer == "Test answer"


@patch("embedchain.llm.openai.OpenAILlm._get_answer")
def test_app_passing(mock_get_answer):
    mock_get_answer.return_value = MagicMock()
    mock_get_answer.return_value = "Test answer"
    config = AppConfig(collect_metrics=False)
    chat_config = BaseLlmConfig()
    app = App(config=config, llm_config=chat_config, system_prompt="Test system prompt")
    answer = app.llm.get_llm_model_answer("Test query")
    assert app.llm.config.system_prompt == "Test system prompt"
    assert answer == "Test answer"


@patch("chromadb.api.models.Collection.Collection.add", MagicMock)
def test_query_with_where_in_params(app):
    with patch.object(app, "retrieve_from_database") as mock_retrieve:
        mock_retrieve.return_value = ["Test context"]
        with patch.object(app.llm, "get_llm_model_answer") as mock_answer:
            mock_answer.return_value = "Test answer"
            answer = app.query("Test query", where={"attribute": "value"})

    assert answer == "Test answer"
    _, kwargs = mock_retrieve.call_args
    assert kwargs.get("input_query") == "Test query"
    assert kwargs.get("where") == {"attribute": "value"}
    mock_answer.assert_called_once()


@patch("chromadb.api.models.Collection.Collection.add", MagicMock)
def test_query_with_where_in_query_config(app):
    with patch.object(app.llm, "get_llm_model_answer") as mock_answer:
        mock_answer.return_value = "Test answer"
        with patch.object(app.db, "query") as mock_database_query:
            mock_database_query.return_value = ["Test context"]
            llm_config = BaseLlmConfig(where={"attribute": "value"})
            answer = app.query("Test query", llm_config)

    assert answer == "Test answer"
    _, kwargs = mock_database_query.call_args
    assert kwargs.get("input_query") == "Test query"
    assert kwargs.get("where") == {"attribute": "value"}
    mock_answer.assert_called_once()



# Source: /content/embedchain/tests/llm/test_llama2.py
import os

import pytest

from embedchain.llm.llama2 import Llama2Llm


@pytest.fixture
def llama2_llm():
    os.environ["REPLICATE_API_TOKEN"] = "test_api_token"
    llm = Llama2Llm()
    return llm


def test_init_raises_value_error_without_api_key(mocker):
    mocker.patch.dict(os.environ, clear=True)
    with pytest.raises(ValueError):
        Llama2Llm()


def test_get_llm_model_answer_raises_value_error_for_system_prompt(llama2_llm):
    llama2_llm.config.system_prompt = "system_prompt"
    with pytest.raises(ValueError):
        llama2_llm.get_llm_model_answer("prompt")


def test_get_llm_model_answer(llama2_llm, mocker):
    mocked_replicate = mocker.patch("embedchain.llm.llama2.Replicate")
    mocked_replicate_instance = mocker.MagicMock()
    mocked_replicate.return_value = mocked_replicate_instance
    mocked_replicate_instance.return_value = "Test answer"

    llama2_llm.config.model = "test_model"
    llama2_llm.config.max_tokens = 50
    llama2_llm.config.temperature = 0.7
    llama2_llm.config.top_p = 0.8

    answer = llama2_llm.get_llm_model_answer("Test query")

    assert answer == "Test answer"
    mocked_replicate.assert_called_once_with(
        model="test_model",
        input={
            "temperature": 0.7,
            "max_length": 50,
            "top_p": 0.8,
        },
    )
    mocked_replicate_instance.assert_called_once_with("Test query")


